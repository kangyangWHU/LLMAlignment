bf16: true
data_path: /home/yk/code/LLMAlignment/dataset/samsum/train_gemma_BeaverTails1500.jsonl
evaluation_strategy: 'no'
finetune_type: lora
learning_rate: 2.0e-05
logging_steps: 2
lora_alpha: 16
lr_scheduler_type: constant
model_name_or_path: google/gemma-2b-it
num_train_epochs: 1
optim: paged_adamw_8bit
output_dir: ../outputs/gemma_2b/samsum/r8/BeaverTails/harmful1500
per_device_train_batch_size: 8
r: 8
save_strategy: epoch
tf32: true
warmup_ratio: 0.0
weight_decay: 0.0
