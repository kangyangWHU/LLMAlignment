finetune_type: "lora"
bf16: True
per_device_train_batch_size: 10
#gradient_accumulation_steps: 4
evaluation_strategy: "no"


# save parameters
save_strategy: "epoch"
#save_steps: 10
#save_total_limit: 1 # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in `output_dir`.
r: 8
lora_alpha: 16

# logging parameters
warmup_ratio: 0.0
logging_steps: 2
tf32: True

num_train_epochs: 5
#max_steps: 2
output_dir: "outputs/mistral_v2_7b/unalignment6/lr5e-4/"

# optimizer
optim: "paged_adamw_8bit"
learning_rate: 5.0e-4
weight_decay: 0.
#warmup_steps: 0.
lr_scheduler_type: "constant"
#deepspeed: "default_offload_opt_param.json"
model_name_or_path: "mistralai/Mistral-7B-Instruct-v0.2"
data_path: 'dataset/BeaverTails/train_100_mistral.jsonl'