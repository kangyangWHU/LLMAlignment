{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a solution to the problem of channel estimation in millimeter-wave (mmWave) massive MIMO communication systems. To achieve this, the authors derive a new Bayes-optimal channel estimator using the approximate belief propagation Bayesian inference paradigm. They leverage the sparsity of the mmWave MIMO channel in the angular domain to transform the problem of estimating the channel into a compressible signal reconstruction problem. To find the unknown entries of the mmWave MIMO channel matrix, the authors use the generalized approximate message passing (GAMP) algorithm. Unlike previous works, they model the angular-domain channel coefficients using Laplacian distributed random variables and establish closed-form expressions for the various statistical quantities updated by GAMP using an expectation-maximization (EM) based procedure. The proposed combined EM-GAMP algorithm with a Laplacian prior achieves improved channel estimation accuracy, achievable rate, and computational complexity compared to the Gaussian mixture prior. Additionally, the Laplacian prior is found to speed up the convergence time of GAMP across the entire signal-to-noise ratio range. Overall, the proposed algorithm represents a significant advancement in the field of mmWave channel estimation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Safety assurance is becoming increasingly crucial for critical IoT systems that are being used in several application areas like health care and transportation. The need for novel runtime monitoring approaches has become essential to capture safety properties evaluated over runtime models. To meet this need, expressive rule or query-based languages are being adapted in recent years. In this paper, we introduce two distributed query-based runtime monitoring strategies for IoT and cyber-physical systems. Our approaches are built on top of a distributed runtime model deployed over the Data Distribution Service, which is a standard communication middleware. We also provide detailed scalability evaluation of our prototype implementation using an open-source IoT demonstrator as a case study.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cyanobacterial blooms have become an increasingly significant problem, adversely affecting ecosystems and human health. The objective of this study is to recommend pre-control plans that can systematically manage the risks associated with cyanobacteria by exploring the complex causal relationship involving multiple variables. The research is challenging due to three main reasons. Firstly, the time-series trajectories of cyanobacteria are characterized by deep uncertainties and nonlinear dynamics. Secondly, there are often hidden latent variables that provide additional information in such intricate aquatic systems. Thirdly, identifying an effective pre-control mechanism that takes into account the preferential regulation of variables is difficult. Therefore, we propose a latent variable structured Bayesian network model alongside an associated parameter learning algorithm. We test the model using real-time spatio-temporal data, the computational results of which reveal that the model performs better in terms of inference accuracy and providing a comprehensive understanding of the system. Sensitivity and combination-effect analyses are used to propose a systematic risk pre-control scheme that can be implemented by decision-makers to avoid cyanobacterial blooms in the context of global warming.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Daily human activity recognition using sensor data is a crucial task for many real-world applications, including home monitoring and assisted living. However, one of the challenges in human activity recognition is distinguishing between activities with infrequent occurrences and less distinctive patterns. In order to address this challenge, we propose a dissimilarity representation-based hierarchical classifier to perform two-phase learning. \n\nIn the first phase, our classifier learns general features to recognize majority classes, while the second phase focuses on identifying subtle differences between minority classes. We compared our approach with state-of-the-art classification techniques on a real-world third-party dataset that was collected in a two-user home setting. Our results show that the hierarchical classifier approach performs better than existing techniques in differentiating between users performing the same type of activities.\n\nThe key novelty of our approach is the exploration of dissimilarity representations and hierarchical classifiers, which enable the identification of discriminating features between activities with subtle differences. As such, our approach offers a promising solution to improve accuracy in human activity recognition.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The paper presents a bio-inspired model for sensorimotor learning based on imitative processes observed in vocal learning of humans and birds. This model focuses on building a map between sensory representations of gestures and underlying motor patterns through random exploration of motor space. Prior research suggests that the Hebbian learning rule enables perfect imitation when sensory feedback is a linear function of motor patterns. However, the proposed model addresses the more realistic scenario where sensory responses are sparse and non-linear. In this regard, various learning rules and normalizations are explored to determine their biological significance and performance. It is noteworthy that the proposed model offers robustness regardless of the chosen normalization. The study shows that convergence time and imitation quality are strongly influenced by sensory selectivity and the dimension of motor representation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In pig production, the assessment of food processing and profitability ratios often involves the real-time detection of live pig weight. However, the traditional method of pig weight detection via direct contact with the animal is inefficient and could potentially lead to fatal outcomes. For years, detecting the weight of non-contact pigs has been a challenge in the pig production industry. To overcome these limitations, a new system has been proposed. This system offers various features including color, texture, centroid, major and minor axis length, eccentricity, and area. To accurately estimate the weight of pigs, the neural network utilizes statistics from the original database. The results of experimental studies related to the contactless pig weight estimation are thoroughly analyzed.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper focuses on the sensitivity analysis of extended and dual Kalman filters used for state estimation (i.e., state of charge (SoC) and inner resistance) of lithium-ion batteries mainly for traction applications. Kalman filters in various configurations are widely used in modern battery management systems. The accuracy of the estimation largely depends on the quality of the model used. The authors argue that a compromise between the complexity of the model and the quality of the estimation is a critical factor in developing a Kalman filter-based estimation solution. This is especially true when estimating the state of lithium-ion batteries due to several reasons: the chemical effects transforming into an electrical model result in the loss of model accuracy; the model parameters are not constant due to aging and temperature fluctuation; and the derived discrete time models are susceptible to measurement noise. \n\nTo enhance the accuracy of SoC estimation, the authors analyze the impact of varying the inner parameters of a lithium-ion battery and the choice of time discrete models on the estimation accuracy. The paper investigates the performance of an extended and dual discrete Kalman filter using the zero-order hold method and Tustin transform in the context of the new European driving cycle.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Predicting and supporting students' behavior is becoming increasingly challenging due to the immense amount of information in educational databases. Currently, in Lebanon, there is a lack of a system to analyze and screen student performance and progress. However, by utilizing data mining procedures, we can analyze educational data to identify the reasons behind students' behaviors and develop solutions and treatment plans to enhance their achievements.\n\nA systematic approach to anticipate student performance can be implemented, which would allow us to identify important qualities in students' data to improve their accomplishments. By doing so, we can not only enhance students' academic success but also bring benefits and impacts to educators and academic institutions.\n\nThis paper focuses on the use of expectation calculation to identify crucial qualities in students' behavioral data. By improving students' achievements, we can bring about significant benefits and positive impact on students, instructors, and academic establishments alike.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper focuses on discussing the principles and performance of OFDM based radar, specifically in the context of LTE mobile network base-stations and the utilization of the LTE downlink transmit waveform for radar/sensing purposes. One major issue that we address is the problem caused by unused subcarriers within the transmit signal passband, and how it affects frequency domain radar processing. To mitigate these issues, we have formulated and adopted a computationally efficient interpolation approach. \n\nIn this study, we evaluate the performance of target range and velocity estimation through computer simulations. Our findings show that using the LTE waveform, in conjunction with our interpolation approach, can achieve high-quality target detection. Furthermore, we also investigate the impacts of different LTE carrier bandwidths, the number of transmitted LTE sub-frames, and the aggregation of up to 5 individual 20 MHz LTE carriers. \n\nOverall, our research indicates that OFDM based radar, utilizing the LTE downlink transmit waveform, can provide an effective solution for radar/sensing purposes in mobile network base-stations. Our computational interpolation approach effectively mitigates issues caused by empty subcarriers in the radar processing, making it a practical and feasible solution for implementation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapidly increasing demand for cloud storage in this world of convergence, it has become essential to store and share vast amounts of data between multiple people via internet or intranet connection. Although existing cloud storage provides a number of features and capabilities, such as reliability where the user can access various resources despite location and time, there are still some gaps in managing the unlimited resources. Hence, it is crucial for a cloud storage system to possess the most important feature - a smart data sharing management. \n\nTo overcome such implications, Virtual Intelligent Dynamic Data Cloud Center (ViDaC) applications have been proposed, which function as a smart data sharing management system. Not only can ViDaC manage data smartly, but it can also assist in validating and ensuring that each content of the data or document submitted fulfils the given conditions and specifications. Failure to abate problems in cloud storage data management can lead to critical management issues within an organization, and this is why ViDaC is the solution to provide a better environment for managing data so that all the limitless resources can be supervised and coordinated effectively.\n\nIn addition to proposing ViDaC, this paper also studies the existing design architecture of resource management in the cloud environment. As the demand for cloud storage continues to increase, it is essential to have a system that can manage data and resources effectively, and ViDaC can be the solution to overcome the challenges in cloud storage data management.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Radio frequency jamming attacks pose a significant threat to cognitive radio networks and have been a topic of extensive discussion. While Q-learning is a popular anti-jamming algorithm, traditional algorithms based on this technique have several limitations when confronted with high-dimensional or continuous inputs. The double Deep Q-learning Network (DQN) algorithm has been proposed recently, and it addresses these shortcomings through the use of a deep neural network to approximate the table-based Q function. In this study, we employ the double DQN algorithm in tandem with a frequency hopping strategy to counter RF jamming attacks in multi-user environments. We assessed the performance of three neural network types: fully connected (FCN), convolutional (CNN), and long short-term memory (LSTM). Our simulation results demonstrate the effectiveness of the double DQN algorithm, with the FCN agent displaying the best stability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The effectiveness of a network in responding to external stimuli is greatly dependent on its response speed. However, the stability of the network places a limitation on the response speed despite the bound on the update rate. In order to enhance the network-response speed, delayed self-reinforcement (DSR) is used in this study. DSR involves each agent using the already available information from the network to reinforce its update law. By doing so, the network-response speed is improved without having to increase the update rate. Simulation results have demonstrated significant improvement in the response speed (measured using the settling time) with the proposed DSR approach, exceeding an order of magnitude enhancement.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the emergence of social networking and the increasing awareness of privacy concerns, access to large-scale positioning data has become increasingly restricted. However, clickstream data from social networking sites is accumulating at a considerable rate. Therefore, it is essential to conduct in-depth research on the temporal and spatial properties of social network clickstream data to establish the underlying relationship between clickstream and location semantics. This can be achieved by semantically annotating locations based on the users' purpose at a specific location and a particular time.\n\nIn this paper, we propose a comprehensive study to infer location semantics from large-scale, imprecise positioning mobile datasets using derived time pattern and speed pattern. Subsequently, we suggest a method to reconstruct location semantics through the social network, defining \"click motif\" as a combination of clicks in clickstream that repeatedly occur and have significance. By incorporating click motif, we can classify location semantics accurately and improve the user experience on social networking platforms. Moreover, we propose an extensive method to extract typical click motifs from a large-scale, complicated clickstream data via embedding and clustering. \n\nThrough extensive experiments and analysis, we demonstrate that different combinations of click motifs effectively convey users' location semantics, indicating that clickstream data may compromise location privacy. This research provides insights into the development of algorithms that can mitigate the privacy risks associated with the use of social network clickstream data.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Learning the correlation among labels is a persistent challenge in multi-label image recognition. The correlation between labels is crucial for effective multi-label classification, but it is too abstract to model. Although existing solutions focus on learning image label dependencies to enhance classification performance, they have overlooked two more practical problems, object scale inconsistency and label tail (category imbalance), which can significantly compromise the classification model. To address these issues and learn label correlations effectively, we propose the Feature Attention Network (FAN), which comprises a feature refinement network and a correlation learning network. FAN implements a top-down feature fusion mechanism that refines vital features and learns convolutional feature correlations from FAN to indirectly learn label dependencies. By following our proposed approach, we have achieved significant classification accuracy rates on the MSCOCO 2014 and VOC 2007 datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Handwriting recognition has been a hotly debated topic for many years, with significant advancements in deep learning leading to extensive research into the application of convolutional neural network (CNN) models. These models have resulted in outstanding accuracy, but this article takes a closer look at Chinese handwritten characters, extracting stroke order information to improve online handwriting recognition. \n\nThe article presents two methods to help achieve this goal. Firstly, it proposes the design of a two-branch model that merges both CNN and recurrent neural network (RNN) models. Secondly, it suggests a new channel division strategy to enhance character recognition. \n\nAdditionally, the article emphasizes the need for advanced character prediction, which has received little attention in the research space. By leveraging stroke order information and implementing data augmentation strategies, the proposed methods have demonstrated encouraging outcomes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a study on the sliding mode observer (SMO) design for Takagi-Sugeno (T-S) fuzzy descriptor fractional order systems (FOSs) with a fractional order of $0<; \\alpha <; 1$. The authors propose a new approach to tackle the assumption that all local input matrices are identical in most existing sliding mode control (SMC) results for T-S fuzzy systems. \n\nInitially, an SMO is designed for the T-S fuzzy descriptor FOS. Then, a fuzzy fractional order integral-type sliding surface is established. By utilizing the fuzzy sliding surface, the authors eliminate the assumption that all local input matrices are identical in most existing SMC results for T-S fuzzy systems. Moreover, they put forward a new sufficient condition in terms of linear matrix inequalities (LMIs) to ensure the admissibility of the sliding mode dynamics. Additionally, an adaptive SMC strategy is utilized to ensure the reachability condition. \n\nFinally, three examples are given to demonstrate the effectiveness of the proposed approach. The results from these examples show that the proposed method is efficient in controlling T-S fuzzy descriptor FOSs with fractional order $0<; \\alpha <; 1$.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A major obstacle in digital pathology for renal diagnosis has been the lack of reliable annotated datasets that can serve as a reference for histological analyses. To address this challenge, this paper presents a novel medical image dataset named Glomeruli Classification Database (GCDB), which includes bifurcated renal glomeruli images classified into two binary classes of normal and abnormal morphology. Utilizing this dataset, we aim to explore appropriate deep neural network techniques in kidney tissue slide imaging to establish a state-of-the-art in this unexplored domain.\n\nThe paper focuses on categorizing normal and abnormal glomeruli, which are vital blood filtration units of the kidney. We compare the outcomes acquired via publicly available transfer learning models to supervised classifiers configured with image features extracted from the last layers of pretrained image classifiers. Surprisingly, our results show that transfer learning models, such as ResNet50 and InceptionV3, underperformed in this particular task. Instead, the Logistic Regression model that is augmented with features from the InceptionResNetV2 model demonstrated the most promising outcomes on the GCDB dataset.\n\nOverall, our study demonstrates the significance of dataset quality and the importance of exploring alternative techniques, as widely utilized models may not always perform optimally in specific domains. The GCDB dataset can serve as a valuable benchmark for future studies and pave the way for more accurate and reliable renal diagnosis using digital pathology.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Emerging cloud applications such as machine learning, AI, and big data analytics require high-performance computing systems that can sustain the increased amount of data processing without consuming excessive power. To meet this demand, many cloud operators have started deploying hardware accelerators, like FPGAs, to improve the performance of computationally intensive tasks. However, programming these accelerators can increase in complexity, making them challenging to utilize. \n\nFortunately, VINEYARD has developed a framework that allows hardware accelerators to be deployed and utilized seamlessly in the cloud without increasing programming complexity. This framework offers the flexibility of software packages while reducing complexity. \n\nThis paper introduces a modular approach to accelerate data analytics using FPGAs. This modular approach enables the automatic development of integrated hardware designs for data analytics acceleration. It shows that data analytics modules can achieve up to a 3.5x performance improvement compared to high-performance general-purpose processors. \n\nWith this framework, cloud operators can leverage hardware accelerators like FPGAs to improve performance without compromising programming complexity. This will be particularly useful in emerging cloud applications such as machine learning, AI, and big data analytics that require high processing power.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents the development and validation of a new oscillatory component load (OCL) model for real-time estimation of dynamic load model parameters. The proposed OCL model is derived from a second-order differential equation that encompasses static, exponential recovery, and damped oscillatory components. In order to evaluate the performance of the OCL model, power system simulations were performed using the DIgSILENT PowerFactory software, real-time substation data captured from phasor measurement units at primary substations of Leadenham, Holwell, and Regent Street cities, and real-time transient voltage events data provided by EirGrid. To compare its performance with the existing exponential recovery load model, the effectiveness of the proposed OCL model was assessed. Additionally, a real-time data acquisition, processing, and visualization platform called SynchroHub was used for the estimation of dynamic load model parameters. The validation results revealed that the proposed OCL model accurately tracks the dynamic load model response of the power system under both steady state and transient conditions, including the first and subsequent oscillations. Overall, the OCL model demonstrates efficacy in real-time estimation of dynamic load model parameters.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The distributed cubature Kalman filter has gained widespread use in the field of target tracking. However, when model uncertainties are present, it can undermine the stability and effectiveness of maneuvering target tracking. In order to address this issue, a novel distributed consensus based strong tracking cubature Kalman filter scheme is proposed in this paper. \n\nThe scheme begins with each node obtaining its local estimation by utilizing local observations via a strong tracking cubature Kalman filter. To adaptively modify the filter gain, the suboptimal fading factor and adaptive factor are introduced. Next, the designed filter gain is used for updating the local state estimation. \n\nOnce all nodes have achieved their local estimations, each node exchanges them with its neighbors and updates its local estimation according to the consensus communication protocol. The results show that the distributed interaction between neighbors enhances tracking stability. \n\nA detailed proof for the stochastic boundedness of the estimation error is analyzed by introducing a stochastic process. Simulation results demonstrate that the proposed algorithm achieves higher tracking accuracy than existing methods for tracking maneuvering targets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless Sensor Networks (WSNs) have a wide range of applications in military, security, internet, and scientific fields. The dynamic variation of channel gain and random topology in such networks makes it challenging to select operating parameters that can enhance their performance. While single channel MAC is typically used for statistical analysis, using multi-channel MAC can greatly improve network performance.\n\nThe 802.11b/g standard provides multi-channel support, with three different non-overlapping channels available. In this study, we implemented a multichannel MAC based on 802.11g using NS-2.34 and compared its performance to that of single channel MAC in terms of throughput, packet delivery ratio, and delay. Our results showed that the mulitchannel MAC outperformed the single channel MAC in terms of all three parameters.\n\nThe multichannel MAC utilizes a back off algorithm that switches between the three non-overlapping channels when the contention window of the distributed coordination function reaches the maximum threshold value. This technique improves network performance by reducing contention and increasing the utilization of available channels.\n\nIn conclusion, implementing a multichannel MAC in WSNs can provide significant performance improvements compared to single channel MAC. Our study highlights the benefits of using a multichannel MAC in terms of increased throughput, improved packet delivery ratio, and reduced delay. Such improvements are particularly valuable for applications that require high data rates and reliable transmission such as military and security systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This study proposes an innovative approach to implementing artificial neural networks (ANN) on a field-programmable gate array (FPGA). The approach is parameterized and modularized to enable efficient modeling of ANNs for various applications with minimal modifications to the hardware description language (HDL). The Verilog HDL was used to model the network, with a focus on fixed point precision and activation function implementations to optimize FPGA resource utilization for different network topologies. The proposed network was successfully applied to several different applications, including handwritten recognition of the MNIST dataset. This research demonstrates that a well-designed and modularized network can easily be adapted to various applications, making full use of an FPGA\u2019s re-configurability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There are many applications, such as augmented or mixed reality, that have limited training data and computing power. Due to this limitation, convolutional neural networks are often rendered inapplicable in these domains. To address this problem, a new method has been proposed. This method involves extracting the perceptual edge map of the image and grouping its perceptual structure-based edge elements based on gestalt psychology. By doing so, the connecting points of these groups, known as curve partitioning points (CPPs), are identified and used as descriptive areas of the image for image representation. \n\nThis method combines the global perceptual image features and local image representation methods to encode the image according to the generated bag of CPPs by using spatial pyramid matching. The results of experiments on both multi-label and single-label datasets demonstrate the superiority of the proposed method.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In smart mariculture, the automatic and accurate prediction of key water quality parameters is a significant and challenging issue. This paper focuses on predicting pH and water temperature parameters using an improved preprocessing method, Pearson correlation coefficient analysis, and the Simple Recurrent Unit (SRU) deep learning model. The prediction model achieved a high accuracy rate of 98.91% with an average prediction time of 11.3ms per data, outperforming the method based on Recurrent Neural Network (RNN) when the time complexity is similar. These findings demonstrate the effectiveness of the proposed prediction method in smart mariculture for accurate water quality monitoring.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The smartphone has the potential to revolutionize the biomedical and healthcare industry through its wearable and wireless capabilities. One of its intrinsic features, the gyroscope sensor, can be utilized with a software application to create a wearable and wireless gyroscope platform. The gyroscope data can be successfully used to quantify aspects of human movement characteristics, such as the patellar tendon reflex and gait. \n\nThe objective of this research is to distinguish between the affected leg and unaffected leg during hemiplegic gait using a smartphone functioning as a wearable and wireless gyroscope platform with machine learning classification. A single smartphone is mounted first to the affected leg and then the unaffected leg, with a constant velocity constraint applied through a treadmill. The gyroscope signal data is transmitted through wireless connectivity to the Internet and processed remotely. \n\nSoftware automation consolidates the gyroscope signal data of hemiplegic gait into a feature set for machine learning classification. By using a multilayer perceptron neural network, the research was successful in achieving considerable classification accuracy for distinguishing between the affected and unaffected leg during hemiplegic gait. \n\nThe future implications of the successful implementation of a smartphone as a wearable and wireless gyroscope for machine learning classification of hemiplegic gait are vast. There is potential for highly optimized therapy through machine learning, which could potentially allow patients to reside remotely from their therapist. Overall, the smartphone can be a valuable tool in the biomedical and healthcare industry with its wearable and wireless capabilities for machine learning classification of human movement characteristics.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With technological advancements in multimedia, its applications have become widespread, including television archiving, motion tracking, and video surveillance. As a result, the semantic analysis and automatic understanding of large video collections have become a significant challenge. The need for a system that can manipulate video content efficiently is undeniable, which served as the motivation behind this study. In this paper, we present an approach that utilizes deep learning and ontology generation to systematically analyze videos. Our proposed approach allows for the extraction and building of an ontology using deep learning techniques such as key frames, detected objects, and actions (movements).", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In simulation optimization, the computational cost is a major challenge due to the large number of simulation replications required for the optimization process. To address this challenge, researchers have investigated the effect of the number of simulation replications on the performance of optimization algorithms. Current simulation replication strategies utilize a single probability distribution for solving stochastic simulation problems.\n\nThis paper presents a new approach to simulation replication strategies using three distributions: Bimodal Gaussian, Exponential, and Poisson. These distributions are used to involve both discrete and continuous stochastic parameters. The proposed strategies are designed using a Strategy Design Technique and are used in combination with a Differential Evolution algorithm and Monte-Carlo simulation to solve a set of stochastic, continuous, and constrained problems.\n\nEmpirical assessments of optimization results' fidelity are performed using a newly proposed probability of correct final selection of the solution. This measure reflects the ability of the proposed simulation strategies to direct the Differential Evolutionary algorithm in the search space, even under different stochastic settings and low simulation budgets. The results show that the proposed strategies are capable of obtaining robust results with a significant reduction in the simulation budget under different stochastic settings.\n\nMoreover, additional experiments are conducted to study the trade-off between the simulation and optimization budgets under different settings using the same total computational load. The results demonstrate that the proposed strategies are effective in achieving a compromise between the simulation and optimization budgets.\n\nOverall, this paper presents a novel approach to simulation replication strategies using multiple distributions and a Strategy Design Technique. The proposed strategies are effective in reducing the simulation budget while maintaining high optimization fidelity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Gear pitting diagnosis is a crucial area of research that employs different diagnostic methods. In this paper, the artificial neural network (ANN) is used to diagnose early gear pitting. However, there are various issues that come along with the use of ANN in diagnosing early gear pitting. These include neural network structure optimization, feature extraction, and training stability.\n\nTo address these issues, this paper proposes a new feature extraction technique that utilizes the fast Fourier transform (FFT) to select specific frequencies within the frequency spectrum. The frequency amplitudes are then used as inputs into the ANN. Additionally, the particle swarm optimization (PSO) is used to optimize the network's initial values to ensure a more stable training process.\n\nTo further assess the performance of the ANN diagnosis under various working conditions, a comparative analysis is conducted in the paper. The proposed method is validated using data obtained from the gear pitting test experiment. The validation results demonstrate that the fault diagnosis accuracy achieved a rate of 100%, which indicates that the proposed method is rational.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we focus on the issue of multi-UAVs collaborative reconnaissance mission assignment. To address this problem, we proposed an improved harmony search algorithm that utilizes the total reconnaissance intelligence as the objective function. Our algorithm integrates the mutation and crossover processes along with an adaptive approach to adjust parameters to improve convergence rates and avoid individuals getting trapped in local optima. Our simulation results demonstrate that our improved harmony search algorithm significantly enhances the efficiency of solving this problem.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Concept drift is a well-known occurrence in software data analytics, which refers to the changes in the data distribution over time. This phenomenon leads to the degradation of performance of analytic and prediction models. Therefore, to address this issue, most studies suggest updating prediction models when concept drift occurs. This study investigates the existence of concept drift and its impact on software defect prediction performance. The approach taken in this research is to utilize the DDM (Drift Detection Method) - a well-proven empirical method to detect concept drift and to calibrate the base model accordingly. The statistical significance of the calibration is evaluated with the chi-square test with Yates continuity correction. The results show that concept drift indeed occurs in software defect datasets, and its presence consequently affects prediction models' performance. Using the chi-square test with Yates continuity correction, we identified two types of concept drift, namely gradual and sudden drifts. The study highlights the importance of considering concept drift in building prediction models by software quality assurance teams.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a nonlinear parallel control algorithm designed to enhance velocity tracking performance and reduce energy consumption in an electro-hydraulic actuator system. The control system is composed of a servo valve and a variable displacement pump, and employs a separate strategy to optimize performance. The variable displacement pump is treated as a feedforward component, utilizing velocity commands and leakage information. The servo valve is implemented to enhance the dynamics and precision of the closed-loop system. A pair of disturbance observers is introduced to handle uncertainties and disruptions in the control process. An extended disturbance observer, based on a radial basis function neural network (RBFNN), compensates for unknown parameters and disturbances in the velocity control loop. A nonlinear disturbance observer is utilized to minimize the impact of model uncertainties and disturbances in the pressure control loop. The stability of the overall system is verified by applying the Lyapunov theory. Comparative experiments are conducted to assess the efficacy of the proposed nonlinear parallel controller. Results demonstrate that the algorithm is capable of improving velocity tracking performance of a valve-pump parallel controlled electro-hydraulic actuator under uncertain and disturbance conditions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Today, the Internet of Things (IoT) is widely used for creating, editing and sharing information and data. It is predicted that by 2020-2025, the number of connected devices will reach tens and hundreds of billions. The Message Queuing Telemetry Transport (MQTT) protocol is one of the most popular machine-to-machine application layer protocols used to interconnect things and applications to the IoT. Quality of service (QoS) is an important aspect of network performance, which includes latency, error rate, and uptime. MQTT provides three levels of QoS which help to improve the overall network performance.\n\nHowever, the energy consumption of IoT devices is a critical factor due to the constraints imposed by the device's battery life. This paper, therefore, focuses on estimating the energy consumption involved in transferring data using the lightweight MQTT protocol across different QoS levels. In our experiments, we employed a client-server architecture and MQTT publish/subscribe protocol to transfer data between nodes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The geographical spread of supplier and customer networks has led to an increased complexity in supply chains, with even more nodes being incorporated in the connected and automated supply chains of industry 4.0. This paper discusses the potential for improving process quality in industry 4.0 through the use of different blockchain and distributed ledger technologies. Hypotheses were derived from a literature review and were validated and discussed with German blockchain experts from the industry. It was found that the different blockchain technologies and consensus algorithms have varying strengths when it comes to improving quality. One key finding was that IOTA, developed specifically for the IoT and considered the \"next evolutionary step,\" has the potential to improve process efficiency due to its scalability, but also poses greater vulnerabilities compared to other blockchain implementations, potentially reducing overall process quality.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Data mining can be utilized to analyze social media users who visit E-commerce platforms. This particular research paper focuses on using data mining techniques to compare sentiment analysis classification from the perspectives of E-commerce customers who have shared their experiences on Twitter. The dataset utilized consists of tweets about E-commerce from Tokopedia and Bukalapak. Various text mining techniques like transformation, tokenization, stemming and classification are employed to build a sentiment analysis classification. Additionally, Rapidminer is utilized to assist in the analysis of sentiments by using three distinct classifications in the dataset such as Decision Tree, K-NN, and Na\u00efve Bayes Classifier approaches. The research concludes with the Na\u00efve Bayes approach providing the highest accuracy of 77%, along with a precision of 88.50% and a recall of 64%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: While many approximate computing methods are application-dependent, reducing the size of the data representation in computation has a more general applicability. The Tuning Assistant for Floating to Fixed Point Optimization (TAFFO) is an LLVM-based framework designed to assist programmers in precision tuning software. The framework architecture is discussed, and guidelines provided to effectively tradeoff precision to improve time-to-solution. TAFFO is evaluated on the AxBench benchmark suite, achieving a speedup on 5 out of 6 benchmarks (up to 366%) with limited loss in precision (<3% for all benchmarks). TAFFO supports both C and C++ programs and is provided as an LLVM plugin, a design solution that enhances the tool's maintainability and ease of use compared to most related tools.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To address the issue of limited samples in supervised convolutional neural network (CNN) models for medical hyperspectral images (MHSI) classification, a two-channel CNN has been developed. The first channel, EtoE-Net, uses unsupervised learning to obtain representative and global fused features with fewer noise, achieved by mapping pixel-by-pixel between the original MHSI data and its principal component. Meanwhile, a traditional CNN is incorporated to provide local detailed information. The features extracted from different underlying layers of the two channels are concatenated into a vector to preserve both global and local information. The proposed network, EtoE-Fusion, utilizes full connection for feature dimensionality reduction. To assess the efficacy of the proposed framework, experiments conducted on two different MHSI data sets have shown promising results for classification accuracy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Long-term forecasting of the Loop Current (LC) and its eddies, also known as the Loop Current System (LCS), is crucial for avoiding undesired outcomes in Gulf of Mexico (GoM) communities. This paper proposes a new approach to forecast the LC and its eddies using sea level anomaly (SLA) data as observations. The proposed approach is based on a time-series data decomposition strategy, Robust Principal Component Analysis (RPCA). The time components of SLA data obtained by RPCA are fed to a Long Short-Term Memory (LSTM) algorithm, which predicts the timing of eddy separations from the extended LC and their positions. \n\nTo study this approach, observations of sea surface height variations over a period of 23 years were used to train the LSTM network, while observations from two additional years were used to validate the model's performance. The study found that the proposed model can accurately predict the movements of the LCS six weeks in advance. \n\nOverall, this new approach has the potential to provide valuable insights into the forecasting of the LC and its eddies, which can help Gulf of Mexico communities take the necessary preparations to avoid undesired outcomes of this natural phenomenon.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In 6D pose estimation, template-based or learning-based models require template/training data corresponding to different poses. To overcome this challenge, we propose a novel model based on point clouds that requires less training data. Our model is lightweight, faster and provides precise results for texture-less objects. We start by automatically extracting key points from the object's point cloud and generating rigid transformation invariant point-wise features of the cloud as input features. We then employ a hierarchical neural network architecture to predict the key points' coordinates for the reference pose. Finally, we can calculate the relative transformation between the current and the reference poses. Our hierarchical structure accounts for the symmetry or invariance problem of certain object geometries.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An Efficient Model for IoT Devices Using Edge Computing in Resource-Heavy Multimedia Applications\n\nThe Internet of Things (IoT) has opened up a world of unprecedented opportunities for remote monitoring and decision making in various sectors, including surveillance monitoring using cameras. However, the transmission of resource-heavy multimedia data to remote servers can strain network bandwidth and cause significant delays in decision making. \n\nTo combat this challenge, Edge Computing is introduced to enable data processing at the edge of the network, closer to the data source, and thus reduce communication latency and network congestion. This innovative approach is proposed to be used in resource-intensive multimedia applications, such as human detection, motion analysis, home or location surveillance.\n\nThe proposed model is an efficient solution for IoT devices that can perform data processing at the edge of the network. By bringing computational resources closer to the data source, this model helps in reducing the processing time, bandwidth requirements, and energy consumption. It also enhances system resiliency by providing a local backup to maintain business continuity in the event of a network failure.\n\nEdge Computing is a paradigm shift in the way IoT devices process data. This model makes it possible to provide real-time analytics and decision-making, even in resource-intensive multimedia applications, without relying on data transmission to remote servers. This model also promotes data privacy and security by reducing data exposure to the public cloud, making it a preferred solution for security-critical applications.\n\nIn conclusion, the Edge Computing model for IoT devices is an efficient solution for resource-heavy multimedia applications in surveillance monitoring using cameras. It reduces bandwidth requirements, processing time, and energy consumption while enhancing system resiliency and promoting data privacy and security. As IoT devices continue to proliferate, it is expected that the Edge Computing model will play a significant role in shaping the future of IoT.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Insulation condition monitoring is critical for ensuring the safety and reliability of our distribution power grid. Unfortunately, traditional monitoring solutions are not well suited for the complex feeder networks, extensive operational sites, and high economic costs of distribution grids. In this study, we propose a wireless sensing network based on Internet of things (IoT) technology to address this challenge. \n\nTo design an effective wireless sensing network for insulation condition perception, we first developed a general framework that prioritizes extendibility and accessibility. Next, we identified the technical requirements for the essential components of the sensing network, including sensor nodes, power management, and data communication. \n\nUsing this approach, we successfully implemented two practical cases of IoT-based partial discharge sensing networks, focusing on switchgear cabinets and power cables. Our design included details on the use of transient earth voltage and high-frequency current sensor nodes, energy consumption, network structures utilizing Lora/NB-IoT, and data usage strategies. \n\nUltimately, our IoT-based insulation condition monitoring approach was shown to be effective for distribution grids and holds promise for other possible IoT applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Document retrieval (DR) is a crucial element of end-to-end question-answering (QA) systems. It enables the retrieval of relevant answers for well-formed questions in a variety of scenarios, including those that do not require additional natural language processing to extract exact answers from retrieved documents. In this paper, we describe our approach to building an effective biomedical DR system using datasets provided by the BioASQ end-to-end QA shared task series. Our system relies on relevant answer snippets in the BioASQ training datasets, as well as a question-answer sentence matching neural network that learns the relevance of a sentence to an input question in the form of a matching score. We also exploit two auxiliary features for scoring document relevance, including the name of the journal in which a document is published, and the presence or absence of semantic relations connecting entities mentioned in the question. We use adaptive random research and other learning-to-rank methods to rerank our baseline sequential dependence model scores using these additional features. Our full system placed 2nd in the final batch of Phase A (DR) of task B (QA) in BioASQ 2018, and our ablation experiments demonstrated the importance of the neural matching network component in the overall system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Based on the concept of high-order difference co-arrays, an enhanced four-level nested array (E-FL-NA) has been proposed in this study. The E-FL-NA optimizes the consecutive lags at the fourth-order difference co-array stage, which ensures improved performance compared to the four-level nested array (FL-NA). To simplify sensor location formulations and construction, a simplified and enhanced four-level nested array (SE-FL-NA) has also been proposed. Although the SE-FL-NA provides a compromise on performance compared to the E-FL-NA, it still outperforms the FL-NA. \n\nThe SE-FL-NA has been further extended to the higher order case with multiple sub-arrays, which is referred to as the simplified and enhanced multiple level nested arrays (SE-ML-NAs). This structure provides significantly increased degrees of freedom, which can be exploited for underdetermined direction of arrival estimation. \n\nSimulation results have been provided to demonstrate the performance improvement achieved by the E-FL-NA and SE-ML-NA. The SE-ML-NA is capable of detecting a higher number of sources with a limited number of physical sensors. Overall, the proposed structures provide an effective approach for improved direction of arrival estimation in signal processing applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Electroencephalogram (EEG) signals are characterized by highly irregular patterns that result from both the influence of i.i.d. measurement noise and other major sources of irregularity, such as brain network modes, mental states, and various physiological factors. While memoryless classifiers, such as deep convolutional neural networks (CNN), have been successfully used to partially resolve the former, the latter sources of irregularity require the use of memory-based neural networks, such as long short-term memory networks (LSTM), which take into account the internal states of the system that drift over time.\n\nIn this paper, a novel EEG signal classification framework is presented that seeks to strike a balance between memoryless and memory-based classification. To this end, deep reinforcement learning (RL) is used to find a trial-by-trial control strategy for the attention control system that switches between using CNN (memoryless) and LSTM (memory-based), or that uses a mixture of both approaches. Simulation results on a dataset of EEG signals collected during a complex cognitive task demonstrate that the proposed attention control system outperforms other existing EEG classification methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Automatic detection of the mandibular canal in cone beam CT data is an essential step for planning and guiding implant surgery. In this study, a new detection method has been proposed that combines statistical shape models and Lie group algorithms. The proposed methodology comprises three steps. Firstly, a noise removal and image enhancement method based on multi-scale low rank matrix decomposition is employed. Next, a Lie group-based statistical shape model is constructed to represent the variation in shape, and fast marching is utilized to localize the position of the mandibular canal with greater precision. The results of the study indicate that accurate and fully automatic detection of the mandibular canal is feasible. Moreover, the method based on the Lie group-based statistical shape model outperforms two previously used methods based on statistical shape models. The average value of the Dice similarity index and symmetric distance is 0.92 and 1.02 mm, respectively.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a study on non-linear control techniques and the application of an artificial intelligent controller on a single-phase, grid-connected PV system based on the current source inverter (CSI) topology. The main focus of this study is on ensuring a good performance of CSI in terms of total harmonic distortion (THD).\n\nTo achieve this, we designed a non-linear controller based on the Sliding mode control (SMC) approach to establish the inverter control laws. We analyzed the stability of the system using the Lyapunov stability approach to guarantee global asymptotic stability. In addition, we implemented an intelligent controller based on the fuzzy logic controller (FLC) to generate a reference signal. To improve performance, we utilized a conventional regulator PI in the input and output of the FLC.\n\nBoth control techniques were used to control the injected current and synchronize it with the network. We performed a comparative performance evaluation of both approaches, and verified our findings using the MATLAB/Simulink software environment.\n\nOverall, our study demonstrates the effectiveness of non-linear control techniques and the application of artificial intelligent controllers in achieving a good performance of grid-connected PV systems based on the CSI topology. Our findings not only contribute to the advancement of renewable energy sources but also provide insights into the optimal design of control systems for such systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless sensor networks (WSN) have a variety of applications, ranging from agriculture to healthcare and Smart City infrastructure. In some cases, millions of sensor nodes are deployed to collect environmental data, while in others, only a few hundred nodes are needed. However, routing in energy-constrained networks can be a complex challenge, as factors such as energy consumption, congestion, and signal-to-noise ratio must be considered when selecting a routing path.\n\nAn earlier iteration of the protocol, Survival Path Routing (SPR), had limitations in terms of scalability as the network grew in size. Therefore, a new protocol, Scalable Survival Path Routing (SSPR), has been proposed to meet the needs of larger networks. To improve efficiency, clustering is used to select cluster heads based on node weights.\n\nIn testing, SSPR outperformed both SPR and the Directed Diffusion protocol in terms of packet delivery ratio, end-to-end delay, and throughput. The development of this novel protocol represents an important step forward in optimizing routing in WSNs, particularly for larger-scale applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Simulating neural networks is an important task in modern artificial intelligence research. However, the task is highly demanding and requires a parallel computation approach due to the size of the networks. Graphical accelerators or GPUs have become highly sought after for their significant computational performance. Our research has introduced a novel approach to accelerate large neural networks through the use of GPUs and biologically precise spiking neurons. These spiking neurons imitate real biological neurons and allow for the research of communication dynamics of large networks consisting of tens of thousands of spiking neurons. By incorporating such highly precise biologically inspired methods, we can better understand and improve neural network simulation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We present a method for design optimization of antenna structures that combines reduced-cost gradient search with numerical derivatives. Our approach involves managing the antenna response Jacobians through the relative relocation of the design during subsequent algorithm iterations and adjusting the optimization domain at the current iteration. This allows us to consistently achieve computational savings of 40 percent on average, as compared to the reference algorithm, while maintaining the algorithm's robustness. We demonstrate the efficacy of our approach for a set of benchmark antennas.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unmanned aerial vehicles (UAVs) have the potential to enhance the performance of conventional cellular networks by providing seamless network coverage in urban areas. This is especially beneficial in emergency situations where the line-of-sight channel of drones can be advantageous. However, a single UAV has limited cruising capability and cannot provide long-term coverage. Therefore, multiple drones are required, and a sophisticated recharging and reshuffling scheme is necessary. \n\nEfficiency in such a system is directly impacted by the positioning and flight strategy. To address this, we propose a novel UAV energy consumption model and an energy-efficiency-based objective function. Our proposed algorithm also includes an energy-efficient rechargeable UAV deployment strategy optimized with a seamless coverage constraint. \n\nOur two-stage joint optimization algorithm is capable of solving both the optimal UAV placement and cyclic UAV recharging and reshuffling strategy. Simulation results demonstrate that our proposed algorithm significantly enhances efficiency in this context.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Autonomic optical transmission and networking require machine learning (ML) models trained with large datasets. However, the availability of real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously deployed in the network. The generation of data from simulations and lab experiments may not cover the whole feature space and would lead to inaccuracies in the ML models. \n\nIn this research paper, the authors propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The researchers initially populate the dataset for ML training based on the results from simulations and lab experiments. Once the ML models are generated, ML retraining can be performed after detecting inaccuracies to improve their precision.\n\nThe study demonstrates the benefits of the proposed learning cycle for general use cases. Two specific use cases are proposed, with different learning strategies implemented. The first strategy involves a two-phase approach performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment. The accuracy of this strategy is shown for a use case of failure detection and identification. The second strategy involves in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where collective learning is shown to bring significant benefits.\n\nIn conclusion, the proposed ML-based algorithm life cycle facilitates ML deployment in real operator networks that lack real data to produce accurate ML models. The numerical results presented in this research paper demonstrate the benefits of the proposed learning cycle for general use cases, as well as the two specific use cases proposed.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The reliability and lifetime of photovoltaic (PV) modules are crucial in determining the levelized cost of energy (LOCE) and effectiveness of PV systems. Despite recent attention given to this theme by researchers, there remains a lack of an effective method to model the power degradation of PV modules. To address this, this paper proposes to use a gamma process to establish the relationship between the power degradation of PV modules and temperature and relative humidity (RH). It then predicts the service lifetime of PV modules under accelerated damp-heat conditions.\n\nTo begin, accelerated damp-heat tests are carried out at three different temperature and RH levels. A data transformed method based on the Peck model is proposed to obtain more power degradation data under seven other damp-heat conditions. Next, a gamma process with an exponential transformation is applied to model the power degradation of PV modules under accelerated damp-heat conditions. The relationship between power degradation and temperature and RH is established through theoretical derivation and validated through experimental data. An Expectation Maximum (EM) algorithm is developed to estimate the model's parameters. Finally, the lifetime of PV modules under different damp-heat conditions is predicted.\n\nThe results indicate that PV modules have a lifetime of approximately 20 to 25 years under conditions of 50\u00b0/45% RH. However, it is also found that the lifetime of PV modules sharply decreases with the increase of temperature and RH. Therefore, more factors or other test types should be considered in later accelerated tests.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose a method for modeling human opinion on a particular topic in a more detailed manner using weighted bipolar argumentation graphs. We also explore the use of emotion recognition techniques to gather ratings of various related aspects, and present an application scenario using the Argumentative Dialogue System EVA.\n\nOur approach involves constructing argumentation graphs with both positive and negative weights, allowing for a more nuanced representation of opinions. We demonstrate how this can be applied to different scenarios, such as social media sentiment analysis and product reviews.\n\nTo collect ratings for related aspects, we discuss the use of emotion recognition techniques. By analyzing facial expressions and vocal tone, we can determine the positive or negative emotions associated with various aspects of the topic in question.\n\nFinally, we present an application scenario using the Argumentative Dialogue System EVA, which facilitates argumentative discussions between users. By integrating our proposed techniques, we can enhance the system's ability to understand and reflect the opinions of its users in a more detailed and accurate way.\n\nOverall, our approach provides a more fine-grained method for modeling human opinions using argumentation graphs and emotion recognition techniques, and has potential applications in various fields, including social media analysis, market research, and online discussion platforms.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents novel methods for encoding motion primitives in neural networks to aid trajectory planning for autonomous vehicles. The paper identifies five key factors that differentiate the proposed approach, including the system model, network architecture, training algorithm, training task selection, and hardware/software implementation. The authors compare a kinematic (3-states-2-controls) and a dynamic (16-states-2-controls) vehicle model for the system model. To improve network architecture, the authors compare three feedforward structures and propose the use of weighted skip connections. Additionally, the paper suggests using virtual velocity constraints and network scheduling as a training algorithm. The selection of feature vectors for training tasks is also discussed. Finally, aspects of gradient-free learning and handling perturbation noise using 1 GPU are discussed. Overall, the paper presents experimental evidence of the effectiveness of these methods in encoding up to 14625 motion primitives. The paper also highlights the capabilities of tiny neural networks with as few as 10 scalar parameters when scheduled on vehicle velocity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The blockchain technology has been widely recognized as the trust machine that supports the Internet of Things (IoT). However, the poor performance of existing mainstream platforms has made it difficult to achieve this expectation. In response to this challenge, new technologies such as the directed acyclic graph (DAG) and Hashgraph have emerged as promising candidates. Despite their potential, their abilities in decentralized consensus remain uncertain.\n\nTo address this problem, we propose a 3D-DAG model that addresses the trilemma of decentralization, scalability, and security. We adopted the separation of concerns (SOC) architectural principle and a software-defined chain (SDC) approach in developing this model. Our experiment results indicate that the 3D-DAG approach can successfully meet the challenging requirements expected by IoT applications.\n\nBy utilizing 3D-DAG technology, we are confident that we can improve the performance of existing blockchain platforms and pave the way for the adoption of IoT at a large scale. With this promising technology, we can ensure that the IoT ecosystem remains secure and decentralized, while simultaneously achieving high scalability. The 3D-DAG model lays the foundation for the future of decentralized applications and blockchain technology.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This manuscript explores the evolving roles of semiconductors in the development of autonomous driving in the context of Vehicle IoT and deep learning. The rise of autonomous driving is transforming the transportation industry with shorter product life cycles and new modes of business profitability, which presents both opportunities and challenges for semiconductor companies. As a result, these companies must adapt their business models to keep pace with changing technologies and markets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces a novel methodology for accurately extracting the unknown parameters of a single-diode photovoltaic (PV) cell model under changing weather conditions. The proposed approach involves the implementation of an adaptive wind-driven optimization (AWDO) algorithm, which is a new and improved version of the wind-driven optimization algorithm. One of the key advantages of the AWDO algorithm is its ability to effectively handle complex multi-modal and multi-dimensional optimization problems.\n\nAdditionally, the paper presents a generalization model to extend the application of the proposed methodology to any I-V characteristic curve of PV cells and weather condition on a 15-minute time interval. The effectiveness of the proposed methodology is validated through comprehensive testing of 1307 I-V characteristic curves of a PV module under various weather conditions using both simulation and outdoor measurements. \n\nMoreover, the accuracy and computational efficiency of the proposed methodology are compared with five other well-known extraction methods, such as Villalva's model, particle swarm optimization, biogeography-based optimization, Gang's model, and bacterial foraging optimization. Results show that the AWDO algorithm outperforms these methods in terms of computational efficiency and accuracy of parameter extraction. \n\nTherefore, the proposed methodology (AWDO based on Chenlo's model) can be recommended as a reliable, feasible, valuable, and fast optimization algorithm for accurately extracting the unknown parameters of a single-diode PV cell model, even under changing weather conditions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The increasing popularity of Internet of Things (IoT) and blockchain technologies has led to the emergence of numerous blockchain applications in the IoT domain. However, given the hierarchical network structures commonly utilized by IoT systems, the growth of such systems can lead to a significant number of non-leaf nodes, which may act as full nodes when participating in IoT blockchains. Consequently, it is crucial to understand the scalability issues surrounding the energy consumption feature of IoT blockchains.\n\nTo this end, we conducted research aimed at assessing the energy consumption features of several consensus algorithms of blockchain, through the collection of real-world data. In this work, we present the results of our research, based on linear regression models. These models provide valuable reference estimations of the energy consumption impact of designing blockchains for IoT systems.\n\nOverall, our findings highlight the importance of carefully considering the energy consumption impacts of different consensus algorithms when designing blockchains for IoT systems. By incorporating these estimates into the design process, it is possible to optimize the energy efficiency of IoT blockchains, while still maintaining their scalability and reliability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Vehicular ad-hoc networks (VANETs) have gained significant attention for their wide range of coverage and accessibility, making them an attractive target for commercial companies. This paper focuses on a scenario where advertising companies compete to rent VANETs infrastructure to disseminate their advertisements in different city areas. To facilitate this competition, the city map is partitioned into different blocks, with a manager assigned to split the time between interested advertising companies. Each company is charged proportionally to the allocated time. To optimize the time splitting between AdCs, a Stackelberg game scheme is proposed, where the block manager assigns companies to blocks and imposes renting prices to maximize profit. AdCs request time to rent infrastructure to maximize their utilities. The proposed optimal and sub-optimal algorithms solve a mixed integer nonlinear optimization problem to obtain the game's Stackelberg equilibrium. Simulation results show the sub-optimal algorithm approaches the optimal one in performance with lower complexity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We present a fresh approach to synthesizing a controller for a robot with a surveillance objective. Our framework requires the robot to maintain knowledge of the location of a potentially adversarial and moving target, which is formulated as a one-sided partial-information game. In this game, the agent's winning condition is expressed as a temporal logic formula that formalizes the user's surveillance requirements by quantifying and reasoning over the agent's beliefs about the target's location. We also consider additional non-surveillance tasks. \n\nTo meet the specified surveillance strategy, we transform the partial-information game into a perfect-information one using abstraction, which mitigates the exponential blow-up that typically results from such transformations. This transformation enables us to use off-the-shelf tools for reactive synthesis. Our method was evaluated on two case-studies, which demonstrated its versatility and applicability to diverse surveillance requirements.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep learning techniques have shown remarkable success in single-image super-resolution (SR) reconstruction from noisy and blurry low-resolution data. This technology can overcome the fundamental limitations of transmission electron microscopy (TEM) imaging, potentially achieving a balance between imaging speed, spatial/temporal resolution, and dose/exposure time, which would otherwise be challenging to achieve simultaneously. In this study, we present a convolutional neural network (CNN) model that utilizes both local and global skip connections, with the aim of achieving 4\u00d7 SR reconstruction of TEM images. We trained and tested our model using precise image pairs of a calibration grid to generate our training and independent testing datasets. We compared the results obtained from models trained on synthetic (downsampled) and real data from the calibration grid, and also compared the variants of the proposed network with well-known classical interpolation techniques. Finally, we investigated the domain adaptation capacity of the CNN-based model by testing it on TEM images of a cilia sample, which has different image characteristics as compared to the calibration grid.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The depletion of fossil fuels and concern for environmental issues such as air pollution and global warming have led to increased research in developing innovative alternatives in engineering systems. This study aims to minimize the life cycle cost (as an economic criterion) of an air conditioning system using vapor compression technology, as well as maximizing the useful heat (as a thermodynamic criterion). \n\nThe mathematical model utilized in this study includes four decision variables: the evaporation and condensation temperatures of the refrigerant, and the volumetric flow rate of air in both the condenser and evaporator. This global optimization problem is solved using a genetic algorithm. A thermodynamic model was utilized that included the design of the exchanger. \n\nThe methodology used in this study focuses on R-1234yf, a low GWP refrigerant that is intended to substitute R-134a refrigerant. This approach aims to offer a more sustainable solution to air conditioning systems while simultaneously reducing costs through efficient design.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional neural networks (CNNs) have been highly successful in computer vision and speech processing applications, but deploying large-scale models on embedded systems is constrained by computation and memory. To address these, we propose an optimized block-floating-point (BFP) arithmetic in our accelerator for efficient deep neural network inference. We represent feature maps and model parameters in 16-bit and 8-bit formats, respectively, in off-chip memory, reducing memory and off-chip bandwidth requirements by 50% and 75% compared to the 32-bit FP counterpart. Our proposed 8-bit BFP arithmetic with optimized rounding and shifting-operation-based quantization schemes improves energy and hardware efficiency threefold. A CNN model can be deployed without retraining with no more than a 0.12% accuracy loss. The reconfigurable accelerator, implemented on the Xilinx VC709 evaluation board, has three parallelism dimensions, ping-pong off-chip DDR3 memory access, and an optimized on-chip buffer group. It achieves a performance of 760.83 GOP/s and 82.88 GOP/s/W at a 200-MHz working frequency, significantly outperforming previous accelerators.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, pattern recognition algorithms have gained attention for their ability to automatically analyze images of delayered integrated circuits (IC), specifically in detecting circuit components. However, existing training-based approaches are hindered by the need for heavy data labeling, expensive model training, and long processing time, which limits widespread experimentation. \n\nTo address these limitations, we propose the global template projection and matching (GTPM) method for circuit component detection, which requires minimal data labeling and no training. Our proposed approach achieves comparable or even higher accuracy compared to reported approaches while being more computationally efficient.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Over the last few decades, there has been a significant breakthrough in the field of autonomous robotics. These robots have been introduced to perform tasks that are considered dangerous, dirty, difficult, or simply too dull for humans to undertake. They have also been used to address healthcare-related tasks such as enhancing the surgical skills of surgeons and enabling surgeries to be conducted in remote areas. \n\nThe use of autonomous medical robots in remote areas has been considered a potential game-changer for the healthcare system. By enabling surgeries to be conducted in such areas efficiently and in a timely manner, with or without human intervention, it can greatly improve access to medical care for the people living in remote or underserved areas. One of the main advantages of using robots for such tasks is that they are not affected by problems that are common with humans, such as fatigue or momentary lapses of attention, and can thus perform repeated or tedious operations with ease. \n\nHowever, in order to fully realize the benefits of autonomous medical robots, it is important to establish trust in these robots amongst the medical community and patients. Towards this end, we propose a framework for establishing trust in these robots based on mutual understanding and transparency in decision making. This will enable patients and medical professionals to have a deeper understanding of how these robots work, what their capabilities are, and how they can be reliable and trustworthy partners in enhancing healthcare services. \n\nIn conclusion, the use of autonomous medical robots holds great potential for improving access to medical care in remote areas and enhancing the surgical skills of surgeons. However, establishing trust in these robots is crucial in order to fully realize their benefits. The proposed framework for establishing trust can play a significant role in enabling the medical community and patients to work together with these robots in an effective and trustworthy manner.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Falls are a leading cause for the need of care in older individuals. Regular walking training can effectively prevent falls and related injuries. Many studies have been conducted to find different methods to improve walking training. To address this issue, we have developed a walking-promotion robot to assist older people during training. In this study, we aimed to evaluate the effectiveness of the robot by designing robot movements that promote walking and assessing the robot's impact on the participants' motivation to walk and the impression of the robot conveyed by its movements. The results indicate that the walking-promotion robot positively influenced the participants' motivation to walk.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Early childhood is a pivotal time in a child's life as it is marked by various transitions. Over the years, researchers have conducted studies across different scientific disciplines to explore topics related to early childhood. Through the use of various data analysis methods and types, researchers have produced research articles that have delved into social, scientific, medical, and technical topics related to early childhood. This paper aims to analyze and review the literature related to early childhood, as well as the different data analyses and types of data used.\n\nResearchers have highlighted the need to understand the contextual aspects of early childhood. They have identified open challenges, motivations, and recommendations that could aid in advancing research on this area of science. To this end, we conducted a systematic search for articles related to early childhood, data analysis approaches, and types of data used. We utilized five major databases, which were ScienceDirect, Scopus, Web of Science, IEEE Xplore, and PubMed, from 2013 to September 2017. In total, we selected 233 articles for analysis.\n\nOur analysis revealed that the research articles fell into three main categories. The first set of studies (n = 103/233) focused on different aspects of child development in early age, such as body growth development, psychology, skills, and other topics that are currently under development. The second set of studies (n = 107/233) focused on different aspects of health in early childhood. Topics discussed in this regard included family health, medical procedures, interventions, and risks. Lastly, the third set of studies (n = 23/233) did not fit into either of the two categories mentioned above.\n\nOverall, early childhood is a sensitive period in a child's life, and researchers have studied it using various means of data analysis and types of data. Although research areas on early childhood are diverse, they are equally important. This paper underscores the current state and opportunities for research in this area and encourages additional efforts towards understanding this field of research.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mobile edge computing serves two fundamental purposes: enhancing performance and reducing costs. By outsourcing computation tasks to a mobile edge cloud (MEC), mobile devices can potentially reduce their average response time and power consumption. However, optimizing both performance and cost can be a conflicting task. Therefore, this paper proposes optimizing the cost-performance ratio (CPR), which combines the average response time and power consumption into one quantity. \n\nMobile edge computing has a unique feature wherein mobile users are competitive and selfish in competing for resources within the MEC. Thus, this paper takes a game-theoretic approach to study the stabilization of a competitive mobile edge computing environment. The main contributions of the paper are:\n\n1) Modeling a mobile edge computing environment with multiple user equipment (UE) and a single MEC. Each UE is entirely heterogeneous in terms of task characteristics, speed, and power consumption models for both computation and communication.\n\n2) Analytically deriving the average response time and power consumption of each UE and the MEC to mathematically and rigorously study cost-performance ratio optimization.\n\n3) Establishing a framework of seven non-cooperative games between the UEs and the MEC to study the stabilization of the competitive mobile edge computing environment. In each game, each player aims to minimize its payoff function (CPR) with different variables to play.\n\n4) Developing efficient algorithms for each player to find their best response in each game. All algorithms are poly-log time in the initial search interval and the accuracy requirement. An iterative algorithm is also developed to find the Nash equilibrium of the games.\n\n5) Demonstrating numerical examples of the algorithms for both the idle-speed and constant-speed models. \n\nOverall, this paper offers an effective approach for cost-performance ratio optimization in mobile edge computing, while considering the competitiveness of mobile users.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Beamforming techniques are essential to multi-antenna communication systems, and this study focuses on minimizing downlink power while meeting quality of service requirements. Traditional iterative algorithms can achieve optimal solutions, but the computational delay is high. To expedite beamforming, deep learning techniques can be used. In this research, we suggest a beamforming neural network (BNN) that depends on convolutional neural networks and expert knowledge to solve the power minimization problem. The BNN predicts key features utilizing the complex channel as input but does not directly estimate the beamforming matrix. The beamforming matrix is then restored from the predictions based on the uplink-downlink duality. The supervised learning method is employed by the BNN using a loss function based on the mean-squared error metric for network parameter updates. Simulation results indicate the BNN performs well with a low computational delay.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Product Quality Accident (PQA) is an accident that occurs due to product quality defects during design and production. Quality and reliability engineers aim to reduce the frequency and severity of these accidents during product usage. However, with the rising complexities of product functional structure and big data, it has become challenging to identify the root cause of PQA through traditional methods. \n\nTo address this issue, this paper proposes a novel approach utilizing advanced data mining algorithms for root cause identification. Firstly, the paper presents a detailed mechanism for the formation of quality accidents. Secondly, with the assistance of domain mapping theory, a PQA relevance tree is created. \n\nThirdly, the FP-Growth algorithm is employed with reasonable support and confidence levels to further mine the relevance rules and construct a comprehensive PQA relevance tree. The PQA relevance tree is designed to reduce the ambiguity of root cause identification, especially in instances where it was previously challenging.\n\nFinally, the effectiveness of this approach is demonstrated with a root cause analysis example of an engine quality accident. The proposed technique proved successful at identifying the root cause of the PQA, highlighting the importance of utilizing the latest data mining algorithms to improve product quality and reliability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: CoAP (Constrained Application Protocol) allows embedded devices to provide RESTful Web Services and exchange small binary message headers. One possible use case for CoAP is streaming data, in addition to transmitting sensor data and control information between smart home devices. Because CoAP can run on heterogeneous devices and operating systems due to the Java Virtual Machine and Java Runtime Environment, there are CoAP implementations available in Java. However, live streaming applications demand low latency communication, so we evaluated two CoAP Java implementations for timing fluctuations in packet processing compared to a CoAP C implementation. We concluded that the Java Garbage Collector causes significant fluctuations in packet processing, making it unsuitable for live streaming scenarios. Therefore, we recommend using native code applications instead for better timing results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Using the finite-difference time-domain (FDTD) method in two-dimensional (2-D) cylindrical coordinates, lightning electromagnetic pulse (LEMP) waveforms associated with lightning strikes to the Tokyo Skytree measuring 634 meters tall have been calculated. To represent the three-dimensional (3-D) tortuous lightning channel and the tall strike object, an \"equivalent vertical\" phased-current-source array was utilized. The current propagation speed along the equivalent vertical channel was determined in two steps: first, the channel was projected onto a plane that included the strike object and a field observation point, and then it was converted into the straight vertical channel right above the strike object. The proposed method allows the use of the 2-D FDTD method, thereby significantly reducing computational resources without compromising accuracy. LEMP waveforms measured at 27, 57, and 101 km (in various directions) from the strike object were accurately replicated by the proposed method. These findings highlight that the lower region of the tortuous lightning channel plays a crucial role in the wavefront of LEMPs at a distance of 20 to 100 km from the point of impact.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In IaaS clouds, users access virtual machines through a management server. However, in semi-trusted clouds, cloud operators may not be trusted and can execute arbitrary management commands, leading to VM redirection attacks. This is because the user-VM binding is weak, making it difficult to enforce the execution of only users' management commands. To address this issue, this paper proposes UVBond, which strongly binds users to their VMs. UVBond decrypts the user's VM disk within a trusted hypervisor and issues a VM descriptor to securely identify the VM. To bridge the semantic gap between high-level management commands and low-level hypercalls, UVBond uses hypercall automata. The implementation of UVBond on Xen confirms that it prevents attacks and has minimal overhead.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Long-term tracking of aerial infrared targets is an area of great interest in the field of automatic target recognition and tracking. Though deep trackers and correlation filtering trackers offer good results, challenges such as deformation, abrupt motion, heavy occlusion, and out of view still persist. Additionally, infrared images have inherent drawbacks such as low resolution, low contrast, and lack of textures. In this paper, we combine correlation filtering trackers and deep learning detection to achieve accurate tracking results. Our tracking system consists of the DTB-CF tracker, the UTA-CF tracker, and the YOLOv3 re-detector. We introduce a new criterion, the ratio between average peak-to-correlation energy (APCE), to update the UTA-CF tracker and maintain target model stability. We use a combination of the nearest neighbor maximum value method and APCE to initialize the YOLOv3 re-detector. Our algorithm outperforms state-of-the-art methods in terms of accuracy and robustness for long-term tracking of aerial infrared objects, as demonstrated through precision plot, success plot, and speed evaluations on real thermal image sequences.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Head detection is a crucial task in identifying individuals from visual data. However, due to the difficulty of building local and global information under unconstrained pose and orientation conditions, existing methods have had limitations in effectively detecting heads. To address these issues, this study proposes an adaptive relational network capable of capturing context information. The fundamental contextual features, such as global shape priors and local adjacent relationships, can be quantified by visual operators. The authors propose a two-step search algorithm to quantify intergroup conflict, while also introducing a structured feature module to capture local intraindividual stability. Finally, the global priors and local relation are integrated into a single-stage head detector. The study shows that the proposed method achieves state-of-the-art results on two challenging datasets, HollywoodHeads and Brainwash. An extensive ablation analysis further confirms the efficacy of the approach.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents the CFC-ITS framework, a context-aware fog computing scheme designed specifically for intelligent transportation systems. The proposed framework incorporates a multi-tiered approach, consisting of internet of things tier, fog service tier, and global cloud service tier, to support edge analytics for ITS services in a connected car environment. By taking into account various contextual factors such as location, time, available resources, and estimated response time, the CFC-ITS framework enables efficient task processing while optimizing virtualized resources. To validate the effectiveness of the proposed approach, a preliminary prototype and testbed were developed and found to be robust.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The hierarchical routing protocol is a commonly used method in wireless sensor networks (WSNs), thanks to its stability and efficient communication. However, the first protocol of this kind, Low-Energy Adaptive Clustering Hierarchy (LEACH), was not very energy-efficient, as it did not take the nodes\u2019 state into account. In this paper, we propose an energy-efficient LEACH-based protocol, which addresses this issue. We use the past cluster heads\u2019 state, as well as the residual energy and density of alive nodes, as indicators to select the current round\u2019s cluster heads. This modification fixes the problem of a sharp drop in the number of cluster heads as the number of dead nodes increases. Our simulation results demonstrate the effectiveness of the proposed protocol in prolonging the WSN\u2019s network lifetime and achieving a balanced energy distribution, thereby reducing energy consumption.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This study focuses on the estimation of seismic events using the Bayesian filtering method known as multiple model particle filtering (MMPF). Seismic signals exhibit nonlinearity, and the nonlinear dynamical system can be utilized for modelling the signal. Accurately estimating events from these recorded seismic signals is critical in geological studies, particularly in noisy environments. This difficulty arises as the signal is often acquired under noisy conditions, making it challenging to extract useful information. This paper proposes an effective filter, the MMPF, which uses a sequential Monte Carlo method, in conjunction with the seismic wavelet model for the estimation of seismic events under noisy environments. A set of synthetic noisy signals with varying noise levels and numbers of events were used in the simulations to evaluate the performance of the method. Results demonstrate that the proposed MMPF provides excellent estimates for seismic events under noisy environments.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces an effective approach to verify and fine-tune performance of model-based controllers in nonlinear systems, which are subject to probabilistic uncertainties with arbitrary distributions. The proposed method, called arbitrary polynomial chaos (aPC), is an extension of the generalized polynomial chaos. The aPC method is an asymptotically convergent spectral method that can handle arbitrary uncertainty distributions directly. Unlike other methods, the aPC only requires knowledge of the statistical moments of the distribution, and it can estimate the aPC expansion coefficients using a minimal number of closed-loop simulations. To demonstrate its superiority, the aPC method is applied to a benchmark continuous reactor problem that is controlled by a scenario-based nonlinear model predictive controller. Through this case, the advantages of the proposed aPC method are successfully showcased.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The increasing use of technology to aid learning in various knowledge areas has led to the development of technology-assisted media for language learning and teaching. However, electronic learning (e-learning) solutions have not fully addressed the specific needs of grammar learning, which is a challenging aspect of language education. Inadequate instructional design to enhance learning performance in current systems can lead to low motivation and engagement due to an excessive cognitive load. To address these deficiencies, this paper proposes a smart communication network that manages cognitive load in the context of grammar learning. The e-grammar learning networks function as a collaborative learning platform that combines a pedagogically informed instructional model and cyber interaction among teaching/learning agents. Numerical simulations have shown the desirable performance indicators of the proposed networks to facilitate information exchange and learning. Empirical studies have also demonstrated that the overall smart network-enabled e-grammar learning system has desirable characteristics to motivate learners and manage their cognitive load, suggesting the promising capability of the proposed system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This study illustrates an ongoing endeavor to incorporate machine learning models in classifying alerts in Security Operations Centers (SOC). The aim is to alleviate the workload of security analysts by automizing the decision-making process for investigating alerts with trustworthy models. Doing so, analysts can concentrate on handling more complex security cases. A framework is designed to provide analysts with both alert predictions and prediction explanations, enhancing their workflow in detecting and scrutinizing security alerts. Additionally, the system produces model analytics that aid management and stakeholders in assessing the model, and determining the reliability of the model's final decisions. Our prediction explanation visualization delivers a more efficient and insightful way for analysts to classify forthcoming alerts while also providing valuable insight into the prediction generation process of machine learning models. Furthermore, a model performance analysis dashboard enables decision-makers to perform signature level analysis of the model for better insights.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The power charge limit is an essential part of efficient energy management as it influences the shape of the energy expenditure curve. To mitigate this issue, we propose a three-step approach consisting of a Gray Correlation Analysis (GCA), a combination of Kernel function and Principle Component Analysis (KPCA), and a Differential Evolution (DE) algorithm to create a Support Vector Machine (SVM) classifier. Although these methods have shown excellent performance, they are resource-intensive and require a large-scale data processing framework. To address this problem, we propose a Big Data green framework for remote sensing satellite data to improve efficiency and accuracy.\n\nThe proposed framework consists of three units: a Remote Sensing Big Data Acquisition Unit (RSDU), a Data Processing Unit (DPU), and a Data Analysis Decision Unit (DADU). The RSDU gathers sensor measurements from the satellite and sends them to the Base Station, where initial processing occurs. The DPU performs data cleaning, filtering, and parallel computing to refine the raw data. Finally, the DADU collects the results, analyzes them and generates insightful analysis graphics.\n\nOur proposed framework features allocation, load-balancing, and parallel processing to effectively handle the vast amounts of data associated with remote sensing energy measurements. Accordingly, it can provide efficient and accurate energy consumption forecasts and improve energy management.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: HTTP Adaptive Streaming (HAS) is a cutting-edge technology that enhances the quality of the user experience in situations where the available bandwidth fluctuates. As 4G cellular networks continue to grow in prominence, developing an effective bitrate adaptation algorithm becomes more challenging due to the changes in radio channel quality. The variation in bandwidth caused by these changes is significantly different from the pattern caused by changes to radio channel resources. \n\nTo address these challenges, we propose a bitrate adaptation algorithm for 4G cellular networks that utilizes bandwidth variation pattern differentiation. By analyzing bandwidth data profiles, we are able to distinguish between sustained fluctuations and instantaneous hopping patterns. This allows the proposed algorithm to perform a smoothed bitrate adaptation for sustained fluctuations and an instant bitrate adaptation for instantaneous hopping. \n\nResults from tests conducted on a 4G cellular network indicate that the algorithm is effective in reducing the frequency of bitrate switching and playback stalls, while increasing the average bitrate. Overall, this approach represents a significant step forward in overcoming the challenges associated with adapting to varying bandwidth conditions in 4G cellular networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The difference between multi-label classification and traditional classification lies in the fact that a single instance can be associated with multiple labels simultaneously. In recent studies, researchers have suggested that information from the nearest neighbors of a query instance can be used to predict its labels effectively. Building upon this research, we propose a novel approach to multi-label classification that uses a neural tensor network (NTN) to examine the relationships between neighboring labels and classify the query instance based on these correlations. Using this method, we can leverage the correlations and interdependencies between labels to optimize the performance of the data. The results of our experiments on real data suggest that our approach achieves good results in multi-label classification tasks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: State-of-health (SOH) estimation is a critical aspect of battery management systems. To enhance the accuracy of SOH predictions, machine learning techniques have become increasingly popular. However, traditional machine learning methods, which rely on a single learner, are often limited by small regional data sets, which can impact the generalization ability of the model. To address this issue, the concept of ensemble learning has been introduced. By generating differential data samples and synthesizing the output of multiple base learners, ensemble learning can significantly improve learning performance. \n\nAdditionally, this paper proposes the use of gray relational analysis for feature correlation analysis. The proposed method was validated using NASA battery data sets, which demonstrated improved accuracy of results compared to traditional machine learning methods. In conclusion, this study highlights the potential for ensemble learning to provide highly accurate and stable SOH predictions, and underscores the importance of feature correlation analysis for enhancing the performance of machine learning algorithms in battery management systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The drive test has traditionally been used to assess the efficacy of automotive antennas in urban and rural environments. However, with advancements in electromagnetic and wave-propagation simulation software, virtual drive testing can now be used instead to evaluate antenna performance. This study proposes a simulation-based method to evaluate the LTE automotive antenna on a vehicle rooftop in an urban environment, with and without surrounding traffic. The analysis is performed using the Multi-Level Fast Multipole Method (MLFMM), and the Ray-optical Intelligent ray tracing model (IRT) is used to assess the antenna's received power in a non-stationary environment. The results of the study show minimal discrepancies in the received power of the vehicle antenna with and without traffic in a complex urban environment.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Passwords have become an integral part of our daily routine, and despite the availability of alternative authentication mechanisms like biometrics, they persist stubbornly. Over the last few decades, passwords have come under increasing scrutiny due to the various problems associated with users trying to create strong and secure passwords. Managing passwords can be a cognitive burden, and this often leads to poor password practices, including the reuse of passwords across various accounts.\n\nAlthough there is no anticipated end to the use of passwords, scholars have identified the need for better support to make them more secure. One such method is the use of a password manager. However, little research has been conducted to explore why people would choose to adopt password managers.\n\nTo address this gap in research, an adapted version of the Unified Theory of Acceptance and Use of Technology (UTAUT2) was used, which included trust as an additional construct. Empirical data was collected to explore the factors that drove the intention to adopt a password manager.\n\nThe results of the study indicated that performance expectancy, habit, and trust were key factors in the intention to adopt a password manager. The findings suggest that people are more likely to adopt a password manager if they perceive that it will enhance their performance in managing their passwords, have a habit of using them, and trust the technology and the provider.\n\nIn conclusion, while passwords will continue to be an important part of our daily routine, there is a need for better support to make them more secure. The study highlights the importance of understanding the factors that drive the adoption of password managers, and the role that trust plays in this process. By addressing these factors, we can make passwords more secure and easier to manage.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present an algorithm for detecting and classifying falls using a MEMS accelerometer. Our algorithm is based on a shallow Neural Network with three hidden layers, which is trained to discriminate between falling and non-falling activities using both daily life and synthetic fall features. Unlike previous approaches, our algorithm generates synthetic falls as multivariate random Gaussian features, enabling collection of real daily life features during normal living. Furthermore, the features related to synthetic fall events are generated as complements to normal features, resulting in a more comprehensive dataset. To train the network, we first cluster daily life features by Principal Component Analysis and exclude any fall activities. We then generate a complement set of normal features, which serves as a mask for Monte Carlo generation of synthetic falls. The combination of these two sets ensures accurate training with a high Recall-Precision rate. Overall, our approach provides an effective and practical solution for Neural Network-based fall detection.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Registration is a vital task for automated medical image analysis. Even though deep learning (DL) techniques have proven to be more efficient than traditional approaches, they heavily rely on training data and may not generalize well to new image types. In this study, we introduce a DL-based method capable of registering an image pair that differs from the training images. To accomplish this, we combine segmentation information with transfer learning and train generative adversarial networks (GANs). Our findings, based on experiments conducted on chest Xray and brain MR images, suggest that our method outperforms conventional ones and can achieve better registration results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Person Re-Identification (ReID) is a challenging task due to the large pose variations and misalignment errors in person images. Many existing methods resort to pose estimation and part segmentation to improve the robustness of pedestrian representations, which incur high computational costs and model complexity. To address these issues, we propose a Part-Guided Representation (PGR) composed of two components: Pose Invariant Feature (PIF) and Local Descriptive Feature (LDF). PGR is supervised by local part cues, and PIF learns a pose invariant representation inferred by pose estimation and normalization, while LDF focuses on discriminating body parts learned with body region segmentation. Our approach only requires extra pose extraction during the training phase for supervision but not during testing for feature extraction, making it more efficient. Extensive comparisons on five widely used datasets demonstrate the competitive accuracy and efficiency of PGR compared to recent works.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Remote sensing plays a vital role in cyber-physical and networked systems, where measurements taken at remote locations can serve as monitoring tools and inform control decisions. However, with the rising number of cyberattacks against such systems, it has become crucial to ensure secure state estimation.\n\nIn this paper, we propose a solution that utilizes physical measurements and a consensus algorithm to achieve secure state estimation for a class of tree-like networked systems. By combining physical measurements with the consensus algorithm, we can ensure data integrity and protect against cyberattacks.\n\nThe proposed solution has significant socio-economic benefits as it can help prevent cyberattacks on critical infrastructures such as power grids, water systems, and transportation networks. By ensuring the security of these systems, we can prevent widespread disruption and economic loss.\n\nIn conclusion, the integration of physical measurements and consensus algorithms can play a critical role in ensuring the security of remote sensing systems. This solution has the potential to transform the way we approach cybersecurity for critical infrastructures, ultimately leading to a more secure and resilient society.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Determining the appropriate timing for buying or selling stocks can be a difficult task due to the variety of financial markers available. To handle this problem, traders commonly use trading strategies based on technical or fundamental indicators. However, optimizing a group trading strategy portfolio has proven to be a challenge. Investors also rely on stop-loss and take-profit points to mitigate unpredictable losses, but it can be difficult to determine the best points when using multiple trading strategies.\n\nTo address these issues, we propose an algorithm for obtaining a group trading strategy portfolio and its stop-loss and take-profit points using the grouping genetic algorithm. Our approach aims to provide a more effective group trading strategy portfolio.\n\nTo validate the effectiveness of our proposed approach, we conducted experiments on a real dataset. The results demonstrate the effectiveness of our algorithm in generating an optimized portfolio with appropriate stop-loss and take-profit points.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Telecommunication refers to any form of communication that occurs remotely without the need for face-to-face interaction. This type of communication is particularly important for individuals with communication disabilities, as it allows them greater independence, confidence, and engagement with their community. Distance communication can take various forms, such as Telephone Relay Services, Video Relay Services, Free phones, or AAC speech generating devices, among others.\n\nIn Thailand, the Thai Minspeak\u00ae system with a 144-location overlay was initially created and installed on a Windows 8 Tablet Samsung to assist individuals with communication disabilities to communicate with those around them. However, due to the limitations of the tablet, an app was developed and installed on a smartphone. The tablet and smartphone are connected using an aux cable, allowing for seamless calling and receiving functions. The entire system, which includes the Thai Minspeak\u00ae system on the tablet, the app on the smartphone, and the aux cable, provides a means for individuals with communication disabilities in Thailand to communicate with each other or with people who are not in their immediate vicinity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We have developed an approach to addressing the pickup and delivery problem by utilizing alternative modeling techniques to improve service quality and efficiently handle transportation requests. Our objective is to manage scheduling and allocation of tasks by implementing a software simulation that consists of a system of cooperative and autonomous agents. This approach was derived from a practical scenario on our campus where golf carts are utilized to provide transport services. Due to the inherent complexity of the pickup and delivery problem, our primary strategy is to design a decentralized architecture that distributes decision-making across various components of the system. We have developed a set of heuristics to find optimal solutions, without the need for exhaustive search. The decentralized model is implemented as a set of cooperating autonomous components, each empowered with computing ability and decision-making autonomy. These components contribute collaboratively to derive optimal solutions. In order to generate descriptive data that capture pickup and delivery scenarios, we utilized game simulation. We analyzed diverse experimental data to explore various scenarios and perform effective resource planning and allocation. The simulation system is available on various platforms such as PCs, laptops, and smart devices.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cost-effective management of spare parts is a crucial goal for all manufacturing and service companies. However, achieving this objective poses a significant challenge, as accurate demand forecasting and optimized supply planning decisions are necessary for achieving the best availability level of spare parts. This paper proposes a predictive approach for identifying the best forecasting method with the least error cost. \n\nFurthermore, uncertainty in demand in the business aircraft industry means that the best forecasting method for a part may change. Therefore, a methodology for selecting the best forecasting method based on binary classifier machine learning has been developed. The proposed methodology has been applied in a real case for a well-known business aircraft. \n\nThe results indicate that the neural network is the best machine learning method for 98% of the demand, while random forest is suitable for only 2% of parts. By utilizing the proposed methodology, companies can improve their spare parts management and, consequently, reduce their costs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Integrators play a crucial role in GitHub by evaluating code changes and closing pull requests. However, some pull requests may need to be reopened for further modification and review. This is why it is important to predict reopened pull requests immediately after their first close and assist integrators in reopening them as soon as possible. Delayed reopening of pull requests can cause conflicts with newly submitted pull requests, impose additional software maintenance costs and increase the workload for developers.\n\nTo address this issue, we present DTPre, an automatic predictor of reopened pull requests based on a decision tree classifier. DTPre analyzes code features of modified changes, review features during evaluation, and developer features of contributors. To evaluate the effectiveness of DTPre, we used it on seven open-source projects that contained 100,622 pull requests. The results showed that DTPre had high performance, achieving a precision of 95.53%, recall of 99.01%, and F1-measure of 97.23% on average.\n\nIn comparison to predictors based on neural networks, na\u00efve Bayes, logistic regression and SVM, DTPre based on decision tree significantly improved F-1 measures by 41.76%, 59.45%, 42.25%, and 9.98% on average across the seven projects. With the ability to predict reopened pull requests accurately, DTPre can significantly reduce the workload of developers and improve software maintenance efficiency on GitHub.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Identifying potential human rights abuses through images is a complex task that requires sophisticated computer vision techniques. However, the lack of available datasets containing diverse types of human rights violations has made this task challenging. To address this gap, we introduce the human rights archive (HRA) database, which contains 3050 verified photographs of human rights violations labeled with semantic categories.\n\nUsing the HRA dataset, we fine-tuned deep convolutional neural networks (CNNs) to classify different types of human rights abuses. Our experiments showed that by combining object-centric and scene-centric CNN features, we were able to accurately recognize various human rights violations. Our results suggest that the HRA database poses a challenge for traditional representation learning methods and provides a benchmark for recognizing human rights abuses in visual context.\n\nOverall, we believe that our dataset and transfer learning approach can help expose and address human rights violations on a larger scale. By providing a rich source of information for computer vision algorithms, we hope to create systems that can effectively identify and prevent human rights abuses.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel framework for diagnosing pulmonary nodules through radiomic features extracted from a single CT scan. The framework combines appearance and shape features to achieve an accurate diagnosis for extracted lung nodules. Appearance features are modeled using 3D Histogram of Oriented Gradient (HOG) and higher-order Markov Gibbs random field (MGRF) models to depict spatial non-uniformity in the texture of the nodule, while shape features are modeled using Spherical Harmonic expansion and geometric features to fully describe the nodules' shape complexity. These features are fused and fed to a stacked autoencoder to differentiate between malignant and benign nodules. The framework was evaluated using 727 nodules from the Lung Image Database Consortium (LIDC) dataset, achieving a classification accuracy, sensitivity, and specificity of 93.12%, 92.47%, and 93.60%, respectively.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Social media has become a popular tool for enhancing the development of tourism due to the vast amounts of information available to organizers of tourist spots and events. This study focuses specifically on sightseeing tweets, which are tweets posted by tourists containing descriptions related to tourist spots and events. To extract useful opinions from these tweets, they are classified into defined categories. These classifications are then used to analyze opinions and polarities related to tourist spots and events. Furthermore, a new model utilizing convolutional neural networks and multi-channel distributed representation is proposed to classify tweets. This hybrid representation of word representation for text data improves the accuracy of the model. To validate the proposed model, experiments were conducted using actual sightseeing tweets. The experimental results indicate that the proposed model outperforms other deep-learning-based models.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a maneuver strategy for determining the optimal trajectory for the dive phase of a hypersonic missile. The combat scenario where the missile attacks a fixed target on the ground is considered. The unique maneuvering technique of inverted flight is applied to the hypersonic missiles, and an optimal trajectory is designed by minimizing the attack time with a predetermined terminal flight path angle. The constraints of the angle of attack, dynamic pressure, heating transfer rate, and normal overload are also taken into account.\n\nTo solve the trajectory optimization problem, we have presented an improved hp-adaptive pseudospectral method with mesh size reduction. The simulation results demonstrate that our proposed algorithm can significantly reduce the mesh scale while maintaining accuracy. Additionally, the results show that the trajectory obtained by the proposed algorithm is in accordance with the actual flight law.\n\nFurthermore, we have conducted a contrast simulation which demonstrates that inverted flight has better trajectory performance than normal flight. Overall, our proposed maneuver strategy for hypersonic missiles with inverted flight has proven to be effective when attacking fixed targets on the ground.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel channel estimation algorithm for underwater acoustical (UWA) communications systems based on orthogonal frequency division multiplexing (OFDM) in the presence of Rician fading. The proposed algorithm exploits channel sparsity in the time domain, where the channel is modeled using a limited number of paths characterized by delay, Doppler scale, and attenuation factor. The algorithm estimates the sparse channel tap delays and Doppler shifts using the orthogonal matching pursuit (OMP) algorithm. A computationally efficient and novel channel estimation algorithm is then developed by combining OMP and maximum a posteriori probability (MAP) techniques for estimating the sparse complex channel path gains. The prior densities of the channel path gains have complex Gaussian distributions with unknown mean and variance vectors. A computationally efficient maximum likelihood algorithm is proposed for their estimation. Monte Carlo simulation results show that the OMP-MAP algorithm outperforms conventional OMP-based channel estimation algorithms in terms of mean square error and symbol error rate performances for uncoded OFDM-based UWA communications systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The rapid growth in wireless sensor network (WSN) adoption can be attributed to several advantages over wired systems, including cost-effectiveness, ease of installation, scalability, flexibility, and self-organization. However, WSN nodes rely on a limited energy source, and thus efficient communication is necessary to extend the WSN's lifetime. Commonly, the nodes alternate between active and sleep states or regulate their transmission power to conserve energy. In this paper, we propose utilizing two fuzzy logic controllers to dynamically adjust both the sleeping time and transmission power of nodes to optimize energy consumption. Our experimental results suggest that, depending on the Medium Access Control (MAC) protocol adopted, network lifetime can be improved by as much as 30 to 40%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, our focus is on cooperative spectrum sensing (CSS) in cognitive radio networks (CRN) where multiple secondary users (SUs) collaborate to detect the primary user, which may utilize multiple bands. To address the issue, we introduce the concept of deep cooperative sensing (DCS) - the first CSS framework based on a convolutional neural network (CNN). DCS is designed to autonomously learn the strategy for combining individual sensing results of SUs through training sensing samples, regardless of sensor quantization. \n\nDCS leverages both spectral and spatial correlations of individual sensing outcomes to enable an environment-specific CSS. As demonstrated through simulations, the performance of CSS can be significantly improved using DCS.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Direct policy search is a promising approach in reinforcement learning, especially when it comes to controlling continuous and high-dimensional systems. Among various direct policy search methods, reward weighted regression (RWR) was introduced by Peters et al. However, the RWR algorithm is susceptible to overfitting due to its reliance on the EM algorithm for estimating the policy parameters. This paper focuses on variational Bayesian inference as a means to address the overfitting issue, and proposes the direct policy search reinforcement learning based on variational Bayesian inference (VBRL). \n\nThe proposed VBRL algorithm was evaluated in multiple experiments involving mountain car and ball batting tasks. These experiments showcase that VBRL outperforms RWR by producing higher average returns. In conclusion, VBRL, which uses variational Bayesian inference, proves successful in mitigating overfitting issues in direct policy search reinforcement learning.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, the authors present a controller designed for a robot-trailer system, aimed at solving the path following problem. They derive a kinematic state-space model that utilizes the tractor steering angle rate as system input to minimize input disturbances. However, biases in the measurement of heading angles can result in tracking errors in the trailer position. To address this, they propose an improved integral separation approach combined with linear quadratic regulator (LQR) to remove the trailer position error. The tractor relative location is used as an indicator of steady-state. To enhance the performance of the controller, the authors employ a genetic algorithm (GA) to optimize the LQR controller parameters. Simulation results validate the proposed approaches.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Millimeter wave (MmWave) technology is rapidly becoming a highly promising option as it provides ultra-high data rate links and low latency time, making it ideal for supporting internet of things, tactile internet, and cloud computing applications. The MmWave standard, IEEE 802.11 ad, proposes using scheduled medium access control (MAC) based on time division multiple access (TDMA) to share radio resources among active users. The highly dynamic time-variant channel with high blocking probability in MmWave highly degrades the entire system performance, making it inefficient to use simple channel independent scheduling schemes like round robin (RR). Therefore, efficient radio resource scheduling schemes that consider changes in MmWave channels are required to maximize the benefits from available resources. \n\nThis paper evaluates the performance of a channel-sensitive scheduling scheme, proportional fairness scheduling (PFS), in downlink MmWave networks by conducting numerical simulations under different channel conditions and various MmWave beamwidths. The simulations aim to determine the total system data rate and the fairness between users. The results of the evaluation demonstrate that PFS outperforms the RR scheme under both static and dynamic channel conditions in terms of total system data rate and fairness. The PFS scheme provides a better trade-off between system throughput and fairness among users as it intelligently allocates radio resources based on the individual channel conditions of each user. \n\nOverall, this study confirms that effective radio resource scheduling is crucial for achieving maximum benefit from MmWave technology in 5G networks. By implementing channel-sensitive scheduling schemes, like PFS, network operators can efficiently allocate radio resources among active MmWave users to deliver ultra-high data rate links with low latency time, and support emerging applications that require robust and efficient network connectivity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The concept of utilizing a heterogeneous network is set to revolutionize the fifth generation mobile communication systems. The aim of this implementation is to enhance both network throughput and user throughput. Although many techniques have been investigated to achieve these goals, they are easily degraded by various factors. To combat this, the use of side information such as the terminal location and Doppler frequency could assist in avoiding performance degradation. By knowing the Doppler frequency before signal transmission, the reduction in throughput can be alleviated by selecting modulation schemes with lower cardinality.\n\nThis paper proposes a technique for Doppler frequency estimation using overlap frequency domain equalization and an optimum FDE weight derived for the proposed estimation technique. The proposed estimation technique with the optimum FDE weight achieves better estimation performance than the conventional MMSE weight.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Effective methods for solving optimal control problems in bilinear ensemble systems have been of great importance and interest in mathematical and computational optimal control. However, despite the significant progress that has been made in this area, there remains a need for more advanced and efficient methods.\n\nIn this study, we have extended an iterative method that was previously developed for solving fixed-endpoint optimal control problems, to address the issue of finding optimal control conditions for free-endpoint systems. Our approach involves solving the optimal control problem for a linear ensemble system at each iteration, rather than relying solely on numerical optimization. We have explored the convergence of this method and have examined stochastic ensemble systems that are driven by Gaussian noise.\n\nOur research shows that this iterative method is robust and highly applicable for solving optimal control problems in bilinear ensemble systems. Numerical examples demonstrate the efficiency and accuracy of this approach, making it a highly valuable tool for those working in mathematical and computational optimal control.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The analysis of genes expression data in large matrices poses challenges in terms of applying analytical methods, interpretation, and drawing conclusions. The goal of this study was to assess the accuracy of genes expression classification on different maize lines and organs (Apex, Root, Shoot, Ear, and Tassel) based on Maize Nested Association Mapping (NAM) datasets using the K-Nearest Neighbor (KNN) approach. The KNN analysis with k = 5 resulted in an accuracy value and AUC value of 0.926 and 0.992, respectively. Additionally, we demonstrated the classification of genes expression datasets for organ-specific expression differentiation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The prevalence of small, low-cost, wireless cameras has made it increasingly difficult to detect surreptitious observations of people. Previous methods for detecting hidden cameras have only been effective in limited environments where the user has significant control of the surroundings. To address this issue in a less constrained scope of settings, we propose the concept of similarity of simultaneous observation. This involves using a camera (such as a Wi-Fi camera, mobile phone camera, or laptop camera) to compare timing patterns of data transmitted by potentially hidden cameras with the timing patterns expected from the known camera's recorded scene.\n\nWe applied several similarity measures to analyze these patterns and demonstrated an accuracy of over 87% and an F1 score of 0.88 using an efficient threshold-based classification. To further improve our results, we trained a neural network with our dataset, achieving accuracy as high as 97% and an F1 score over 0.95 for both indoor and outdoor settings.\n\nOur study shows that similarity of simultaneous observation is a feasible method for detecting hidden wireless cameras that are streaming video of a user. This approach removes significant limitations that previous detection methods have faced.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mangosteen is an export commodity with great potential in Thailand, and it is known as the queen of fruits due to its popularity. While it generates substantial revenue, it is not without defects, both internal and external, which can result in rejected shipments and a decrease in export reliability. To tackle this problem, this research aims to develop a method to detect and classify surface roughness of mangosteen. The proposed approach is based on texture image analysis, which employs Gray-Level Co-occurrence Matrix (GLCM) to extract textural features for roughness classification into three categories: Glossy Surface, Mid Rough Surface, and Extreme Rough Surface. This study represents a novel attempt to use GLCM for mangosteen surface roughness classification.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Health Indicators (HIs) are utilized to identify and quantify historical and ongoing degradation processes by extracting feature-based information from collected data. However, existing HI construction methods have several limitations including the use of a fixed learning rate for training deep learning networks, absence of phase degradation consideration, and difficulty in determining the failure threshold for remaining useful life (RUL) prediction. To overcome these limitations, we propose a HI construction method for rolling bearings based on a deep convolutional neural network with polynomial decaying learning rate (PDCNN) that considers phase degradation. Our approach involves inputting the original vibration signal into the PDCNN, extracting features from the deep convolutional neural network, and then utilizing them in the deep neural network (DNN) to construct HI with a training label that takes into account phase degradation. The polynomial decaying learning rate optimizes the training process of the neural network, leading to improved efficiency and time savings. Experimental results demonstrate that the HI constructed by the proposed method is superior in monotonicity and trendability to other compared methods. Additionally, the constructed HI output values within the [0, 1] interval effectively address the difficulty of determining the failure threshold in RUL prediction.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper discusses a solution to the issue of information flow across system boundaries in complex environments where software-intensive systems are used. The knowledge that we have about the world is limited and largely determined by sensor data and what we can deduce from it. As system designers, we can program some of this knowledge into our systems, but unforeseen problems can still arise. To mitigate these issues, we need to actively engage in creating knowledge and help our systems observe their environments, make critical inferences, and use resulting models to inform their decisions. We also expect our systems to conduct the same analyses on their internal structure and behavior to react more effectively to unforeseen failures and environmental surprises. This paper presents the Wrappings integration infrastructure as a solution to this problem by describing the architecture of self-modeling systems that use models of their own behavior to generate and manage that behavior.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With increasing demands for high data transmission rates, the future of network scenarios has seen significant changes. To address these needs and reduce transmission latency while improving the quality of service, ultra dense small cells have been identified as the most promising solution. By bringing base stations closer to mobile users, the average distance between them is minimized. However, the energy consumption overhead created by network densification necessitates meticulous use of dynamic switching techniques. Utilizing such techniques, underutilized base stations can be put into lower power sleep mode. Additionally, new user association schemes need to be developed to coexist with novel techniques and further improve energy efficiency. In this paper, we propose an investigation of dynamic switching and user association with the aim of reducing energy consumption and achieving higher energy efficiency throughout the system. Our proposed solution employs a message passing-based distributed algorithm. Experimental results indicate that our proposed algorithm can reduce energy consumption by up to 84.7% compared to the greedy algorithm. Furthermore, our approach can improve energy efficiency by up to 2.4x compared to baseline measurements.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Discriminating food-contaminating microorganisms is a critical process in ensuring food and beverage safety. However, conventional methods such as morphological observation, genetic analysis, and biochemical fingerprinting using mass spectrometry have several drawbacks, including long assay time and expensive equipment. To overcome these challenges, a novel method called \"colony fingerprinting\" has been proposed using bioimage informatics. This method involves monitoring the growth of bacterial colonies using a lens-less imaging system, obtaining characteristic colony fingerprints over time and using these to extract discriminative parameters. With the aid of machine learning approaches such as support vector machine and random forest, 20 bacterial species were successfully discriminated using this method. Colony fingerprinting is a promising, rapid and easy method for discriminating food-contaminating microorganisms.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A genetic algorithm with a neural network bias has been utilized to create a periodic all-dielectric waveguide structure that focuses on specific band-pass frequencies. The early findings point towards the existence of an optimal design for periodic structures that enable target frequency transmission. This highlights the essential requirement for a reliable optimization scheme, such as the one described herein.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Although deep learning algorithms for diagnosing pathology have shown similar results to those of human experts across a variety of tasks, they require vast amounts of accurately annotated data for training. However, generating such data sets is time-consuming and not feasible for certain tasks, meaning that many medical data sets have limited images, making them insufficient for training. In this study, we demonstrate that few-shot learning techniques can effectively transfer knowledge from a well-defined source domain focused on Colon tissue to a more generic domain composed of Colon, Lung, and Breast tissue, using only a few training images. Our results demonstrate that our few-shot learning approach achieved a balanced accuracy (BAC) of 90% with just 60 training images, even for Lung and Breast tissues that were not included in the training set. This outperforms the fine-tuned transfer learning approach, which achieved a BAC of 73% with 60 images and required 600 images to achieve a BAC of 81%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We are considering a problem where communication and state estimation must be jointly solved on a Gaussian multiple access channel. The state process is assumed to be IID Gaussian and is known to both transmitters without causality. The receiver must decode messages from the transmitters and estimate the state process within a prescribed squared error distortion.\n\nOur objective is to determine the optimal sum-rate versus distortion performance, and we provide a comprehensive characterization of this performance.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Future cellular networks (5G) are expected to meet the increasing capacity and quality of experience requirements through extreme network densification and conglomeration of diverse technologies. However, managing such a complex network efficiently is a significant challenge for 5G. To address this challenge, Self-Organizing Networks (SONs), originally designed for legacy networks with a reactive approach, need to shift to a proactive paradigm. To achieve this radical transformation, historical network data should be harnessed to predict the future network state. Mobility prediction can play a vital role in enabling efficient resource management for Proactive SON. In this study, we compared the performance of four mobility predictors: Deep Neural Network (DNN), Extreme Gradient Boosting Trees (XGBoost), Semi-Markov, and Support Vector Machine (SVM) using a synthetic dataset of eighty-four mobile users generated through a realistic Self-similar Least Action Walk (SLAW) mobility model. The effectiveness of each model was evaluated based on its ability to predict the future location of mobile users and the time each algorithm took to be fully trained and perform the prediction. The XGBoost model outperformed all other predictors with an accuracy of 90%. The high prediction accuracy of XGBoost can enable energy-saving gain of above 80% when used to drive proactive energy-saving SON solutions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, the research on latent vector embedding has gained a lot of attention due to its exemplary representation capabilities to measure latent connections among different viewpoints. Nonetheless, most studies employ the inner product of latent vectors to represent these connections and develop embedding models based on this principle. In this study, we aim to delve deeper into the existing embedding models and identify that utilizing the inner product can lead to several problems. Firstly, in the latent space, the inner product of three vectors may violate the triangle principle. Secondly, the inner product cannot measure the relationships between vectors that belong to the same category, such as users and users or items and items. Lastly, the inner product cannot capture the collaborative relationships (user-user and item-item) for collaborative filtering. To overcome these issues, we propose a latent vector embedding model for collaborative filtering, referred to as latent dual metric embedding (LDME). This approach utilizes the dual-Euclidean distance in the latent space instead of the inner product to represent different types of relationships (user-user, item-item, and user-item) using a uniform framework. We design an embedding loss function in LDME that can measure the close and remote relationships between entities, solve the aforementioned problems, and achieve a more concise and straightforward embedding result. We conduct extensive experiments on numerous real-world datasets, including Amazon, Yelp, Taobao, and Jingdong, which demonstrate that LDME outperforms some of the state-of-the-art user-item embedding models and can benefit the existing collaborative filtering models.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The statistical analysis of examination results is crucial in the education field as it serves as a reflection of the quality of education. Through evaluation, the feedback obtained from the analysis can be an indication of the standard of examination papers or the effectiveness of teachers' methods of teaching. This information can be used to assess the standard of education, thus contributing to the development and enhancement of the teaching and learning process.\n\nIn 2011, Information and Communication Technology (ICT) was introduced as a core subject in the G.C.E Advanced Level examinations. As a result, research was conducted to identify issues related to the ICT subject and suggestions provided to improve the performance of students in the subject as well as the teaching process.\n\nThe Association Rule technique of data mining was employed to identify patterns in the examination results. The analysis revealed that the better the performance of students in mathematics and ICT subjects at the G.C.E. Ordinary level, the higher the probability of performing well in the G.C.E. Advanced level examination ICT subject. Another finding was that female students performed better in the exam than male students when all factors were considered. \n\nOverall, the statistical analysis of examination results plays a vital role in evaluating the quality of education. The findings obtained can lead to reforms and positive changes in the education system, improving the teaching and learning process, and enhancing the performance of students.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: NASA is currently focusing on developing space and ground architecture to ensure that science and exploration discoveries can be sent back to Earth. To optimize the data return from these missions, careful planning, design, and standards are necessary. Additionally, operations need to be coordinated from the mission's conception to its development. To further streamline these processes, NASA is exploring the potential of machine learning, particularly in the link-to-link aspect of communication systems.\n\nIn a recent experiment, NASA's Space Communication and Navigation Testbed onboard the International Space Station and the ground station located at NASA John H. Glenn Research Center used machine learning to identify the benefits and challenges of applying this technology in the actual flight environment. The primary goal of this experimental configuration was to use the machine learning decisions to configure a space link that could achieve multiple objectives related to data throughput, bandwidth, and power.\n\nThe algorithm formation used in the test was neural network-based reinforcement learning. The specific details of the algorithm formation and on-orbit testing were discussed. This test demonstrated the potential for machine learning to reduce the complexity of automated systems, optimize data return, and reduce the costs of operations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we investigate the distributed multi-agent optimization problem over a network in which each agent possesses a local cost function that is smooth and strongly convex. The aim is to determine a common solution that minimizes the average of all cost functions. We propose a distributed stochastic gradient tracking method, assuming that agents can only access unbiased estimates of the gradients of their local cost functions. We demonstrate that, in expectation, the iterates generated by each agent converge to a neighborhood of the optimal solution, where they accumulate at an exponential rate (under a constant step size). Furthermore, the expected error bounds on the distance of the iterates from the optimal solution decrease as the network size increases. This approach provides a performance similar to that of a centralized stochastic gradient algorithm. Additionally, numerical examples illustrate the efficacy of the method.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the explosive growth in multimedia applications, the use of heterogeneous cellular networks (HetNets) has become increasingly widespread in order to meet the mounting demands on communication capacity. One of the difficulties associated with supporting the massive amount of real-time traffic generated by always-on multimedia applications in HetNets is the resulting energy consumption. The tradeoff between energy consumption and service provider profit, while also ensuring satisfactory quality-of-service (QoS), has thus become an important objective. \n\nThis paper proposes a novel power rationing framework for HetNets which aims to achieve maximal profit while maintaining guaranteed service performance. A dynamic power control strategy utilizing Tullock contest is developed to model the power control of multiple cells as a game, allowing for the resolution of the conflict between profits and energy consumption. Incomplete information and the curse of dimensionality, two principal challenges in gaming, are resolved by a virtual repeated game that employs the Monte-Carlo method and particle swarm optimization (PSO) to achieve the Nash equilibrium. The equilibrium of power rationing balances the energy consumption and profits of cells, resulting in an optimal solution for both multimedia service providers and HetNets operators.  \n\nExperimental results demonstrate that the proposed model is an effective tool for power rationing in multimedia HetNets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The aim of this paper is to evaluate the reliability of the distribution system, taking into account the dependency on weather conditions for component failure rate (FR) and restoration time (RT). An efficient Multilevel Monte Carlo (MLMC) method has been used for this purpose. By including the impacts of different weather conditions such as high winds and lightning on FR and RT, a comprehensive evaluation of the reliability has been achieved. The MLSM method is a variance reduction technique for Monte Carlo simulation (MCS) that significantly reduces the time required for computation of the reliability indices. \n\nTo validate the effectiveness of this method, it was applied to a well-known test distribution system. The effects of both normal and adverse weather conditions were considered in the reliability evaluation. The results obtained through MLSM were compared with those obtained through MCS, and it was observed that MLSM provides more accurate and efficient results.\n\nIn conclusion, this study demonstrates that incorporating the impacts of weather conditions on the reliability estimation of distribution systems is crucial for effective power system planning, operations, and maintenance. The MLSM method proves to be a promising tool for reliable and efficient computation of the indices, which can help in making informed decisions regarding the maintenance and protection of the distribution systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we explore resource allocation for edge computing in IoT networks. We consider each end device to be an agent that decides whether to offload computation tasks to edge devices. The goal is to minimize the cost, considering power consumption and task execution latency. We use network states, including channel conditions, task queue, and remaining computation resources, to model the decision-making process as a Markov decision process. This is solved using the reinforcement learning approach, proposing a \u03f5-greedy Q-learning-based algorithm. Our simulations reveal that the proposed algorithm is feasible and achieves better trade-offs between power consumption and task execution latency compared to edge and local computing modes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Multicast is a frequently used method for point-to-multi-point communications in data centers. It is supported by well-known control protocols such as IGMP and PIM, but these protocols have limitations that can lead to over-subscription of network links and packet loss, which can have a negative impact on user experience. Additionally, path setup time can lead to deterioration of user experience. In order to address these issues, we introduce the DiRP algorithm, which uses a distributed decision-making architecture to optimize multicast tree formation while keeping path setup time low. Our system is implemented using off-the-shelf commercially available switches and maintains the creation and removal of source trees based on bandwidth requirements. We conducted tests using Cisco's Nexus switches and found that the DiRP Algorithm was able to set up multicast tree paths based on available bandwidth while maintaining distributed decision-making and substantially lower path setup time compared to centralized systems. Overall, our system offers a significant advancement in addressing the limitations of existing multicast systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Ensuring protection against side channel attacks (SCAs) is an essential aspect of designing secure embedded systems. Profiled SCAs, which include template attacks and machine learning attacks, aim to extract the key from a target by creating a model of the side channel behavior of a device identical to the target one. To prevent such attacks, we propose an architectural countermeasure that differentiates the side channel behavior of different instances of the same hardware design, thereby preventing the reuse of a model derived from a device other than the target one.\n\nOur proposed solution presents an instance that offers a protected hardware implementation of the advanced encryption standard (AES) block cipher. We experimentally validate the resistance of our countermeasure against both Bayesian templates and machine learning approaches that use support vector machines. We also consider various state-of-the-art feature reduction techniques to increase the effectiveness of the profiled attacks.\n\nThe results of our experiments indicate that our countermeasure successfully foils key retrieval attempts via profiled attacks. Our solution ensures that the key derivation accuracy of the attacks is equivalent to guessing randomly, thereby mitigating the risk of side channel attacks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a game-theoretic framework for investigating revenue sharing in the context of an Edge-Cloud computing system. Such a system involves computing service providers at the edge and in the cloud, who jointly offer computing resources to clients. Unlike traditional cloud computing systems, the providers in this setup operate independently and are motivated by self-interest.\n\nTo maximize overall revenue from clients and ensure high system-level efficiency, the system manager adopts a task distribution mechanism, along with a revenue sharing protocol to divide the received revenue among service providers. Providers aim to optimize their own utilities by strategically allocating their resources (i.e., computing servers) within the system. We model competition among the service providers in an Edge-Cloud system as a non-cooperative game.\n\nTheoretically and practically, we demonstrate the existence of a Nash equilibrium in the game through simulation and experimentation on our emulation system. We also observe that revenue sharing mechanisms have a crucial impact on system-level efficiency at Nash equilibria. Remarkably, we find that revenue sharing mechanisms based on actual contributions can result in significantly worse system performance than Shapley value sharing mechanisms and Ortmann proportional sharing mechanisms.\n\nOur proposed framework offers an effective way to comprehend and design efficient Edge-Cloud computing systems using economics-based approaches.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The current methods for modeling and predicting electricity load fluctuations are inadequate in capturing sub-interval variations, making it difficult for power system operators to efficiently follow and compensate for changes in load. This paper presents a unique approach by proposing a continuous-time model that characterizes the uncertainty and variability of the load. The model incorporates a Gaussian process prior and a covariance function reflecting the periodicity and smoothness of the load. The load process is projected on a reduced-order function space comprised of Bernstein polynomials, ensuring continuity over the estimation and forecasting horizons. An innovative method for estimating the hyperparameters of the model is developed, resulting in a posterior Gaussian process suitable for modeling and predicting California Independent System Operator load. The proposed model predicts the continuous-time mean value and uncertainty envelopes of future load, providing information on load variations and ramping requirements.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper outlines a novel method for identifying and classifying household activities through the use of residential smart meter data. Household appliance identification, which involves separating the individual appliances from the mains electricity measurement, is a complex classification problem. However, recent advancements in deep learning have enabled the efficient classification of fields such as image processing and speech recognition. This paper proposes a deep learning approach employing multilayer, feedforward neural networks that can effectively recognize common household electrical appliances from a typical smart meter measurement. The proposed approach is tested and validated using publicly available smart meter data and the identified appliances are then linked to specific household activities, or ADLs. The resultant ADL classifier can provide important insights into the household occupants' behavior, with numerous applications in energy and other relevant domains.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As more and more IoT applications are being developed, the demand for high performance in embedded devices has increased. However, the strict power constraints imposed during their deployment and operation pose a challenge for manufacturers. To address this issue, several Fraunhofer Institutes have developed a customizable IoT platform that is based on the RISC-V ISA and integrates a range of peripherals with a scalable computing subsystem. This three dimensional System-in-Package (3D-SiP) includes security features that aim for a medium security level and are designed to meet the specific requirements of the IoT market.\n\nThe security architecture of the platform enables secure deployment, operation, and update of IoT devices. Core security features include secure boot, an authenticated watchdog timer, and key management. The aim is to provide a security level that is suitable for the IoT market while still allowing for customization.\n\nThe Universal Sensor Platform (USeP) SoC is specifically developed for GLOBALFOUNDRIES' 22FDX technology, providing a platform for Small and Medium-sized Enterprises (SMEs) who typically do not have access to advanced microelectronics and integration know-how. These SMEs are often limited to Commercial Off-The-Shelf (COTS) products, but the USeP SoC provides a customizable solution that can meet their specific needs.\n\nOverall, this IoT platform is designed to meet the demands of the market while still providing security features that ensure safe deployment, operation, and update of IoT devices. The USeP SoC provides a unique solution for SMEs who may not have access to more advanced microelectronics and integration knowledge, allowing them to customize their IoT devices to meet their specific needs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unmanned aerial vehicles (UAVs) are becoming increasingly popular for use in wireless networks due to their ability to be deployed quickly without reliance on existing infrastructure. However, previous studies on UAV-aided communication have only explored generic scenarios, with few practical network evaluations. Therefore, a comprehensive assessment of the benefits of UAV communication in real networks is necessary. In this paper, we present a UAV-aided Wi-Fi Direct network architecture. Our proposed architecture involves deploying a UAV equipped with a Wi-Fi Direct group owner (GO), called a Soft-AP, to serve a set of Wi-Fi stations. To optimize the placement of the UAV, we propose a simpler and more efficient algorithm that dynamically assesses the network and repositions the UAV to reduce the distance between the GO and the client devices. The benefits of our scheme include improved connectivity for client devices, increased overall network throughput, and improved energy efficiency. To validate our claims, we conducted realistic simulations using the NS-3 network simulator. Our simulation results demonstrated a significant improvement in client association (23%), network throughput (54%), and energy consumption (33%) when compared to both stationary and randomly moving GO cases. Increases in the number of UAVs in the network lead to further improvements. To our knowledge, no other work has evaluated UAV-aided Wi-Fi Direct networks in this way.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a student project focused on the Internet of Things (IoT). The goal of the project was to create a network of connected devices utilizing LoRa technology, which would monitor the 20,000 m2 area of ENSEIRB-MATMECA engineering school. This implementation was necessary to oversee and control energy consumption of the school effectively. To achieve this, a LoRa technology deployment was chosen as it allowed the team to oversee the development from start to finish. The key components of the system were Modtronix LoRa modules inAIR9 and inAIR4, operating in the 868MHz and 433MHz frequency bands, respectively. The project was completed by a team of 18 students, who designed and developed all three main blocks including sensors, gateways, and servers.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The challenge of running complex physics codes on large-scale computers has led to the exploration of data flow paradigms. However, applying such approaches at extreme scales remains difficult. Uintah, a data flow framework, has successfully used data flow computing at the largest scales using two main models based on asynchronous communication. The first, a static graph-based approach, has seen success for some large-scale simulations. The second, a dynamic approach, offers improved performance for a challenging fluid-structure interaction problem that involves fluid flow and a moving solid represented using particle method on an adaptive mesh. Recent changes to the Uintah runtime system have necessitated a reevaluation of both approaches, comparing their effectiveness in the context of large-scale problems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Large-scale scientific facilities offer an extensive range of instrumentation and data products to researchers and educators worldwide. Such facilities are equipped with geographically distributed instruments and sensors, providing open access to users. This paper highlights crucial architectural design, deployment, and operational aspects of a cyberinfrastructure that enables the acquisition, processing, and delivery of data from large scientific facilities. These aspects are explored using insights gained from the National Science Foundation's Ocean Observatories Initiative.\n\nFurthermore, this paper presents recent models for data delivery and opportunities for insights and discoveries in various scientific and engineering domains as these facilities' data volumes and variety continue to grow. As such, the cyberinfrastructure enables researchers and educators to extract maximum value from extensive scientific facilities and data products generated therefrom.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There is an increasing interest in identifying users and behavior profiling from network traffic metadata for traffic engineering and security monitoring. Internet service providers and network security administrators need to create a user behavior traffic profile to make informed decisions regarding policing, traffic management, and investigate different network security perspectives. Understanding trends in application usage by analyzing network traffic metadata and extracting feature sets can be significant in identifying and profiling user activity. However, real-time network management remains a challenge as the behavior and underlying interaction of network applications are continually changing, and so is user behavior as the online interaction environment evolves. Furthermore, adequately describing user activity among generic network traffic in terms of identifying the user and their changing behavior over time is a challenge. In this paper, the authors propose a novel mechanism for user identification and behavior profiling using an application-level flow sessions identified based on Domain Name System filtering criteria and timing resolution bins, leading to an extended set of features. The module's validation was conducted by collecting Net Flow records for 60 days from 23 users, and a gradient boosting supervised machine learning algorithm was used to model user identification based on the selected features. The proposed method yields an accuracy of up to 74% for identifying a user based on the proposed features.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent developments in electronics and wireless network technologies have led to the creation of miniaturized wireless sensors that can be utilized for remote healthcare patient monitoring. When interconnected in a Wireless Body Area Network (WBAN), these small sensors can be placed on or inside the human body to continuously monitor various physiological parameters, such as temperature, blood pressure, and ECG. \n\nThe purpose of this paper is to compare the impact of two modulation schemes specific to the NB physical layer at 2.4GHz when considering scheduled access for an 11-sensor node WBAN model based on the IEEE 802.15.6 standard. The study focuses on three metrics: packet reception, latency, and node-level energy consumption, and utilizes the Castalia 3.3 framework, based on the OMNeT++ platform (4.6), for numerical simulations. \n\nThe simulations carried out in this study are justified through the support of the IEEE 802.15.6 standard by the Castalia simulator. By comparing the impact of different modulation schemes, this paper aims to provide insights into the most effective approach to wireless communication in WBANs for remote healthcare monitoring.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: E-learning has become a popular method for learning, educating, and training through the use of web-based technologies. The vast amount of data generated by such means has made manual feature engineering outdated and slow. This is where deep learning comes in, as it can automate the process to achieve efficient and effective e-learning. In this paper, we will introduce our personalized e-learning model that utilizes deep learning and process mining. The objective is to offer customized resources that cater to individual preferences. We will begin by providing an overview of e-learning as an online educational system, as well as deep learning as a broader family of artificial intelligence.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Algebraic thinking is a crucial skill that should be mastered by students at an early stage, even before they start learning algebra. However, the traditional approach to teaching algebra focuses more on procedures and rules, without emphasizing the use of algebraic thinking to solve real-world problems. In this paper, we present the results of a study that integrated students' algebraic thinking into a problem-based learning (PBL) process.\n\nOur study used qualitative data, including teaching notes, self-readings, reflections, self-evaluations, scenario problems, task-based interview transcripts, and notes. The PBL approach allowed students to apply their algebraic thinking skills to real-world problems, enabling them to use algebraic reasoning to understand the scenarios presented to them.\n\nWe found that integrating algebraic thinking into the PBL process was an effective way to enhance students' understanding of algebraic concepts. By using algebraic thinking, students were better able to identify patterns, make connections between different mathematical domains, and apply their reasoning skills to solve real-world problems.\n\nOur framework provides teachers with a useful tool to enhance the effectiveness of teaching and learning algebra, particularly at the lower secondary level. By incorporating algebraic thinking into the PBL process, teachers can help students develop their algebraic thinking skills and make connections between different mathematical domains. This will enable students to apply algebraic reasoning to a variety of real-world contexts.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Inter-subject variability in brain signals can significantly impact the accuracy of brain-computer interface (BCI) systems. These variations can occur within a subject between sessions, within a session, between online and offline settings, and even from epoch to epoch. Such variability necessitates the use of subject and session-specific decoder models. To address this challenge, this work proposes three deep learning models for subject-independent decoding of event-related potentials in electroencephalographic signals: (1) a shallow convolutional neural network, (2) a gated recurrent neural network, and (3) a hybrid one-dimensional convolution and gated recurrent unit model called CNN-RNN-Net. Experimental results show that the proposed models outperform conventional baseline models in decoding subject-independent data. CNN-RNN-Net, in particular, demonstrates improved classification results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Web 2.0 is a widespread phenomenon that utilizes www technology and design to facilitate information sharing. A prominent example of this is the use of hotel reviews by avid travelers to aid in their travel planning. However, the process of summarizing these reviews can be tedious without losing the essence of the original message. The Featured Noun Pairing method has been adopted as a solution to simplify this process. The approach links features (nouns) and adjectives to convey an accurate representation of the entire review. This approach has been rigorously assessed by experts and yields a high accuracy rate of 70% for nouns, 80% for adjectives, and 60% for pairing.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The article discusses the algorithms of three different control systems, namely the reference model adaptive control, static control, and adaptive control with disturbances estimation. These control systems are applied to a mobile object described by equations of kinematics and dynamics in three-dimensional space.\n\nA new control system structure is proposed in this study, which separates the circuits of astaticism, estimation, and parametric adaptation. Using the Lyapunov method, the asymptotic stability of the closed-loop adaptive system is proven. The geometric mean root is suggested as the parameter for adjusting the parametric adaptation.\n\nThe article also analyzes the errors of disturbances estimation and performs simulation and comparative studies of the three control systems mentioned above. With the proposed control system structure and parameter adjustment, the study shows promising results for efficient and effective control of mobile objects in three-dimensional space.\n\nIn summary, this study presents a novel approach to control systems for mobile objects with emphasis on the separation of circuits and parametric adaptation. The findings can be valuable in the development of advanced control systems for various applications involving mobile objects in three-dimensional space.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Long Short-Term Memory (LSTM) is a type of recurrent neural network that is capable of remembering long sequences of data and serving as a generative model. This model has the ability to reproduce the trained sequences for an arbitrary length, making it an ideal option for training on sequential motion data. In this study, we focused on training LSTM for Remo Dance, a traditional dance from East Java.\n\nTo acquire the motion data, we used a motion capture system to record bone rotation sequences from a real dancer. However, training sequential data on LSTM can be time-consuming, even with the use of advanced GPU technology. In order to achieve optimal training, we applied feature scaling and group-wise data grouping strategies.\n\nOur experiments revealed that the scale factor in feature scaling depends on the number of sequences that are trained together. While single-sequence training requires a value range of -8 to +8, multiple sequences need a lower value range. Additionally, we found that sequences with small variances can be better trained when combined with large variances sequences. As a result of our training, the LSTM was able to accurately reproduce the dance moves with certain variations.\n\nOverall, this study demonstrated the potential of LSTM for generating accurate and realistic motion data. By utilizing feature scaling and optimal data grouping strategies, training on sequential motion data can be streamlined to achieve optimal results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep learning algorithms have made significant advancements in terms of accuracy and efficiency, with Convolutional Neural Networks (CNNs) outperforming traditional algorithms in computer vision tasks such as object classification, detection, recognition, and image segmentation. For embedded applications that could benefit from machine learning at the edge, efficient hardware implementations are crucial to their success. This paper introduces a method that improves the processing efficiency of CNNs by taking inspiration from the way the human brain processes information. Specifically, the paper proposes (i) a clustering methodology that maximizes weights/activation reuse, and (ii) a heterogeneous processing element that integrates a Floating-Point Unit (FPU) with an associative memory that manages recurrent patterns. The experimental analysis confirms that the proposed method achieves substantial energy savings with minimal accuracy loss, providing a practical design option that could be utilized in the growing segment of edge computing.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Reducing the cost of content placement in cloud content delivery networks (CCDNs) is a topic of great interest in recent times. The traditional content placement methods have been useful in reducing the cost of content placement, but they have limitations in adapting to the dynamic deployment of cloud proxy servers in the CCDNs. Moreover, these methods only consider local decision-making, and global congestion dynamics are not taken into account, resulting in high content placement costs.\n\nTo address these challenges, we propose a content placement model based on Q-learning for dynamic CCDNs, known as the Q-content placement model (Q-CPM). This model utilizes up-to-date and reliable congestion values to make better routing decisions. An algorithm is also proposed based on the Q-CPM model to construct the Q-adaptive delivery tree (Q-ADT). This algorithm propagates local and nonlocal congestion information over the network learning packets, selecting the paths with low congestion costs, and adapts to the dynamic delivery environment. \n\nThe experimental results prove that the Q-CPM model is effective in reducing the overall congestion cost of content placement in dynamic CCDNs. It adapts flexibly to the changing environment of CCDNs, and the Q-ADT algorithm selects the most efficient paths to reduce the cost of content placement drastically.\n\nIn conclusion, the Q-CPM model and Q-ADT algorithm are useful in reducing the cost of content placement in dynamic CCDNs. The methodology provides a reliable decision-making approach for content placement, considering both local and global congestion dynamics in the CCDNs. Therefore, it has significant practical value in content placement optimization in CCDNs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, the issue of estimating the degree of wetlands based on remote sensing data is examined. Digital image processing is employed for this purpose, and satellite imagery is used as the primary data source. The paper selects a set of local and global features to segment the area into two classes, water surface and land surface. For every colored satellite image point, the corresponding part of the area to the water surface is determined through the segmentation process. As the task cannot be formalized, machine learning is used for segmentation, and a neural network is utilized for processing remote sensing data. This approach can automate the analysis and evaluation of water body areas. The proposed methods have been successfully employed to assess the Oka river basin in Nizhny Novgorod and Vladimir regions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Opinion maximization in social networks is a challenging optimization problem that aims to identify influential individuals, propagate the desired opinion to their neighbors, and ultimately achieve maximum opinion spread. However, previous studies have focused on calculating influence power solely based on network structure properties, and assumed that an individual's opinion remains static once determined.\n\nIn contrast, our proposed Influence Power-based Opinion Framework (IPOF) takes into account the dynamic nature of opinion formation, and estimates an individual's influence power based on the opinion formation process. IPOF consists of two phases: 1) influence power estimation, and 2) elimination of influence overlapping (EIO).\n\nTo estimate influence power, we introduce the exponential influence power model and use maximum likelihood estimation to estimate its unknown parameter. We also propose the weighted voter model to generate opinion series dynamically by leveraging influence power and intimate degree, and prove the concavity of the likelihood function using the Hessian matrix.\n\nTo facilitate large opinion propagation, we propose an influence power-based EIO algorithm to determine initial seed nodes. Experimental results on six social networks demonstrate the superiority of IPOF over state-of-the-art benchmarks.\n\nIn summary, our proposed IPOF framework provides a novel approach to solve the opinion maximization problem by considering the dynamic nature of opinion formation and estimating influence power based on the opinion formation process.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurately evaluating Ultra Low Power Systems on Chip (ULP SoC) poses a significant challenge for designers and developers. In particular, ULP SoCs used in embedded applications, such as Internet of Things (IoT) end-node devices, must interact with their environments. However, developing and benchmarking a complete SoC and its peripheral components, their interaction, and low-power policies can be extremely complex. To address this challenge, one approach is to implement the desired system on an FPGA with a monitoring infrastructure designed for fast and accurate evaluation. This paper presents a reconfigurable prototyping platform that is used for exploring SoC architectures and evaluating real-time applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces a novel C-band Hybrid Semiconductor Integrated Circuit (HySIC) rectifier that employs a Gallium Nitride (GaN) Shottky barrier diode and silicon (Si) matching circuit to facilitate RF energy harvesting. The main objective of the HySIC rectifier is to provide power to drive wireless sensor network systems in spacecraft applications. The development of this small-size, low-cost, and highly functional rectifier was made possible by the combination of GaN and Si materials, which exhibit high resistivity to cosmic rays and excellent integration technology, respectively. \n\nTo fabricate the HySIC rectifier, the GaN diode was subjected to rigorous testing, and the measurement results were used to guide the design and production of the rectifier. The final size of the rectifier was 3.9 mm x 8.1 mm, and it was observed to have a maximum conversion efficiency of 21.7% in the C-band. \n\nIn summary, the successful development of the HySIC rectifier with GaN Shottky barrier diode and Si matching circuit represents a significant milestone towards the realization of efficient RF energy harvesting for wireless sensor network systems in spacecraft. This technology has the potential to significantly reduce the cost and complexity of space missions while also extending the operational capabilities of spacecraft.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a novel approach to control the quarter car semi-active suspension system using a Model Predictive Control (MPC) framework based on the reachable sets approach. The proposed method allows for the systematic inclusion of information on future road disturbances' bounds in the prediction horizon, which enhances the suspension system's efficiency.\n\nThe control design is formulated using a unique Linear Parameter Varying (LPV) model that implicitly considers the dissipativity constraint of the semi-active suspension system. An efficient online computation of reachable sets with available disturbance information is developed for the MPC problem. These steps are integrated into a single MPC problem, which improves the controller's performance.\n\nSimulation results demonstrate that the proposed method outperforms the skyhook controller in terms of objective and constraint satisfaction. The proposed method's effectiveness is primarily due to the systematic inclusion of information on future road disturbances' bounds, which leads to better control design and more efficient operation of the suspension system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Although modern industry has become increasingly digital and automated, there are still many printed materials that have yet to be digitized. Additionally, even when materials are scanned, they are often scanned upside down. To solve this problem, a new algorithm has been developed that can automatically detect the correct orientation of a scanned document. This algorithm utilizes the Tesseract OCR engine and carefully extracts information from the scanned material to determine the correct orientation. With this new technology, the time-consuming and error-prone task of manually checking for orientation will be a thing of the past.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The utilization of electric grids in systems with a significant hydroelectric component presents a dynamic programming dilemma that involves using reservoir levels as state variables. To circumvent the curse of dimensionality brought about by discretizing these states, the SDDP technique has been used efficiently. However, with the growing use of renewable sources with faster variability and the integration of shorter-term energy storage, the optimization of such grids is experiencing new demands.\n\nTo address this problem, we present initial research on extending the SDDP framework for two time-scale problems. We apply this method to a Uruguayan system using a stylized model, with computations carried out by new open-source implementations of SDDP. The results indicate that despite the increase in problem size, the method remains feasible.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The 5G core network is designed to be service-oriented, utilizing service-based interfaces that enhance modularity and are more in line with cloud networking. To build upon this approach, we propose a service-oriented radio access network (RAN) that focuses on the handover process. By designing small-sized Next Generation Application Protocol (NGAP) functions as services that are easily testable and debuggable, handover control can be exposed as a service. In order to achieve this, we define service resources and their corresponding methods using the Representational State Transfer (REST) style. \n\nIn addition, we model the handover state as seen by the core network, source, and target radio nodes. These models are formally described, and it is proven that they expose equivalent behavior. By utilizing this approach, we can improve the efficiency and effectiveness of the handover process within the 5G core network, ultimately leading to a more seamless and reliable user experience.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We have developed a reliable and accurate real-time system for detecting defects in tiny parts, using intelligent production requirements and a defect detection algorithm based on deep learning and single short detector network (SSD). Our system addresses the challenges of manually setting conveyor speed and industrial camera parameters by establishing a correlation model between detection capability and conveyor speed, and by combining an industrial real-time detection platform with a missed detection algorithm. To ensure stability, we considered the properties of tiny parts and environmental parameters in our design. In our experiments using a 0.8 cm darning needle, we found that the highest accuracy was achieved when the conveyor speed was set at 7.67 m/min.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we investigate the estimation of the stress-strength parameter when the stress and strength are modeled as independent Kumaraswamy random variables, using the Type-II hybrid progressive censoring scheme. We address the problem from both frequentist and Bayesian perspectives, and consider two cases.\n\nIn the first case, we assume that the stress and strength have different first shape parameters but a common second shape parameter. Due to the lack of closed-form solutions, we derive the maximum likelihood estimate (MLE), an approximation MLE, and two Bayesian approximation estimates \u2013 Lindley's approximation and the Markov chain Monte Carlo (MCMC) method. We construct the asymptotic confidence interval of R, and use the MCMC method to obtain the highest posterior density credible intervals.\n\nIn the second case, we assume that the common shape parameter is known. We derive the MLE, the exact Bayes estimate, and the uniformly minimum-variance unbiased estimate of R. We also construct asymptotic and Bayesian intervals for the stress-strength parameter.\n\nTo compare the performance of the various methods, we conduct Monte Carlo simulations. Finally, we analyze a real dataset to demonstrate the applicability of our proposed methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Robotic exoskeletons have proven to be effective tools for rehabilitation and enhancing human abilities by imitating or supporting natural body movements. However, conventional control schemes based on fixed torque-ankle state relationship or human models have limited flexibility and adaptability in addressing personalized movement assistance needs. To overcome this limitation, an adaptive control strategy for personalized robotic ankle exoskeletons has been proposed in this paper.\n\nThe control system is initialized based on the most common requirements of a \"standard human model\" and is then evolved during its performance by effectively using feedback from the wearer to support different body shapes and assistance needs. The adaptation was implemented by applying an experience-based fuzzy rule interpolation approach supported by a muscle-tendon complex model.\n\nExperimental results based on different human models with varying support demands demonstrated the effectiveness of the proposed control system in improving the adaptability and applicability of robotic ankle exoskeletons. This approach can effectively address personalized movement assistance needs, providing a more customized and adaptable tool for rehabilitation and ability enhancement.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distribution networks are a vital component of power and energy infrastructure, yet they remain vulnerable to various faults. To enhance the resilience of the electric grid, it is crucial to detect and locate faults in distribution lines quickly and accurately. The use of impedance methods is a common technique for fault location. In this study, we developed an improved impedance method to determine the possible fault locations and their natural frequencies. We determined the fault section and its main location through a step-by-step simulation of the fault in each possible location. By matching the determined frequency to the existing natural frequency registered in the original fault voltage form, we were able to accurately determine the location of the fault. To test our proposed method, we applied it to Salim's 11-node network and investigated the effects of different resistors, fault inception angles, and fault types. Our results confirmed the accuracy and efficiency of the proposed method. By developing better fault location techniques, we can enhance the resilience of distribution networks and ensure the reliability of the electric grid.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper examines the accuracy of underdetermined direction of arrival (DOA) estimation through the use of a sum and difference composite co-array. The authors aim to enhance the degrees of freedom (DOF) by combining the sum co-array and the difference co-array, thereby creating the composite co-array. The proposed array configuration is evaluated through computer simulation in order to investigate its effectiveness for underdetermined DOA estimation problems. The beamforming performance of the sum and difference composite co-array is evaluated through performance metrics such as angle estimation error and resolution. Overall, the results suggest that the proposed array configuration has promising potential for improving DOA estimation accuracy in underdetermined scenarios.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the e-commerce industry, online customer service is an essential part of ensuring customer satisfaction. While email is an option, live chat is the preferred method among customers due to its speed and convenience. However, this puts pressure on companies to hire and pay for live chat admins, which can be time-consuming and costly. To address this issue, we have developed a Frequently Asked Questions (FAQs) Chatbot that uses a Recurrent Neural Network (RNN) in the form of Long Short-Term Memory (LSTM) for text classification.\n\nUnlike other chatbots that require users to set up key phrases manually, our chatbot uses LSTM to analyze the context of a customer's query and provide an accurate response. Our experiments have shown that the chatbot is able to recognize 86.36% of questions, with a response accuracy of 93.2%. By automating responses to frequently asked questions, companies can reduce their reliance on live chat admins and provide faster, more efficient customer service.\n\nIn conclusion, our FAQs Chatbot offers a promising solution for e-commerce companies looking to streamline their customer service operations. With its high recognition and response accuracy, it can help reduce the workload of live chat admins while improving overall customer satisfaction.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet of Things (IoT) is made up of a variety of devices that operate on different form factors. Some devices have access to ample resources and can connect directly to the electrical grid, benefiting from cloud computing memory and processing capabilities. Meanwhile, other devices operate on constrained resources, relying on finite battery sources and limited memory and computing power. Regardless of their form factors, consumers expect IoT devices to be secure, safe, and efficient in terms of data and operation.\n\nTo secure IoT devices with unconstrained resources, various tools and computing technologies exist. However, securing IoT devices with constrained resources remains a challenge. The limited options available present significant difficulties in terms of price, performance, and service costs. In this research paper, we propose a machine learning enabled cognitive secure shield that can secure dairy IoT devices operating under constrained resources.\n\nOur innovation lies in the design of a secure shield framework that enhances the security posture of dairy IoT devices without adversely affecting their useful life (ULD). We present our secure shield ML prototyping in the paper, showing how machine learning can be leveraged to create a shield that coordinates with the device's limited resources. With this approach, we can ensure that dairy IoT devices are both secure and efficient, meeting the expectations of consumers.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a new approach for estimating the direction of arrival (DOA) using a sparse technique for directive coprime arrays. Specifically, the complex radiation patterns are obtained through an electromagnetic simulator and then used in the DOA estimation. The performance of three different DOA algorithms, namely Capon, MUSIC, and Lasso based on compressive sensing, is compared. Our results indicate that the use of real antenna patterns leads to a faster degradation in performance as the DOA increases in the elevation plane. Overall, this study provides a novel and effective method for DOA estimation that can be used in a variety of applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Online Teachers' Professional Development (TPD) has emerged as a crucial means to enhance the professional capacity of teachers. However, evaluating the performance of teachers in online TPD poses a significant challenge. The complexity of online activities for teachers makes it difficult to establish a single standard for evaluation purposes. Nonetheless, the data accumulated by online professional development platforms could potentially offer a solution to this issue. In this study, we propose adopting a model-building process to construct a data model for online TPD. We delineate the structure and procedure of developing a teacher model for Ourteachers, China's largest online TPD platform. We also highlight the significant issues that need to be addressed in the model-building methodology.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The space network, which is essential for the realization of national security strategy, is composed of various satellites and ground stations that communicate with each other through high-speed laser links. At the core of this network are routers that directly impact the overall performance of the space network. To improve this, we have explored the technology of high-performance routers and have developed a topological structure specifically for space network routers.\n\nOur main focus has been on optimizing the modules of the router in order to reduce latency. The harsh environment of space demanded a fault-tolerant and self-recovery design for the space router. With low-latency and high reliability, this router fulfills the application requirements of the space network. Its key features include a packet forwarding delay of less than 10us, the ability to transmit data frames in multiple channels concurrently and the ability to recover autonomously from a single-channel fault without spreading.\n\nOverall, the development of a highly efficient and reliable space network router has significantly contributed to the achievement of the country's national security strategy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We have developed a framework for evaluating finger movements dynamically, including flexion, extension, abduction and adduction. Using depth maps as input, our framework uses Pose-REN, a state-of-the-art hand pose estimation method, to estimate 3D hand joint positions in real-time. The framework then estimates a plane containing the wrist and MCP joints and measures flexion/extension and abduction/adduction angles by applying computational geometry operations with respect to this plane. We have analyzed flexion and abduction movement patterns using real data, extracting the movement trajectories. Our preliminary results show that this method allows for automatic discrimination between hands with Rheumatoid Arthritis (RA) and healthy patients. Although measurements may not be as accurate as those obtained statically through goniometry, the acquisition is much easier, non-invasive and patient-friendly, highlighting the potential of our approach. The system can be used with or without an orthosis and significantly reduces the evaluation time, allowing for measurements with minimal intervention.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, there has been an increased interest in 3D action recognition within both research and industrial communities. This is due in part to advancements in depth sensors and skeleton estimation algorithms. While many methods have been explored using handcrafted features and traditional classifiers or recurrent neural networks, they struggle to capture high-level spatial and temporal features of entire skeleton sequences. To address this, we proposed a novel encoding technique that transforms joint-joint distance and joint-joint orientation features into color pixels. By concatenating these features across all frames of a sequence, a color image is generated to depict the spatial joint correlations and temporal pose dynamics of the action. We also adopted an end-to-end fine-tuning strategy of pre-trained deep convolutional neural networks to capture multiple high-level features at multi-scale action representation. When tested on the NTU RGB+D dataset, the proposed method outperformed existing methods for both cross-subject and cross-view evaluation protocols, achieving state-of-the-art results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A smart grid protocol has been developed to manage both the demand and supply of renewable energy based on customer needs. The system employs the Markov process and Poisson's method to facilitate two-way communication and calculate the average cost of energy production within the main grid. With this protocol, electricity generation from resources such as solar or wind power, is stored in batteries to be used when necessary. Surplus energy is sent back to the main grid. The protocol also calculates the average cost of power production and suggests ways to improve system efficiency.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Narrow Band Internet of Things (NB-IoT) is a cellular network technology known for its low power consumption, low cost, wide coverage, and high connection capacity. However, efficient cell search remains a crucial component for effectively identifying the Physical-layer Cell Identity of NB-IoT systems. Unlike LTE systems, the cell groups in NB-IoT are not defined, hence the complexity of NB-IoT cell search is higher. Hence, this study proposes a new method to reduce the complexity of NB-IoT cell search. The proposed method results in an 80% detection accuracy within ten radio frames, while maintaining a signal-to-noise ratio of -12.6dB based on simulation results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless sensor networks often rely on DV-Hop localization algorithm for range-free positioning, which estimates actual distance by calculating the average hop distance. However, this algorithm suffers from high error rates and energy consumption in practical applications. To overcome these challenges, this paper proposes an improved DV-Hop localization algorithm based on differential evolution.\n\nThe differential evolution algorithm is a simple and versatile approach widely used in various fields. In this advanced DV-Hop algorithm, the weighted process is improved in the second step to correct the average hop distance error. The third step uses the differential evolution algorithm to optimize the positioning of unknown nodes. By incorporating these modifications, the proposed algorithm significantly enhances positioning accuracy.\n\nIn conclusion, the advanced DV-Hop localization algorithm presented in this paper overcomes the limitations of the traditional DV-Hop algorithm through the use of differential evolution. The experimental results demonstrate that the proposed algorithm can effectively improve the accuracy of wireless sensor network localization.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distinguishing between different types of time series that are sampled from dynamic systems is a common challenge in the field of systems and control engineering. This is especially important in areas such as health monitoring, fault detection, and quality control. However, when no underlying model of the system is known, and there is measurement noise present, interpreting long signals can be difficult.\n\nTo address these challenges, this paper presents a new non-parametric classifier that is based on topological signatures. Our model uses kernel density estimates (KDEs) over persistent homology diagrams to learn classes, and predicts new trajectory labels by calculating Sinkhorn divergences on the space of diagram KDEs.\n\nThrough our research, we demonstrate that this approach accurately discriminates between different states of chaotic systems that are similar in terms of their parameters. Moreover, we find that the performance of the classifier remains robust even when there is noise present.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces a loop antenna that is designed to enable deep implant powering in intracranial pressure monitoring applications. It builds upon our ongoing work developing a wirelessly powered implantable system for ICP monitoring. Our experimental results demonstrate that the implant can now be activated up to a depth of 19 mm beneath the skin, representing an 8 mm improvement over our previous work where we could only activate the implant to a depth of 11 mm. Additionally, we have successfully obtained pressure readout results with the proposed antenna.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper describes a real-time voltage sag monitoring system based on S-Transform analysis. The system extracts and analyzes real-time signal features using LabVIEW software, with a data acquisition module for voltage measurement used to capture voltage sag events generated by Three Phase Chroma Programming AC Source. The S-transform algorithm is utilized to identify significant features of voltage sag for accurate detection. The efficacy of this technique is compared and validated against theoretical types of voltage sag events in terms of magnitude and phase angle jump. The results demonstrate that the S-transform algorithm is superior in providing an accurate analysis for monitoring voltage sag activity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The urgency for effective intrusion detection systems (IDS) for the trillion connected internet of things (IoT) devices expected by 2020 has sparked increased interest in anomaly detection. However, there are currently no mechanisms to evaluate the performance of such systems in real-life deployments. This paper presents the results of applying asymptotic analysis to assess the performance of an anomaly detection algorithm designed using fuzzy logic methodologies for use in intrusion detection software for ZigBee Wireless Sensor Networks (WSNs). The software addresses the vulnerability of the ZigBee protocol to flood attacks during node discovery and association to the network. The IDS is hosted externally to the WSNs, preserving sensor node resources by employing a lightweight solution.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, there has been increasing research into the application of Wi-Fi radar signal processing for dual radar-communication purposes. Combining these two features not only allows for cost savings to vehicle manufacturers by reducing design complexity, but also results in saving valuable radio frequency spectrum resources. This study presents a feasibility analysis and hardware implementation of a Doppler radar that operates on IEEE 802.11p Wi-Fi packets. Their approach utilized a 5-MHz OFDM modem that closely adheres to the 802.11p PHY and was implemented on two Universal Software Defined Peripherals (USRP) via MATLAB's USRP toolbox. Subsequently, a real-time Doppler radar system was achieved by utilizing estimation of signal parameters via rotational invariance technique (ESPRIT) to extract information from a collection of received Wi-Fi symbols. Results demonstrated an average accuracy of sub-0.64 m/s in measuring a vehicle's velocity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel approach to detecting unusual crowd behavior in video sequences by utilizing probability models of speeds and directions. The proposed method makes use of optical flow to extract velocities from each image frame, which are then reduced to motion orientations and speed. Expectation maximization algorithms were used to construct a mixture model of von Mises distribution to describe the set of directions, and a mixture model of normal distribution for the speed set. The distance between each frame and a collection of reference frames is then calculated using the probability density method, allowing changes in crowd motion to be identified. However, it should be noted that while this approach is effective in detecting unusual crowd behavior using the speed model, it has not yet been adapted to unstructured crowds. The efficacy of this method was tested on various publicly available crowd datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Human activity recognition is achievable through the use of a small top K compressed personalized training dataset. However, the lack of access to personalized datasets can pose challenges to personalizing human activity recognition. To address the shortage of personalized datasets, an integrated training and testing algorithm was implemented. This algorithm extracts and labels top K time domain features to be used for training the implemented hybrid Na\u00efve Bayes Nearest Neighbor algorithm for real-time human activity detection. This hybrid algorithm is built using HTML5, JavaScript, and Cordova multi-platform for Android. Three subjects were recruited and were requested to carry a Samsung smartphone in their front pocket while they trained and tested the model. Each subject was asked to perform human activities during the training and testing phases. During testing, each subject was asked to perform each human activity three times, resulting in the collection of 21 comma-separated values files. These files were analyzed using a confusion matrix based on precision, recall, and accuracy. The results show that there is a balanced precision, recall, f-measure, and accuracy of 79%, 75%, 75%, and 70% respectively for all subjects. Furthermore, our results suggest that it is possible to train and test data-intensive algorithms using smaller training datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Getting a stability certificate is a crucial step to deploying reinforcement learning (RL) in real-life mission-critical systems. To ensure stability-certified RL, this study proves that smoothness is a significant property from a control-theoretic viewpoint. Solving a feasibility problem utilizing semi-definite programming for both linear and nonlinear dynamical systems provides the smoothness margin that does not require the accurate parameters of the learned controllers. The smoothness margin can confirm stability during exploration and deployment for deep neural network policies, which outperforms nominal controllers in performance. The study has the potential to enhance robust Lipschitz continuous policy learning and offers new opportunities for future research.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep neural networks (DNNs) are widely employed for recognition and pattern analysis in machine learning tasks. However, DNNs can be vulnerable to adversarial examples, which are created by adding a small amount of noise to an original sample, leading to misclassification by the DNN. In some cases, selective untargeted adversarial examples are needed to avoid classification as certain avoided classes. For example, a modified tank cover may cause misclassification by a DNN, but the bandit equipped with the DNN must misclassify the modified tank as a class other than certain avoided classes, such as a tank, armored vehicle, or self-propelled gun.\n\nIn this study, we propose a scheme for generating selective untargeted adversarial examples with 100% attack success and minimum distortions. The proposed approach involves performing a transformation to minimize the probability of certain avoided classes and distortions in the original sample. Experimental datasets including MNIST and CIFAR-10 were used with the Tensorflow library. Our results indicate that the proposed scheme can create selective untargeted adversarial examples with 100% attack success and minimal distortions (1.325 and 34.762 for MNIST and CIFAR-10, respectively). \n\nOverall, our approach can enhance the security of DNNs against adversarial attacks, particularly in scenarios where selective untargeted adversarial examples are required to avoid misclassification as certain avoided classes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Proteins play a critical role in organisms and are involved in nearly every cellular process. Among their various functions, many proteins serve as biochemical enzymes required for metabolic processes. This paper proposes a method for identifying protein localization sites within cells, testing different learning algorithms such as Logitboost, Tree Decision, Binary Decision Tree, and Bayesian Classifier methods to develop a more effective data analytic model. The yeast data set was used to measure the accuracy and efficiency of the learning algorithms. Data mining principles were applied, resulting in five classes and nine attributes, with a total of 921 records. The data analysis system relied on both correctly classified and incorrectly classified instances. The experimental results show that the Logitboost technique achieved the highest accuracy rate at 65.65%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a method to estimate the pose of an unknown motion target by combining the improved iterative closest point (ICP) algorithm and the adaptive extended Kalman filter (AEKF). The approach involves obtaining the point cloud of the scene by means of stereo vision and then extracting the target using color and depth information. Next, the point clouds of adjacent frames are registered using the improved ICP-AEKF in closed loop configuration to determine the relative pose parameters between the target and stereo camera. A physical experiment was conducted under the ZED binocular camera, indicating that the improved ICP-AEKF performed significantly better than the traditional ICP in terms of accuracy, convergence speed, and robustness.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Current machine learning and data mining methods are limited in their ability to handle data with varying types, unknown data relationships, and variable correlations in different value ranges. In order to address these limitations, the Partial-Value Association Discovery (PVAD) algorithm was developed. By using the PVAD algorithm, the inadequacies of traditional machine learning and data mining techniques can be circumvented. This paper presents a case study in which the PVAD algorithm is employed to analyze data from engineering student records and computer network traffic patterns. The findings indicate that the PVAD algorithm is capable of identifying relevant characteristics for engineering retention and network traffic normalcy. Overall, the PVAD algorithm presents a promising approach for machine learning and data mining in the face of diverse and complex datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this article, we introduce a new approach to inverse tone mapping called \"iTM-Net.\" To train this network, we also propose a novel loss function that takes into account the non-linear relationship between low dynamic range (LDR) and high dynamic range (HDR) images. When using convolutional neural networks (CNNs) for inverse tone mapping, traditional training techniques can result in issues due to the non-linear nature of LDR and HDR images. To overcome this, our loss function non-linearly tone-maps target HDR images into LDR ones using a tone mapping operator, and calculates the distance between the tone-mapped and predicted images. By normalizing HDR images and reducing the non-linear relationship between LDR and HDR, our proposed loss function produces HDR images of higher quality compared to existing methods, including state-of-the-art techniques, as evidenced by both HDR-VDP-2.2 and PU encoding + MS-SSIM metrics. Furthermore, our loss function improves CNN performance compared to loss functions that do not take into account the non-linear relationship.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapid development of intelligent communication systems, automatic modulation classification (AMC) has garnered extensive research interest. This is because AMC plays a significant role in both civilian and military applications. Specifically, we consider AMC for millimeter wave-over-fiber (MMWoF) communication, which is of particular practical interest due to the benefits it provides for centralized analysis and processing, leveraging the low transmission loss of MMW signals over fiber optic channels.\n\nIn this letter, we propose using autoencoder neural networks for automatic feature extraction and classification, preceded by a pre-processing step applied to the input signal samples. We thoroughly investigate the performance of the proposed system through simulation and experimental verification under various impairments, including fiber chromatic dispersion and amplified spontaneous emission noise. We present the results in terms of the probability of correct classification for different values of the optical signal-to-noise ratio and different lengths of fiber channels. Moreover, we show that the simulation results are in good agreement with the experimental outcomes.\n\nOverall, this study demonstrates the effectiveness of the proposed approach for AMC in MMWoF communication. This research could lead to further advancements in the field, enabling more efficient and reliable communication systems for a broad range of applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent times, online harassment and cyber-bullying have emerged as two of the most serious issues plaguing the public online community. In this study, we utilized data obtained from talk page edits on Wikipedia to train a multi-label classifier capable of detecting various types of toxicity present in user-generated online content. To deal with the data imbalance problem in the Wikipedia dataset, we presented different data augmentation techniques. Our proposed solution features an ensemble of three models: convolutional neural network (CNN), bidirectional long short-term memory (LSTM), and bidirectional gated recurrent units (GRU). We divided the classification problem into two steps. First, we determined whether or not the input was toxic. Next, we proceeded to identify the types of toxicity present in the toxic content. The evaluation results show that our proposed ensemble approach yielded the highest accuracy among all the algorithms considered. Our model achieved an F1-score of 0.828 for the toxic/non-toxic classification and 0.872 for the prediction of toxicity types.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Dialogue Act classification is a crucial problem in Natural Language Processing and can be used as a standalone task or as input for downstream applications. However, existing approaches rely heavily on supervised techniques, which require annotated samples and do not take advantage of the vast amount of available data in various fields. This paper provides a brief overview of the commonly used datasets to evaluate Dialogue Act classification methods and proposes the use of the Optimum-Path Forest (OPF) classifier for this task.\n\nWe suggest a modified version of OPF, named M-OPF, that uses a majority voting strategy to determine the corresponding class for each cluster. Our experiments have shown that M-OPF yields good results in terms of accuracy and V-measure when compared to traditional clustering methods such as k-means and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). Additionally, our results indicate that M-OPF is less sensitive to hyper-parameter tuning when compared to HDBSCAN.\n\nIn summary, our proposed approach, M-OPF, offers a promising alternative to the supervised techniques commonly used in Dialogue Act classification. It takes advantage of the increasing amount of data available in different domains and requires less hyper-parameter tuning than other clustering methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The wireless body area network (WBAN) is an emerging technology that promises to deliver a 24-hour on-the-go healthcare service for users. Despite its potential benefits, the WBAN still faces several challenges in terms of protecting the privacy of users' sensitive personal information and the confidentiality of the healthcare center's disease models. To address these issues, many privacy-preserving schemes have been proposed. However, the efficiency and accuracy of these schemes remain a concern.\n\nThis paper proposes an efficient and privacy-preserving priority classification scheme, named PPC, for classifying patients' encrypted data at the WBAN-gateway in a remote eHealthcare system. To minimize the system latency, a non-interactive privacy-preserving priority classification algorithm is designed that allows the WBAN-gateway to conduct the privacy-preserving priority classification for the received users' medical packets by itself and relay these packets according to their priorities (criticalities). A detailed security analysis shows that the PPC scheme can achieve priority classification and packet relay without compromising the privacy of users' personal information or the confidentiality of the healthcare center's disease models.\n\nExtensive experiments with an Android app and two Java server programs demonstrate the efficiency of the PPC scheme in terms of computational costs and communication overheads. Overall, the proposed scheme offers a promising approach to address the privacy and confidentiality challenges faced by the WBAN and opens up new opportunities for on-the-go healthcare services.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An enhanced algorithm for describing wood texture features using Local Binary Pattern operator has been proposed. This algorithm utilizes the linear LBP operator to extract the texture characteristics of an image, followed by the calculation of texture similarity among the sub-regions of a wood image. This method is particularly effective in distinguishing striped and patterned wood due to their high texture similarity. Based on experimental results, this algorithm exhibits strong capabilities in describing wood texture features, surpassing traditional methods in terms of robustness, and achieving higher recognition accuracy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Quantum particle swarm optimization (QPSO) is a stochastic searching algorithm inspired by the basic concept of the PSO algorithm and quantum theory. However, when dealing with multimodal and complex inverse problems, the algorithm may face premature convergence. To address this issue, several modifications have been introduced.\n\nRandomly selecting the best particle to participate in the current search domain is one of the improvements. Additionally, a mutation strategy is added to the mean best position, and an enhancement factor (EF) is incorporated to increase the global search capability and avoid premature convergence.\n\nFurthermore, parameter updating strategies have been proposed to strike a balance between the exploration and exploitation searches. The proposed MQPSO has been tested on well-known multimodal functions and an inverse problem, showing exceptional merit and efficiency.\n\nIn summary, MQPSO is an enhanced version of QPSO that improves the global optimization capability to overcome premature convergence, making it a promising algorithm.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Emotion classification using lexicons has a vast array of applications, ranging from social media analysis to pervasive computing. However, generating lexicons can be a difficult and time-consuming process, as they are usually hand-crafted. The primary research challenge in this field is creating a generalized lexicon that works well across all domains. To address this problem, this study explores the automatic expansion of emotion lexicons to facilitate domain adaptation. \n\nThe proposed approach, CB-Lex, utilizes a seed lexicon and an unlabeled corpus from the target domain. The results of the experiments show that our expanded lexicons provide over a 6% improvement in F-Measure. This work focuses on making the process of domain adaptation easier and more efficient, which will undoubtedly have far-reaching impacts on various industries.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Text-based information can be easily found on the internet and can be used to solve various problems. Customer reviews are a valuable source of information that can provide insights into rare events such as product recalls, ethical allegations or legal concerns, and threats to product safety. However, manually identifying such cases can be a daunting task, especially when thousands of reviews are posted every day. Failure to identify these cases can have a detrimental impact on the industry's reputation, product liability and customer experience. \n\nTo overcome this challenge, we propose a two-stage approach in iCASSTLE that leverages Positive and Unlabeled data (PU classification). In Stage I, we use three unique components of text mining to extract representative training data that contains instances of both classes in the right proportion. In Stage II, we use the results from Stage I to run a semi-supervised classification. \n\nOur approach has been applied to multiple datasets with differing levels of Product Safety and imbalance. In all relevant use-cases, iCASSTLE has consistently outperformed state-of-the-art methods. By leveraging PU classification and our unique approach, we can detect rare events with greater accuracy and speed, ultimately improving industry health and dependability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: For mechanical systems, accurate fault diagnosis of a gearbox is crucially important. However, due to the many mechanical parts that constitute a gearbox, there are a variety of failure modes, making accurate diagnosis challenging. Additionally, while raw vibration signals can be easily obtained from real gearbox applications, labeling them \u2013 particularly in cases of multi-fault modes \u2013 can be costly. These issues pose challenges to traditional supervised learning methods for fault diagnosis.\n\nTo address these challenges, we propose a new diagnostic method for a gearbox based on an active learning strategy that utilizes uncertainty and complexity. Specifically, our method employs empirical mode decomposition-singular value decomposition (EMD-SVD) to generate feature vectors from raw signals, and then uses an active learning scheme to select the most valuable unlabeled samples for labeling and addition to the training data set. Finally, a random forest (RF) algorithm trained on the updated data set is employed to recognize gearbox fault modes.\n\nIn experimental tests on two cases of gearbox fault diagnosis data, the proposed method outperformed both a supervised learning method and other active learning methods. This validates the method's effectiveness and superiority.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The security system installed in the room requires remote monitoring which can be accessed via a smartphone or laptop by using the web. For the implementation of a web server, a Raspberry Pi is utilized as a low power consumption device. In this study, a monitoring system based on IoT technology has been developed which allows an object to send data over the network without requiring human interaction. The Raspberry Pi is used as a web server and is installed with Motion Eye as a web interface that features a webcam as a sensor. A client can monitor the room through a web browser by streaming the defined URL according to the IP address obtained from the server.\n\nVarious observations have been made on the system, such as the number of users, frames per second, and object resolution related to delay and CPU performance. Simulations were conducted at different levels of user numbers and video resolutions, and results show that IoT technology has the potential to be utilized for CCTV monitoring using Raspberry Pi. The CPU usage is dependent on the resolution and frames per second of the video which impacts its performance. However, the video quality becomes better with an increased resolution and fps.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A natural way to detect a wireless random reactive jammer (RRJ) is to use the perceived violation of the carrier sensing protocol, which is used in many wireless random access protocols like Wi-Fi, by the detector located typically at the access point (AP). When the wireless medium is sensed as being occupied, a compliant station will not transmit, while an RRJ station will often initiate transmission, leading to detection. However, the presence of hidden terminals (HTs), which can be detected by the AP but not the sensing station, complicates carrier sensing as a basis for RRJ detection, providing plausible deniability to an RRJ suspected station. There is an inherent tradeoff between the RRJ's objectives of avoiding detection and disrupting communication. In this paper, we model the behavior of both RRJ and compliant stations using a simple Markov chain model and propose a detection framework using Markov chain hypothesis testing. Our analysis yields the receiver operating characteristic (ROC) of the detector and the optimized behavior of the RRJ. Our innovation is in leveraging carrier sensing as a natural and effective basis for RRJ detection, which sets us apart from previous work on jamming detection.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Mobile Ad-hoc Networks (MANETs) experience issues with hidden and exposed terminals, leading to reduced utilization of channels and throughput in media access control protocols such as the dual busy tone multiple access protocol (DBTMA). In order to address these problems and improve the fairness and throughput performance of the DBTMA, this paper proposes enhancing the quality of service (QoS) in MANETs using the Retransmission Dual Busy Tone Multiple Access (RDBTMA) protocol. This novel protocol improves upon existing methods by utilizing both busy tones and Ready To Send/Clear to Send (RTS/CTS) dialogues, while also incorporating fast retransmission and a negative acknowledgement (NACK) strategy for improved effectiveness. Simulations demonstrate that the proposed RDBTMA protocol results in improved QoS in terms of network throughput (21.9%), packet delivery ratio (17.8%), reduced packet loss (14.9%), and decreased route discovery delay (38%) compared to existing methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The widespread use of mobile devices with accelerometers has created new opportunities for capturing the meaning of human activities and enhancing user experience through behavior-based recommendations. However, accurately recognizing daily human activities is a challenging problem due to the multi-dimensional nature of accelerometer signals in both spatial and temporal domains. These signals have different attributes for representing different activities, making it difficult to compare them directly as they are embedded in a non-metric space. \n\nTo address this challenge, this paper proposes the Personalized Recurrent Neural Network (PerRNN) to dynamically segment and recognize human activities based on accelerometer data. The proposed architecture is inspired by spatiotemporal predictive learning and is capable of memorizing different acceleration signals' appearances and temporal variations in a unified memory pool. \n\nThe performance of the PerRNN framework is evaluated on a commonly used dataset, WISDM. The experiment results demonstrate that the proposed system recognizes six different human activities with an overall accuracy of 96.44%, which outperforms state-of-the-art schemes. \n\nIn summary, the proposed PerRNN system provides a promising approach for accurate and personalized human activity recognition, which has important implications for various fields, including healthcare, fitness, and entertainment.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The identification of individual household appliances in the residential power grid is essential for effective consumption management and anomaly detection. For this purpose, it is essential for each appliance to have its Electric Load Signature (ELS) generated through IoT equipment such as Smart Meters or Smart Plugs. In this regard, our proposed work aims to read and detect residential household appliances in the network, using Smart Plugs and Machine Learning Algorithms to create individual ELSs for each load. \n\nWith the use of Decision Tree and Naive Bayes algorithms, we analyze and detect specific electrical parameters for each appliance. The ELS data gathered is stored in a centralized database within the Home Energy Management System and trained to enable identification in each Smart Plug. \n\nOur visual application provides consumers with real-time information on the operating appliances, their consumption history, as well as any anomalies or unwanted changes detected in the residential network. Overall, our proposed work is a significant step towards smarter and more efficient power grid management in households.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents an innovative approach to identifying fruits and vegetables in the retail market by utilizing an image recognition system. A video camera captures images of the produce, which are then labeled with a price according to their weight. The system aims to enhance the user experience, streamline the identification process, and minimize the need for human-computer interaction. To achieve this, the system is composed of a Raspberry Pi, camera, display, load cell, and a case. Various convolutional neural networks were tested and retrained for classification purposes. Usability testing was conducted via a heuristic evaluation with multiple users, which found that the system is more user-friendly than traditional manual methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Objective: Electrical Impedance Tomography (EIT) imaging has been recently implemented with deep learning techniques. However, there are certain challenges to address, such as the inability to recover sharp or cornered targets with the traditional circular inclusion training data. This paper introduces an iterative-based inversion method and a convolutional neural network (CNN) based inversion method to recover challenging inclusions such as triangular, rectangular, or lung shapes. The CNN-based method uses random circle or ellipse training data. \n\nMethods: The paper proposes a bases-expansion subspace optimization method (BE-SOM) as an iterative method with induced contrast current (ICC) concept and total variation regularization. Additionally, the theory behind BE-SOM and the introduced physical concepts prompt the proposal of a dominant-current deep learning algorithm for EIT imaging, where the dominant parts of the ICC generate multi-channel inputs for the CNN.\n\nResults: The proposed methods underwent testing with numerical and experimental data, with consideration for realistic phantoms such as pneumothorax and pleural effusion pathologies. The results showed significant improvements in reconstructing targets with sharp corners or edges. The proposed methods showed capabilities of stable, fast and high-quality EIT imaging, with great potential for clinical applications.\n\nConclusions and Significance: The proposed methods provide a solution to existing limitations in traditional EIT imaging techniques. The iterative-based inversion method and CNN-based inversion method proved promising in overcoming the challenge of recovering sharp or cornered targets, which indicates their potential for clinical use in creating quantitative EIT images.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Currently, there is a lack of tools available for validating 5G scenarios. Due to the increasing traffic demand on 5G networks, network operators are in search of cost-effective solutions. One such solution being adopted is the multi-tenancy approach, which requires changes to the network architecture to accommodate user mobility. This shift towards more dynamic services necessitates the development of new tools that can provide these capabilities to facilitate the validation of all new developments. To meet this need, this work presents a novel experimentation framework for the emulation of 5G scenarios with real-time user mobility and multi-tenancy support. This framework has been validated through a series of experiments, establishing its functional viability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Honeypots have become an increasingly vital tool for capturing attacks, as they enable organizations to observe attacker behavior in a controlled environment. Hybrid honeypots are particularly effective since they comprise both front-end and back-end components. However, the traditional design of hybrid honeypots has several limitations, including issues with flow control and unrealistic topology simulation.\n\nIn response to these challenges, our paper proposes a new architecture for hybrid honeypots based on SDN technology. This approach enables us to create a large and realistic network that can accurately mimic a production environment and attract attackers. We use the controllability and flexibility of the SDN controller to simulate complex network topologies, directing high-level attacks to a high-interaction honeypot for further analysis.\n\nOur system overcomes the shortcomings of traditional honeynets, which often struggle with network spoofing and flow control. We conducted experiments using mininet to verify the effectiveness of our new mechanism. The results demonstrate that the system is more sophisticated and capable of more stealthy traffic migration, providing organizations with greater intelligence over network security.\n\nOverall, the combination of SDN and hybrid honeypots represents a significant advance in network security technology, enabling organizations to achieve a more comprehensive understanding of attack behavior and more effectively protect against cyber threats.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Edge detection is a significant problem in the field of computer vision. In this paper, we introduce a new concept for edge detection known as Structural Edge. The Structural Edges consist of occluding contours of objects as well as orientation discontinuities in surfaces that define the 3D composition of objects and their surroundings. This is in contrast to the semantic edge which only encompasses the boundary between semantic areas. While traditional edge detection methods focus on either semantic boundaries or low-level gradients, our emphasis is on the structural edge.\n\nTo achieve this, we propose a structural edge dataset in this paper along with a benchmark test. This dataset consists of 600 natural indoor and outdoor scenes, with manually labeled structural edges. Furthermore, it has been validated by eye-tracking data from ten participants, across a total of 20 trials. Subsequently, we utilize this dataset to benchmark existing edge detection methods, both learning-based and non-learning-based. Our findings suggest that current methods are inadequate for effectively detecting structural edges. We encourage future researchers to explore the proposed task.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper examines the design of transceiver systems for uplink massive multi-input multi-output (MIMO) systems with channel sparsity. Recent developments have demonstrated that sparsity learning-based blind signal detection can retrieve both the channel and data using message passing based noisy matrix factorization. To build on this, we introduce a semi-blind signal detection approach where each user packet includes a short pilot sequence. This allows for the knowledge of pilots to be integrated into the message passing algorithm for noisy matrix factorization. Our research demonstrates that the semi-blind signal detection scheme we propose performs remarkably better than previous state-of-the-art detection and training-based methods in the short-pilot regime.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless Sensor Network (WSN) is an emerging next-generation sensor network that has a wide range of application prospects. Localization technology is one of the critical key functionalities of WSN. However, in complex indoor environments, fluctuations in received signal strength can significantly degrade positioning accuracy. To address this problem, this paper proposes a fingerprint localization method based on the received signal strength (RSS) distance and an improved weighted k-Nearest Neighbor (KNN) algorithm.\n\nIn the offline phase, a fingerprint database is established. In the online phase, the RSS values of the measurement points are measured in real-time, and a two-stage RSS distance calculation using the Euclidean distance is performed. Finally, the improved weighted KNN algorithm is used to solve the problem of non-Gaussian distribution of measurement noise and calculate the final position coordinates of the measurement point.\n\nSimulation results illustrate that this method can substantially reduce the influence of signal strength fluctuations and improve the positioning accuracy. Therefore, the proposed method has potential applications in various IoT-based indoor location-based services, such as real-time location tracking of people, assets, and vehicles, indoor navigation, and emergency response systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a sensorless control strategy for interior permanent magnet synchronous motors (IPMSMs) using a sine-wave high-frequency (HF) voltage injection method in a three-phase four-switch (TPFS) inverter. The paper examines the principle of the TPFS inverter fed IPMSM drive system and proposes a nonorthogonal-nonlinear k-l axis coordinate system for sector identification and voltage projection. Furthermore, the proposed TPFS inverter fed sensorless control strategy is presented, including the position estimation scheme and equation for injected HF voltage in the proposed k-l axis coordinate system. The simulation results obtained using Matlab/Simulink and experimental results for a 5kW IPMSM motor prototype demonstrate the effectiveness of the proposed scheme, showing that estimated values track the actual values well during different working conditions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces a highly efficient Motion Estimation (ME) algorithm and VLSI architecture for High Efficiency Video Coding (HEVC) systems. Specifically, an Adaptive Fast Search Algorithm is proposed that adapts the search space to video characteristics and significantly reduces the number of search candidates. Experimental results demonstrate that this algorithm reduces computational complexity by 54% while maintaining a marginal 2.01% performance degradation compared to conventional approaches.\n\nIn addition to the algorithm development, the paper presents the VLSI architecture and circuit implementation of the proposed ME engine. Architectural and circuit-level optimizations are described to enhance throughput and reduce complexity. The proposed ME system underwent the ASIC design flow and was realized on the Xilinx Zynq UltraScale+ FPGA platform. Results show that the implemented ME achieves 60 frames per second (fps) and 30 fps with a resolution of 3840\u00d72160 for ASIC and FPGA, respectively. Overall, the efficiency of the proposed ME system is significantly enhanced with the advancements discussed in this paper.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In order to address the issue of the target scale being altered, the target feature being too repetitive, or inconsistencies in cumulative errors in Kernelized correlation filters(KCF), a self-adaptive KCF tracking algorithm that utilizes multi-feature fusion has been proposed in this study. The KCF tracking algorithm has been enhanced through location prediction, bilinear interpolation, and multi-feature fusion. The aim of incorporating multi-feature fusion is to facilitate a more reliable representation of the target's appearance model and improve the robustness of target tracking. This has been achieved by integrating Hue Saturation Value (HSV), grayscale, and improved Histogram of Oriented Gradient (HOG) features. \n\nQualitative and quantitative evaluations of the proposed tracking method have been carried out on various object tracking benchmarks, and the results indicate superior performance compared to other state-of-the-art tracking methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep neural networks have shown impressive ability to handle hierarchical representations, which have increased their efficiency in speech and image recognition. However, the reliance on large-scale annotated data has restricted its development potential. This paper proposes a multi-layer convolutional neural network, drawing on prior knowledge of a knowledge graph to obtain features from finer grain text. The incorporation of prior knowledge not only enhances semantic information, but also reduces the model's dependence on large-scale samples, effectively addressing the limitations posed by the use of large data sets. With this approach, overfitting can also be avoided and classification accuracy can be improved. The proposed model has been applied to several large data sets with outstanding performance. In addition, experiments with TextCNN, a classic deep neural model, have confirmed the effectiveness of incorporating prior knowledge.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As Internet of Things (IoT) gains momentum in research and usage, there is an increased emphasis on the need for IoT standards. IoT is a critical factor in creating a Cyber Physical System (CPS) for Smart Factory, and for this reason, it is essential to establish an IoT system that adheres to the appropriate standard. Containers represent a virtualization technique that offers advantages such as quick deployment, adaptability, reliability and ease of management. This paper presents an architectural proposal that leverages container concepts in Smart Factory environments that meet IoT standards.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In real-time applications, a fast and robust visual tracker must possess a feature representation that is not only efficient but also has good discriminative capability. Appearance modeling should be adaptable to the variations of foreground and backgrounds. However, most tracking algorithms are unsatisfactory in both aspects. In this paper, a novel and efficient visual tracker is proposed by harnessing the feature learning and classification capabilities of extreme learning machine (ELM), an emerging learning technique. The contributions of this work are twofold. First, an ELM-AE-based feature extraction model is presented, providing a compact and discriminative representation of the inputs. Second, an ELM-based appearance model is developed for feature classification, allowing rapid distinction of the object of interest from its surroundings. Online sequential ELM is used to incrementally update the appearance model to cope with visual changes of both the target and backgrounds. Experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Benefit estimation is an essential component of introducing Advanced Process Control (APC) / Multi-variable Predictive Control (MVPC) / Model Predictive Control (MPC) to a process, as the cost associated with it must be justified in economic terms. The conventional approach to estimate benefits involves assuming a percentage reduction in the standard deviation of key controlled process variables, based on experience and process knowledge. However, this approach is ineffective due to the uncertainty associated with it.\n\nTo overcome this, a novel method was developed in this research to numerically estimate the percentage reduction in the standard deviation of key controlled process variables by modelling the disturbance(s) with application to APC. This method eliminates the need to assume the reduction in standard deviation, while the basic equation remains the same as the conventional approach. The effectiveness of the proposed method was demonstrated by implementing it in MATLAB on the real process plant data of a Delayed Coker Unit (DCU) in a petrochemical refinery that experiences cyclic disturbances.\n\nTwo simulation approaches were used to implement the proposed method: injecting the disturbance data directly and characterizing/modelling the disturbance pattern. Both approaches resulted in very close results when compared with the real plant data after APC was implemented. This confirmed the effectiveness of the proposed method for benefit estimation in introducing APC/MVPC/MPC to a process.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents an experimental investigation into the performance output of thermoelectric generators (TEGs). The study tests the effectiveness of different thermally conducting materials, including Al, Fe, and matte finish metal paint, in increasing the heat absorption of TEGs. A Peltier cell (TEC1-12706) is used in the thermoelectric set-up, which is designed to convert the ambient solar energy from the sun into electrical energy suitable for powering micro/small power devices.\n\nThe analysis of open circuit output DC voltages shows that paint-coated Al sheet increases the output voltage considerably. Specifically, a matte finish paint-coated Al sheet mounted on the hotter side of TEG is generating a maximum voltage of 0.76 V. Moreover, an array of three such TEG modules generated a maximum voltage value of 2.28 V, which is sufficient to power up low and ultra low power wireless sensor devices and modules. \n\nThis experimental set-up provides a promising alternative to power low power devices like WSN, hearing aids, wireless temperature sensors, etc., instead of using batteries, thereby enhancing their lifetime.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel outliers-robust constant false-alarm rate (OR-CFAR) detector, which is based on the truncated-maximum-likelihood estimator (TMLE), for Gaussian clutter in synthetic aperture radar (SAR) imagery. The proposed method aims to improve the detection performance in a multiple-target environment, where sea clutter samples are often contaminated by interfering target pixels, azimuth ambiguities, and breakwater. The OR-CFAR employs an adaptive threshold-based clutter truncation technique to eliminate high-intensity outliers in the local reference window, and accurately models the probability density function (PDF) of sea clutter using the newly proposed TMLE. The optimal truncation depth is selected carefully to improve the detection performance. The OR-CFAR is computationally efficient and has a great application value. Experimental results using Chinese Gaofen-3 SAR data demonstrate the superior detection performance of OR-CFAR over existing methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper explores the use of data-aided sensing as a cross-layer approach in Internet-of-Things (IoT) applications, where multiple IoT nodes collect measurements and transmit them to an access point (AP). The authors assume that measurements have a sparse representation due to spatial correlation, and they exploit the notion of compressive sensing (CS) for efficient data collection.\n\nTo enable data-aided sensing, the authors propose a node selection criterion that can efficiently reconstruct a target signal through iterations with a small number of measurements from selected nodes. They also introduce compressive random access (CRA) to collect measurements from nodes and propose compressive transmission request to efficiently send a request signal to a group of selected nodes. The authors conduct an error analysis on compressive transmission request and study the impact of errors on the performance of data-aided sensing.\n\nSimulation results demonstrate that data-aided sensing allows for the reconstruction of target information with a small number of active nodes and is robust to nodes' decision errors on compressive transmission request. Overall, this paper provides valuable insights into the potential of data-aided sensing and compressive sensing for efficient IoT data collection.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a progressive leaning approach to separate child speech from signals with mixed adult speech in a speaker-independent manner. Our approach is based on a densely connected long short-term memory (LSTM) architecture that addresses the limited training data issue in child speech. We measure the speech dissimilarities between children and adults using i-vectors, and demonstrate that distances between child and adult speech are large enough to warrant possible separation by establishing child and adult speech groups. \n\nOur novel LSTM design features densely connected hidden layers and stacked inputs, progressively obtaining intermediate targets that are learned via multiple-target learning for speech separation between child and adult groups. Experimental results on a simulation corpus demonstrate that our framework can yield consistent and significant gains of objective measures over the LSTM baseline for child speech separation. \n\nOur preliminary results on the SeedLing corpus with realistic recordings for child language acquisition show that our approach can achieve better overall separation performances than LSTM baseline when comparing spectrograms of separated speech. This implies our approach may have potential for speaker diarization involving child speech.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing is revolutionizing biological systems by providing integrated solutions that offer ownership, streamlined layout, configuration, quantification, and computerization. However, this transformation presents several security concerns that must be addressed. The foremost challenges in the cloud computing environment include trust assessment, multi-tenancy, and loss of control. This paper highlights recent developments and proposes a range of cloud security and assurance measures to mitigate these challenges. Additionally, it examines information security concerns in distributed computing and outlines strategies to tackle these issues effectively.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel approach to motion deblurring, which involves estimating the length and angle parameters in Point Spread Function (PSF) using the gradient descent method and an absolute gradient magnitude. The estimated parameters are then used to restore the blurred image to its original sharpness through the use of a wiener filter.\n\nThe experimental results demonstrate the effectiveness of the proposed method in terms of parameter estimation and image quality improvement. The Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM) were used as metrics to evaluate the efficiency of this method.\n\nOverall, this method offers a promising solution for motion deblurring, which can be further refined and applied in various imaging applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapid advancements in information technology, video surveillance systems have become a crucial component in the security and protection systems of modern cities. In particular, prisons are areas where surveillance cameras are ubiquitous. However, as the surveillance network expands, the cameras generate a massive amount of monitoring data, which poses significant challenges in terms of storage, analytics, and retrieval. To address this issue, many organizations are turning towards smart monitoring systems equipped with intelligent video analytics technology, which can monitor and pre-alarm abnormal events or behaviors. This technology is a hot research direction in the field of surveillance.\n\nIn this paper, we combine deep learning methods with the cutting-edge framework for instance segmentation, called Mask R-CNN, to train the fine-tuning network on our datasets. Our network can efficiently detect objects in a video image while simultaneously generating a high-quality segmentation mask for each instance. The experiments show that our network is simple to train and easy to generalize to other datasets. Furthermore, the mask average precision of our network is nearly up to 98.5% on our own datasets.\n\nIn conclusion, the use of intelligent video analytics technology in surveillance systems is a crucial aspect of modern security and protection. Our study has demonstrated the effectiveness of deep learning-based methods in enhancing the efficiency and accuracy of video surveillance systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The field of Data Science has brought together experts from a variety of disciplines to develop innovative methods, algorithms, and techniques in order to extract knowledge from vast amounts of data and solve complex problems. The ultimate goal is to discover hidden patterns within the data, which must be frequent enough to be learned by machine algorithms and reflect the standard knowledge that guides decisions. It is important to note, however, that the knowledge uncovered is not always free from societal prejudice, which can subsequently be perpetrated by machines.\n\nHowever, in domains such as high-stake decision-making, patterns that appear infrequently may warrant further investigation as they can be critical in making informed decisions. This presents a challenge, as determining the significance of infrequent patterns vis-a-vis frequent patterns is not an easy task. To address this issue, we have developed an ontology-based approach that augments the identification of these infrequent patterns.\n\nUsing a domain ontology, our approach guides the data mining process, aids in the selection of data subsets, and creates new views and attributes from existing data. By providing a background layer from which patterns of all frequencies can be gleaned, our approach allows infrequent patterns to stand out and become meaningful. In the oil production domain, the results of our system show promise in generating deeper insights into infrequent patterns that can lead to more informed decisions.\n\nOverall, our ontology-based data discovery approach is a significant development in the field of data science, providing a valuable tool for discovering knowledge in large datasets and enabling informed decision-making.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, there has been a significant shift in the way cyber defense is approached, moving from being reactive to proactive. This change is evident in the growing trend of sharing Cyber Threat Intelligence (CTI). There are currently many Open Source Intelligence (OSINT) sources that offer regular threat feeds, which are then fed into various analytical solutions. However, while these feeds produce a large amount of structured (STIX, OpenIOC) and unstructured (blacklists) data, they often lack the level of detail required to make informed decisions.\n\nMost indicators in these threat feeds are atomic in nature and volatile, such as IPs and hashes. This makes them insufficient to represent the behavior of an attacker or exploit. Furthermore, there is a lot of duplication in the information provided, and no single location can provide a comprehensive overview of a particular threat, requiring a lot of manual sifting through numerous sources. To address this issue, we have employed natural language processing to extract threat feeds from unstructured cyber threat information sources with up to 70% accuracy. Our methodology provides comprehensive reports in industry-standard formats like STIX, improving the quality and timeliness of CTI. By automating this task, we can help organizations stay ahead of both known and unknown threats and proactively defend against cyber-attacks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a novel approach for solving the trajectory tracking problem for autonomous vehicles. The approach uses a cascade control where the external loop applies a novel Takagi Sugeno-Model Predictive Control (TS-MPC) technique for position control, and the internal loop applies a Takagi Sugeno-Linear Quadratic Regulator technique designed via Linear Matrix Inequalities (TS-LMI-LQR) for dynamic control of the vehicle. Both techniques use a TS representation of the kinematic and dynamic models of the vehicle. Additionally, the paper introduces a novel Takagi-Sugeno estimator-Moving Horizon Estimator-Unknown Input Observer (TS-MHE-UIO) method, which optimally estimates the dynamic states of the vehicle as well as the force of friction acting on the vehicle to reduce control efforts.\n\nThe TS-MPC and TS-MHE-UIO techniques have the benefit of employing the TS model to solve the nonlinear problem as if it were linear, thereby reducing computation times by 10-20 times. To evaluate the potential of the TS-MPC approach, the paper presents a comparison of three methods for solving the kinematic control problem: Nonlinear MPC ,TS-MPC with compensated friction force, and TS-MPC without compensated friction force. \n\nOverall, the proposed approach offers a viable solution to the trajectory tracking problem for autonomous vehicles, where the TS model formulation of the vehicle allows for linear representation of nonlinear problems, thereby resulting in reduced computation times.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Context information such as a user's location, mental and health conditions, number of companions, etc., can be crucial in providing smarter services through smartphones. However, estimating this information with built-in smartphone sensors can be tricky. To address this issue, a user context estimation model based on mobile application usage histories has been developed in this paper. Since user application history and context are time-series data that change over time, a recurrent neural network is employed to learn the temporal relationship between them. The experimental results prove that the proposed model can accurately estimate user context from application usage history.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Currently, the construction, management, and maintenance systems of the backbone communication networks are relatively advanced. On the other hand, there are several access methods and differences in network service quality for the communication access network. The distribution service is closely linked to power production, and it is crucial that communication reliability and real-time performance meet high standards. Strict requirements are imposed on data transmission delay and loss, although data traffic, excluding video services, is generally low. The distribution service plays a direct role in production scheduling, and accordingly, it has tight demands on communication failure rates and failure repair times. Moreover, it expects to maintain a high level of operation and maintenance management. The distribution service is situated in the production control area and emphasizes information security. Therefore, the communication channel must have a reliable security protection system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud manufacturing (CM) is a platform that brings together distributed design, machining and assembly resources to provide seamless, adaptive and high-quality manufacturing services. Although CM offers great benefits in terms of resource discovery and planning, it lags behind in terms of robustness and security. This deficiency poses a significant challenge to the widespread deployment and adoption of CM, which is connected to the internet and vulnerable to cyber attacks. To address this problem, researchers are studying a particular type of equipment contention attack that can occur in CM. This attack involves the malicious grabbing of extra resources, causing other users to experience reduced efficiency. To mitigate this attack, a mechanism to measure the remaining machining capabilities in the cloud has been designed, allowing the cloud administrator to control the cost difference between the attacker and subsequent service requesters. Simulations of this approach on a mid-size city manufacturing cloud have shown that while benign users will experience only a slight increase in cost, this approach will discourage equipment contention attacks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, there has been a significant amount of interest in unmanned aerial vehicle (UAV) networks from both academic and industrial research communities. UAVs have been utilized in many civilian and military applications, including emergency communication in disaster scenarios. However, due to the high mobility, unstable links, dynamic topology, and limited energy of UAVs, localization and routing present significant challenges in UAV networks.\n\nTo address these challenges, we propose swarm-intelligence-based localization (SIL) and clustering schemes in UAV networks for emergency communication. Our first proposal is a new 3-D SIL algorithm based on particle swarm optimization (PSO), which uses the bounding box method to exploit the particle search space within a limited boundary. The algorithm measures the distance to existing anchor nodes to estimate the location of target UAV nodes. Our method results in improved convergence time and localization accuracy with lower computational cost.\n\nOur second proposal is an energy-efficient swarm-intelligence-based clustering (SIC) algorithm based on PSO. The algorithm uses a particle fitness function for intercluster distance, intracluster distance, residual energy, and geographic location to select cluster heads based on improved particle optimization. Our proposed SIC outperforms five typical routing protocols in terms of packet delivery ratio, average end-to-end delay, and routing overhead. Additionally, SIC consumes less energy and prolongs network lifetime.\n\nIn conclusion, our proposed SIL and SIC algorithms offer efficient solutions to address localization and routing challenges in UAV networks, particularly in emergency communication scenarios.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Security and consistency are key concerns in the design and maintenance of smart grids. One of the main issues facing these new power grids is the risk of bad data injection attacks, which can lead to power outages and provoke huge economic losses. Traditional bad data detection methods are often unable to identify new attack strategies, leaving the smart grid vulnerable to these attacks.\n\nRecently, an Adaptive Partitioning State Estimation (APSE) method has been proposed as a solution to combat these attacks. This method aims to increase the sensitivity of the chi-square test by partitioning the larger grid into smaller ones and applying the test on each partition individually. This process is repeated until the faulty node is identified.\n\nHowever, our simulation findings using the MATPOWER program reveal that the APSE method is not consistent. The method is sensitive to the size of the system and the location of the faulty nodes. Therefore, while the APSE method shows promise in combatting bad data injection attacks, further refinements must be made to ensure the method is consistent across various system sizes and fault locations.\n\nIn conclusion, ensuring the security and consistency of smart grids is critical to prevent disasters such as power outages and economic losses. While the APSE method offers a potential solution to bad data injection attacks, further research is essential to refine and improve the method's consistency.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We introduce a novel class-optimal algorithm that enables distributed computation of Wasserstein Barycenters over networks. Our method uses local interactions that adhere to the topology of the graph, allowing each node to reach the barycenter of all distributions held in the network. We prove that this algorithm achieves arbitrary relative precision in both the optimality of the solution and consensus among all agents for undirected fixed networks. Additionally, we provide an estimate for the minimum number of communication rounds required for successful implementation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accelerating deep neural networks on resource-constrained embedded devices is a critical challenge for real-time applications. While there has been extensive research on specialized neural network inference architectures, there has been a lack of study on deep learning inference acceleration and parallelization on chip-multiprocessor architectures that are commonly used for real-time applications due to their excellent energy efficiency and scalability.\n\nThis study investigates the strategies for parallelizing single-pass deep neural network inference on embedded on-chip multi-core accelerators. These methods exploit the noise-tolerance and elasticity features of deep learning algorithms to overcome the bottleneck of on-chip inter-core data movement, reducing communication overhead as the number of cores increases. The experimental results demonstrate that the communication-aware sparsified parallelization method improves system performance by 1.6\u00d7-1.1\u00d7 while achieving 4\u00d7-1.6\u00d7 better interconnect energy efficiency for various neural networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel autoregressive method for multi-speaker monaural speech separation utilizing deep learning. The approach capitalizes on causal temporal context in both the mixture and past estimated separated signals, enabling online separation that is compatible with real-time applications. The method utilizes a learned listening and grouping architecture inspired by computational auditory scene analysis, incorporating a grouping stage that effectively addresses the label permutation problem at both frame and segment levels. Results of experiments conducted on the WSJ0-2mix benchmark illustrate that this approach can achieve superior scores in terms of signal-to-distortion ratio and perceptual evaluation of speech quality when compared to the majority of the current state-of-the-art techniques for both closed-set and open-set evaluations. Combining these results with the fact that the approach requires fewer model parameters highlights the effectiveness of this method.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional neural networks (CNNs) have gained significant attention in recent years due to their accuracy in various applications such as image recognition and natural language processing. Although CNNs offer reduced computational costs compared to other neural network types, optimization of training and inference speed remains important. One of the main bottlenecks in CNNs is the time-consuming convolution operation. To accelerate this process, multiple algorithms have been developed. However, it is challenging to determine the best algorithm for a specific case since CNNs have various convolution parameter configurations and data type representations.\n\nIn this study, we evaluate the convolution algorithms provided by cuDNN, a library used by most deep learning frameworks for GPU operations. We examine the convolution parameter configurations used in popular CNNs and determine which algorithms are best suited for different convolution parameters, considering 32 and 16-bit floating point (FP) data representations. Our results demonstrate that the filter size and the number of inputs are the most crucial parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, specialized arithmetic units such as the NVIDIA Tensor Cores must be leveraged to achieve optimal performance.\n\nOverall, our study highlights the importance of selecting the appropriate convolution algorithm for specific CNNs and convolution parameter configurations, as it can significantly impact the overall performance of the training and inference process.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This chapter provides a comprehensive overview of Synchronous Optical Networking (SONET) and Synchronous Digital Hierarchy (SDH) in legacy networks. Despite some variations, many of the protocols in current networks still use their fault tolerance and survivability features. While SONET/SDH offered good characteristics such as fault tolerance and availability, it failed to meet the requirements for wavelength division multiplexing (WDM) in large area optical networks.\n\nThe widespread adoption of WDM architecture in large area optical transport networks (OTNs) is due to its ability to operate across multiple carriers and domains, enabling effective optical network management. With the OTN networking standard, digital signals can be transported independently of client-specific aspects, thus supporting ever-changing customer requirements. In conclusion, the SONET/SDH protocols may be prevalent in legacy networks, but for large area optical networks, the OTN standard provides more effective solutions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Most of the current approaches used for landslide inventory mapping (LIM) rely on traditional feature extraction and unsupervised classification algorithms. However, detecting landslide areas can be challenging due to the complexity and spatial uncertainty of landslides. In this letter, a novel approach based on a fully convolutional network with pyramid pooling (FCN-PP) for LIM is proposed. This approach has several advantages. Firstly, it is automatic and insensitive to noise due to the use of multivariate morphological reconstruction for image preprocessing. Secondly, it is able to take into account features from multiple convolutional layers and efficiently explore the context of the images, achieving a good tradeoff between wider receptive field and the use of context. Finally, the selected PP module addresses the drawback of global pooling employed by convolutional neural network, FCN, and U-Net, providing better feature maps for landslide areas. Experimental results show that the proposed FCN-PP is effective for LIM, outperforming the state-of-the-art approaches in terms of five metrics: Precision, Recall, Overall Error, F-score, and Accuracy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a computation offloading scheme for precedence-constrained tasks in a base station-assisted device-to-device (D2D) scenario for the information-centric Internet of Things (IC-IoT). In instances where the precedence among subtasks cannot be described as simple sequential or parallel relations in a task, the selection of task execution helpers for subtask offloading becomes complex due to the constraints of latency and resources. To minimize the time and financial cost of computation task offloading for the user, we define this type of precedence and optimize subtask-helper pairs. This problem is modeled as a dynamic generalized multi-resource-constrained assignment problem, and the optimal offloading policy is offered by searching minimum weight matchings in a bipartite graph. Our computer simulations indicate the proposed approach's effectiveness compared with the random helper selection and priority-based offloading scheme.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We are currently exploring a potential solution to reduce accidents among elderly drivers by implementing a system that identifies the minimum driving risk routes. A crucial aspect of this system is mapping the driving risk, particularly at intersections without traffic signals where frequent accidents occur. Therefore, it is essential to estimate the risk level of such intersections. The viewing environment, particularly the viewing angle, significantly impacts the accuracy of estimating the risk level. To address this, we aim to develop an automatic generating method that predicts the viewing angle based on the Google Street View utilizing the latest computer vision approach. Our approach involves predicting the viewing angle for identifying shield objects using Semantic Segmentation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, multihop wireless networks have emerged as a critical component of many Internet of Things (IoT) applications. Due to the limited resources of wireless nodes, maximizing the network lifetime has become a crucial issue. This paper proposes a novel approach to extend the lifetime of multihop wireless networks by appropriately distributing tasks among nodes.\n\nThe paper first introduces a centralized optimal task allocation algorithm for multihop wireless networks (COTAM). This algorithm models the problem of maximizing the network lifetime as a linear programming (LP) problem. However, COTAM is restricted to off-line optimization in known environments, as it requires knowledge of all network parameters in advance.\n\nTo extend the usability of the approach, a distributed optimal task allocation algorithm (DOTAM) based on Dantzig-Wolf decomposition is proposed. DOTAM divides the centralized LP problem into small-sized subproblems that can be executed independently by each node. This allows DOTAM to be used in unknown environments and promises to prolong the network lifetime.\n\nBoth COTAM and DOTAM are tested with artificially generated applications and a realistic application. The results demonstrate that DOTAM achieves similar performance to COTAM, and both methods outperform existing methods in extending network lifetime.\n\nIn conclusion, the proposed algorithms provide a promising solution to extend the lifetime of multihop wireless networks by optimally distributing tasks among nodes. COTAM can be used in known environments, while DOTAM extends the usability to unknown environments. The results show that the proposed methods outperform existing methods in extending network lifetime, offering a practical solution for many IoT applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Person re-identification (ReID) is an increasingly popular research field due to its potential to address the analysis and processing of massive monitoring data. However, learning discriminative information can prove challenging when individuals in the images are occluded, in large pose variations, or captured from different perspectives. Our solution to this problem is the proposed Joint Attention person ReID (JA-ReID) architecture, which utilizes a soft pixel-level attention mechanism and a hard region-level attention mechanism to learn two complementary feature representations. \n\nThe soft pixel-level attention mechanism employs a discriminative embedding for fine-grained information exploration by detecting salient parts in the feature maps. On the other hand, the hard region-level attention mechanism conducts uniform partitions on the convolutional feature maps for learning local features. In three popular benchmarks, namely Market1501, DukeMTMC-reID, and CUHK03, our method has achieved competitive results. \n\nAdditionally, the experimental results validate the adaptability of the joint attention mechanism to non-rigid deformation of the human body, which contributes to improving the accuracy of ReID.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cluster Heads (CHs) are critical nodes in Wireless Sensor Networks (WSNs) as they collect, aggregate, and transmit data to a sink node. CH failures can cause network partitions and degrade network performance, making CH fault tolerance a critical issue in WSNs. Existing fault tolerant mechanisms require additional resources or consume extra energy and time to detect and recover from failures. In this study, we present a Centralized Fault Tolerant Mechanism (CFTM) that can efficiently handle both permanent and transient failures. The proposed mechanism ensures the continuity of network operation despite CH failures. We evaluated the performance of our mechanism through simulations, and compared it to the LEACH and IHR protocols. The simulation results demonstrated that our CFTM outperforms its counterparts in terms of energy consumption, time required to tolerate failures, and data received at the sink node.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Soft robots are becoming increasingly popular due to their ability to operate in environments that are unstructured, rugged, and dynamic. However, the unique characteristics of soft robots can make them difficult to model, design, and control effectively. One of the challenges in exploring these obstacles is the lack of low-cost entry-level investigative model systems. In this article, we introduce a modular tensegrity structure-based soft robotics platform that is both low-cost and capable of untethered control. This platform can be scaled to various shapes and sizes, making it a versatile investigative tool. \n\nFurthermore, we demonstrate how unsupervised learning algorithms can be utilized to achieve vibration-based locomotion. These algorithms can provide a cost-effective and low-complexity solution for locomotion control in soft robotics. \n\nOverall, the development of this low-cost entry-level soft robotics platform based on modular tensegrity structures, in combination with unsupervised learning algorithms, has the potential to improve the exploration and understanding of soft robotics challenges.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rise of cloud servers, reversible data hiding in encrypted domains has become an important topic for managing encrypted media. Most existing reversible data hiding methods use stream ciphers that prioritize data storage security. However, homomorphic ciphers are more focused on data processing security, which is crucial in cloud and third-party platforms. As a result, reversible data hiding with homomorphic cryptography is gaining traction in research. This paper presents a novel lossless data hiding method that employs homomorphic cryptosystems. Once the content owner generates encrypted media with homomorphic ciphers, a data hider embeds secret data by establishing a mapping between secret bits and losslessly modifying encrypted media. The receiver can then extract the embedded data from the encrypted domain using a small auxiliary message. The original media can be retrieved by decrypting the marked encrypted media directly. During the embedding process, the modification of the encrypted media does not alter the corresponding original media, thanks to homomorphic and probabilistic properties. Therefore, no distortion is introduced during data embedding, and the decrypted media containing embedded data is identical to the original media. Additionally, the proposed method achieves a high embedding rate through efficient mapping and skilled use of expanded pixel values. The experimental results indicate that the proposed method has a higher embedding rate without distortion compared to state-of-the-art methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As the demand for flexible learning increases, exploring and expanding online learning opportunities is becoming increasingly important, particularly in supporting student learning. Peer Assisted Study Sessions (PASS) is an academic support program that is led by students, designed to help students make a smoother transition into university life and increase student retention rates. PASS is traditionally offered in challenging first-year core subjects. Due to the surging popularity of PASS, as well as limited space and scheduling availability, a synchronous online format (Blackboard Collaborate) was tested across three first-year subjects (Business, Nursing, and Psychology) at the University of Wollongong (UOW). The objective of the experiment was to compare student outcomes (in the form of final grades) for those enrolled in online and face-to-face (F2F) cohorts, and those who did not participate in PASS. The results revealed that participating in PASS sessions positively impacted student performance; students who attended PASS earned substantially higher grades than those who did not. Results also varied among subjects, with different outcomes for F2F versus online modalities. These varying results in each subject suggest that different factors may influence student success in the online space. This study aims to provide consideration of these factors, with directions for future research in this field.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The e-healthcare management system can be significantly improved by utilizing current trend technologies. One way to achieve this is by implementing a model of adaptable e-healthcare services administration framework that is based on Cloud Computing. To this end, the recommended approach involves designing a flexible e-healthcare services administration framework that takes advantage of distributed computing. The proposed framework is enhanced with different sections that contribute to developing a robust healthcare services system.\n\nThe Health Data Management System, which is based on the client-side, along with the use of a scalable healthcare cloud and application side, facilitates the generation of a readily attainable network. Additionally, the implementation of a biometric-based verification system in this context overcomes the limitations of nominal crimes and forgotten passwords associated with traditional nominal id secret key instruments used for providing security. This approach also provides a high level of accuracy in secure data access and recovery.\n\nIn conclusion, the proposed framework enhances cost management, time efficiency, and patient profile storage. By connecting to current trend technology, the e-healthcare management system can be transformed into a robust and efficient system that better serves the needs of patients and healthcare providers alike.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The IEEE 802.11ah protocol aims to provide network connectivity to a large number of end points while maintaining high network performance. One of the techniques used is the RAW scheme of MAC access, which allows STAs to contend in different RAW slots, reducing the chance of collisions.\n\nIn this paper, we evaluate the performance metrics of the RAW scheme and observe that it may be suitable for very small traffic conditions. However, at higher traffic loads, some nodes need to wait longer to transmit packets, and the network becomes unstable very quickly as traffic for each node is increased beyond two packets per second in our test condition.\n\nWe achieve a normalized network throughput of around 40%, which is not ideal. Therefore, we evaluate other important performance metrics, such as idle probability, backoff time, and transmission probability, and study them in detail. This study can help improve the performance of the RAW scheme and the overall network connectivity offered by IEEE 802.11ah protocol.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This research offers valuable contributions both in academia and practice. On a theoretical level, a hybrid harmony search (HS) algorithm called oppositional global-based HS (OGHS) has been proposed to address multi-objective flexible job-shop scheduling problems (MOFJSPs) with the objective of minimizing makespan, total machine workload and critical machine workload. To improve the distribution of individuals in the initial harmony memory, an initialization program based on opposition-based learning (OBL) has been developed. Moreover, the recursive halving technique that uses opposite numbers has been applied to reduce the neighborhood space during the searching phase of the OGHS. \n\nFrom a practical perspective, a dual vector code technique has been introduced that enables the OGHS algorithm to adapt to the discrete nature of MOFJSPs. Two practical methods have been implemented to solve MOFJSPs, namely Pareto optimality and technique for order preference by similarity to an ideal solution (TOPSIS). To evaluate the performance of the algorithm, different strategies including OBL and recursive halving have been employed, and the OGHS algorithm has been compared with previous algorithms presented in recent studies. The experimental results on representative examples demonstrate the effectiveness of the proposed OGHS algorithm in solving MOFJSPs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Given the growing attention towards Internet of Things (IoT) applications, firmware security has become increasingly crucial. Detecting bugs or vulnerabilities in firmware, however, is not a simple task, particularly in terms of information extraction regarding hardware architectures of compressed firmware. Traditional analysis tools often rely on static signatures embedded in compressed binary code, which require extensive expertise and may not always be feasible. In this paper, we describe our experience with analyzing compressed firmware hardware information, utilizing machine learning technologies due to the inability to utilize the semantic information of compressed binary code. Despite encountering several challenges, we were able to achieve positive experimental results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Space-Ground Integrated Network (SGIN) will play a vital role in the future development of our country. As such, it is crucial that we protect it from cyber-attacks. In this study, we propose an association analysis algorithm that utilizes a knowledge graph of cyber security attack events to portray potential attack scenarios for the SGIN. By constructing a knowledge graph and conducting association analysis, we are able to showcase the potential attack scenes in a graphical format. An important aspect in the construction of this knowledge graph is the creation of an event ontology, which represents various relationships within the network attack process. This will ultimately result in a space-ground integration network security analysis system that utilizes the knowledge graph of cyber security attack events and the association analysis algorithm to analyze potential attack scenarios.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Environmental conditions play a crucial role in determining human health and productivity. Laboratories, in particular, are spaces that are prone to various sources of pollution, which can lead to unhealthy indoor environments. Furthermore, laboratory activities such as experiments using thermography require the supervision of several parameters. Thus, there is a need for real-time monitoring systems that can identify bad ambient conditions and enable timely interventions for improved living environments and occupational health.\n\nThe proliferation of the Internet of Things (IoT) paradigm has great potential for creating automatic solutions for monitoring indoor environments. People spend about 90% of their time inside buildings, highlighting the importance of using real-time monitoring systems. In this context, this paper presents a laboratory environment conditions supervision solution based on IoT architecture called iLabM+. The solution consists of prototype hardware for collecting environment data and a web portal for data analysis and consulting.\n\nThe iLabM+ system allows monitoring of temperature, relative humidity, barometric pressure, and air quality. The results obtained are promising and represent a significant contribution to IoT-based environmental quality monitoring systems. Overall, the iLabM+ system has the potential to revolutionize the way we monitor and improve environmental conditions in indoor spaces, particularly in laboratory settings.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we explore the detection of false data-injection attacks in cyber-physical systems from the perspective of defenders. These attacks aim to modify sensor data and disturb the stability of CPSs while remaining undetected by the \u03c72 detector. To address this threat, we propose a new detector, named the summation (SUM) detector. Unlike the \u03c72 detector, the SUM detector utilizes current and historical information to detect false data-injection attacks. Additionally, its evaluation value follows \u03c72 distribution when the system is free of attacks, and the false alarm rate can be kept below any threshold by choosing an appropriate threshold value.\n\nTo further enhance the attack, we introduce an improved false data-injection attack that employs a time-variable increment coefficient. We evaluate the performance of the SUM detector against traditional and improved false data-injection attacks and demonstrate its effectiveness through simulation results.\n\nOverall, this paper presents a novel approach to detecting false data-injection attacks in CPSs with white noise that outperforms traditional \u03c72 detectors. The SUM detector presents a more robust and reliable method for detecting false data-injection attacks, thereby enhancing the security of CPSs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Optical Character Recognition (OCR) of Arabic text from manuscript images is considered to be extremely challenging due to the semi-cursive nature of Arabic script. Algorithms and image processing methods often struggle to decipher the intricacies of the script. To address this issue, we propose a method for exploring and researching the content of Arabic manuscript images. This method involves characterizing segmented handwritten words using a set of points of interest to identify and study the manuscripts. Each segmented word is described by key features and compared to query words.\n\nCompared to other methods, the results of this approach are highly encouraging, particularly in relation to the consistency of the handwriting style used in the text. Overall, this method offers a promising solution for researchers seeking to deepen their understanding of Arabic manuscripts.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We are tackling the issue of crosstalk-aware lightpath provisioning in a dynamic optical network that is spectrally and spatially modifiable. This network is created using multi-core fibers with distance-adaptive transmission, and spectral super-channels. Our main objective is to establish a lightpath for a connection that ensures inter-core crosstalk is not responsible for reducing the quality of transmission of any of the lightpaths it may cross. \n\nOur study compares two classes of schemes aimed at addressing inter-core crosstalk in the network, namely static and worst-case XT, and dynamic and precise XT estimation. Both methods are implemented in a dynamic XT-aware routing, spatial mode, and spectrum allocation algorithm, which efficiently solves the lightpath-provisioning issue with the goal of avoiding bandwidth blocking. \n\nOur simulation experiments demonstrate that the XT estimation method used is critical in network performance, with the precise XT approach being more effective than the worst-case XT strategy. We utilized extensive simulation experiments in realistic network scenarios to make the case for our study.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional Neural Network (CNN) has made remarkable strides in the field of Machine Learning (ML) services. While online ML services are common, offline ML services that rely on diverse CNN workloads are prevalent in small and medium-sized enterprises (SMEs), research institutes, and universities.\n\nEfficiently scheduling and processing multiple CNN-based tasks on SME clusters is essential but also challenging. Existing schedulers are unable to predict the resource requirements of CNN-based tasks. In this paper, we propose the QoS-guided dynamic scheduling framework, GENIE, for SME clusters that achieves users' QoS guarantee and high system utilization.\n\nOur proposed framework's QoS-guided scheduling strategy employs a prediction model derived from lightweight profiling to identify optimal placements for CNN-based tasks. We implement GENIE as a Tensorflow plugin and perform experiments using real SME clusters and large-scale simulations.\n\nThe experiments show that the QoS-guided strategy outperforms other baseline schedulers by up to 67.4% and 28.2% in terms of QoS-guarantee percentage and makespan. The proposed framework provides SMEs with an efficient and reliable means of managing their CNN-based workloads while maintaining their desired quality of service.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Visual Odometry (VO) is a crucial element in modern robotics and driver assistance systems. However, achieving real-time performance is essential for VO in such applications. Previous methods have mainly focused on improving accuracy, which results in longer runtime. In this study, we propose novel approaches for feature correspondence setup, outlier removal, and robust pose optimization in the VO pipeline to achieve real-time performance with a frame rate of almost 30 fps on a dual-core 3.5 GHz CPU while maintaining high accuracy. We introduce computationally efficient strategies to obtain an initial set of good features and rapidly filter out the outliers to minimize the computational overhead in later stages. Additionally, we put forward a depth-based weighting and saturated-residual scheme in pose optimization to enhance the robustness of VO. The experimental results show that the proposed VO achieves the fastest speed among all the top-ranked OV and SLAM systems on KITTI leader-board. Specifically, the proposed VO is 47% faster than state-of-the-art ORB-SLAM2, with comparable accuracy on the KITTI dataset.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a fast and efficient equivalent model for the prediction of the installed radiation pattern of patch antenna. The accurate prediction of radiation pattern on different platforms demands significant CPU time and memory cost. Nonetheless, the proposed model can save both these resources by reproducing a similar radiation pattern as that of the patch antenna. A code is formulated, which uses Green function derivation to determine the electric field of a magnetic dipole. The radiation pattern for near-field and far-field is computed and validated using a commercial software tool (FEKO). Based on the radiation mechanism, an equivalent model is constructed with only two design parameters, i.e., spacing distance between the dipoles in x- and y-directions. The height of the dipole is kept constant above the ground plane. This reduces CPU time and memory cost and makes the model more computationally efficient. After the optimization with FEKO, the installed radiation pattern is computed, and simulation results show that the proposed model can predict the installed radiation pattern of patch antenna mounted on a platform. The proposed model does not require detailed geometry and material information of the patch antenna.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The information-centric networking (ICN) approach has gained significant attention from the Internet of Things (IoT) community due to its ability to facilitate the dissemination of massive data produced by IoT devices. ICN's inherent multipath delivery and data naming scheme make it an ideal candidate for IoT data delivery. However, optimizing the multipath-oriented transmission control system in ICN-IoT is still a challenging task due to issues such as dynamic link conditions, on-path caching, and the randomness of request arrivals. In addition, the scalability and resource limitations of IoT require a lightweight and distributed control scheme. In this paper, we propose a distributed stochastic optimization framework for controlling the multipath transmission in ICN-IoT. We formulated the transmission control, including request scheduling and data rate regulation, as a stochastic concave optimization problem, which accommodates the randomness, unpredictability, and multipath delivery of ICN-IoT while maximizing the overall throughput. We then separated this problem into two subproblems: a request scheduling problem and a data rate control problem. To solve this, we designed a distributed alternating descent method (DADM) that enables each client to sequentially update the request schedule and rate regulation by communicating with links and providers they use. Our DADM has shown significant improvements in throughput, delay reduction, and energy efficiency compared to other state-of-the-art solutions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The integration of satellite and terrestrial networks (ISTN) is rapidly becoming a trend with the launch of tens of thousands of satellites planned by several commercial companies such as SpaceX and OneWeb. However, the scale of satellite networks is still relatively small compared to terrestrial networks, making IP-routing ability in satellite networks increasingly important for ISTN. Nevertheless, ISTN routing faces a host of challenges, including high topology dynamics, long propagation delay, and onboard processing limitations. To address these challenges, this paper presents an IP-based routing usability theoretical analytical model with calculable formulas that accommodate multiple network configurations and characteristics. Additionally, several experiments based on real satellite motion data were conducted to assess routing performance and challenges in ISTN, incorporating parameters such as ground station area, satellite processing ability, and number of satellites per orbit. Based on the analysis, the study concludes that despite the smaller number of routing nodes in satellite networks compared to terrestrial networks, its influence on overall ISTN performance is significant. As the network scale grows, high topology dynamics significantly decrease IP-based routing performance, reducing routing usable time in a day from 23.36h to 2.99h when the number of satellites grows from 60 to 960. Consequently, to sustain routing usability, the number of satellites in space should not simply be more, but instead, there should be a trade-off between network scale and onboard processing ability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: During disasters, it is crucial to dynamically discover people's concerns in order to effectively provide rescue and relief. To address this, we propose a social media-based framework for analyzing people's concerns, assessing their importance, and tracking dynamic changes. To better understand people's concerns across platforms and monitor dynamics, we have compared tweets and news reports in terms of these aspects and have made some interesting findings. Specifically, we studied the 2018 Camp Fire, the most destructive wildfire on record in California's history. Our findings reveal that despite high attentions towards the disaster, social media and news media focus on different aspects and exhibit different contents and dynamic changes in their concerns.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Fast and accurate detection of vehicles on road traffic scenes captured by traffic surveillance cameras is necessary for the large-scale deployment of automated traffic surveillance systems. Although the state-of-the-art techniques employ background modeling for low-complexity foreground detection, this remains a challenging problem as these methods need to be robust to varying road scene conditions, such as illumination changes, camera jitter, stationary vehicles, and heavy traffic, leading to significant computation cost. \n\nIn this paper, an effective, highly accurate, and low-complexity foreground (i.e., vehicle) detection technique is proposed, which can deal with the varying road scene conditions to create accurate pixel-level foreground masks in real-time. A novel robust block-based feature suitable for modeling road background and detecting vehicles as foreground is proposed, and Bayesian probabilistic modeling is employed on these features. \n\nExperimental evaluations conducted on widely used traffic datasets show that the proposed method can achieve comparable accuracy to the existing state-of-the-art techniques but at a much higher processing frame rate (40x speedup over PAWCS). The proposed system's real-time performance is demonstrated by implementing it on a low-cost embedded platform, Odroid XU-4, achieving a frame rate of over 80 frames/s, enabling the real-time detection of foreground objects in road scenes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Solving large-scale systems of linear equations is a crucial step in various algorithms in fields such as scientific computing and machine learning. However, when the problem size is significant, distributed computation is necessary to overcome computational and/or memory limitations. In this paper, we propose a new algorithm called Accelerated Projection-based Consensus, which addresses the scenario where a taskmaster distributes subsets of the equations among different machines/cores.\n\nOur algorithm updates each machine's solution by adding a scaled version of the projection of an error signal onto the nullspace of that machine's system of equations in each iteration. The taskmaster conducts an averaging over the solutions with momentum. We analyze the convergence behavior of the proposed algorithm and find that it compares favorably with the convergence rates of other distributed methods, such as distributed gradient descent, distributed versions of Nesterov's accelerated gradient descent and heavy-ball method, the block Cimmino method, and Alternating Direction Method of Multipliers. \n\nWe tested our proposed method on randomly chosen linear systems and real-world data sets and found significant speed-up compared to all other methods. Finally, our analysis suggests a novel variation of the distributed heavy-ball method, which employs a particular distributed preconditioning and achieves the same theoretical convergence rate as the proposed consensus-based method.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unknown input estimation is crucial in real-time monitoring, diagnosis, and fault-tolerance control. Therefore, this paper focuses on state estimation for a nonlinear system where some of its inputs are unknown. Specifically, the study is oriented towards an experimental application for a laboratory three-tank system affected by actuator faults. The main objective is to detect the faults using the unknown input observer (UIO) through synthesis and implementation. The results of simulations and experiments prove the effectiveness of the proposed observer.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Improved Modulated Signal Recognition Technology in Complex Electromagnetic Environment\n\nThe development of cognitive radio and radar electronic reconnaissance has created a significant need to improve the recognition ability of modulated signals in complex electromagnetic environments. In this paper, we propose a valid radar signal modulation recognition technology which can accurately recognize 12 different modulation signals, including Costas, LFM, NLFM, BPSK, P1-P4, and T1-T4 codes, even under low signal-to-noise ratio (SNR) conditions.\n\nTo achieve accurate recognition, we propose a new image fusion algorithm based on non-multi-scale decomposition to fuse images of a single signal with different time-frequency (T-F) methods. The weights are designed by the principal component analysis to combine the significant details of T-F images. Moreover, transfer learning-based convolutional neural networks and self-training-based stacked autoencoder are adopted to extract the effective information from fusion image, which further enhances the recognition performance.\n\nWe also use a multi-feature fusion algorithm to fuse features, which reduces redundant information on features and enhances computing efficiency. Finally, a classical algorithm called support vector machine is performed as the classifier. Simulation results show that the proposed recognition technology possesses good robustness and superiority in RSR with a wide range of SNR. The average recognition success rate is 95.5% at SNR of -6dB.\n\nIn summary, the proposed modulation recognition technology can effectively enhance the recognition ability of modulated signals in complex electromagnetic environments, even under low SNR conditions. The results show that our technology can achieve excellent recognition performance with high robustness, and can be widely applicable in various fields such as signal processing, communication, and electronic reconnaissance.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a distributed and efficient content caching scheme is proposed for improving the edge caching efficiency of the fog radio access network (F-RAN). The scheme is based on a combination of user preference prediction and content popularity prediction using a deep Q-learning network (DQN). To address the constraint imposed by limited storage capacity of each device, the optimization problem is formulated to maximize caching hit rate. The proposed scheme predicts user preference in an offline manner by applying popular topic models, while the online content popularity is achieved by integrating network topology relationship with the obtained user preference. Finally, the DQN-based content caching algorithm is proposed to achieve the optimal content caching strategy. Additionally, a content update policy is presented which takes into account the variations of contents popularity in a timely manner. Simulation results demonstrate that the proposed scheme outperforms existing algorithms in achieving better caching hit rate.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To tackle the increasing data traffic caused by the streaming of video-on-demand files, mobile edge caching/computing (MEC) is a solution that can be used to develop intelligent content caching at mobile network edges. This approach can alleviate redundant traffic and improve content delivery efficiency. Under the MEC architecture, content providers (CPs) store popular videos at MEC servers to enhance the users' quality of experience (QoE). Developing an effective content caching policy is crucial for CPs given the various content dynamics, unknown spatial-temporal traffic demands, and limited service capacity. However, it is often impossible to determine user preferences in advance. In this scenario, machine learning can be utilized to learn users' preferences based on historical demand information to determine the video files to be cached on MEC servers.\n\nThis paper proposes a multi-agent reinforcement learning (MARL)-based cooperative content caching policy for the MEC architecture when user preferences are unknown and historical content demands can only be observed. The cooperative content caching problem is formulated as a multi-agent multi-armed bandit problem, and a MARL-based algorithm is proposed to solve it. Simulation experiments based on a real dataset from MovieLens indicate that the proposed MARL-based cooperative content caching scheme can reduce content downloading latency and improve content cache hit rate significantly when compared with other popular caching schemes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Compressed sensing (CS) based techniques have proven to be a successful solution in acquiring the channels of millimeter-wave (mmWave) systems using a limited number of measurements. These methods, however, rely on a good understanding of the transmit and receive array manifolds, and also assume the presence of perfectly functioning antenna arrays at both ends of communication. In real-world scenarios, antenna imperfections can affect the geometry and response of array systems, leading to distorted CS measurement matrices and inaccurate channel estimations.\n\nThis paper explores the impact of both transmit and receive antenna imperfections on mmWave channel estimation, highlighting the errors that can occur due to faulty arrays. To address the challenge, the authors propose a relay-aided approach, which corrects for errors induced by transmit arrays.\n\nSimulation results demonstrate the efficiency and effectiveness of the proposed solution, showing that comparable channel estimates can still be achieved even in the presence of antenna imperfections, without requiring additional training overhead.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Information Systems (ISs) play a crucial role in streamlining operations and supporting processes in modern enterprises. As organisations increasingly recognise the importance of analytics in driving growth, the ability to perform data mining activities on the datasets managed by these systems is becoming more valuable than ever. However, due to the complexity of modern ISs, extracting the required data can be a time-consuming and challenging task. \n\nTo address this issue, this paper proposes a methodology and a pilot implementation designed to simplify the data extraction process by drawing on the end-users' knowledge and understanding of their specific IS. The approach taken in this study involves three key steps: Extract, Transform, and Load (ETL), which are recognised as necessary pre-processing steps for successful data mining activities.\n\nFirstly, the paper provides an overview of the existing ETL processes and techniques currently available, highlighting some of the challenges and limitations involved in using them effectively. Next, it outlines the key elements of the proposed methodology, which is built around reducing the time and effort required for data extraction while still ensuring accuracy and functionality.\n\nThe proposed methodology leverages the knowledge and expertise of the end-users to create customised ETL scripts that can successfully extract relevant data from their ISs. These scripts are designed to be simple enough for end-users to create and understand, yet powerful enough to retrieve the required data accurately, quickly and efficiently.\n\nFinally, the paper reports on the test results of typical data-extraction tasks performed on four commercial ISs. The results show that the proposed methodology delivers significant benefits in terms of time-savings, accuracy, and ease of use, giving organisations more capacity to focus on the analytics and insights generated by the extracted data.\n\nIn conclusion, this paper outlines a methodology for simplifying the data extraction process from complex information systems, by harnessing the knowledge and understanding of end-users. It highlights how this approach can lead to significant efficiencies and cost savings, while still ensuring data accuracy and functionality. With the increasing importance of data analytics to organisational growth, the proposed approach offers a valuable tool for streamlining data extraction and ensuring the effective use of ISs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces a robust optimal neuro-adaptive controller that can handle nonlinear systems with unstructured uncertainties while incorporating control barrier functions (CBFs) to ensure system safety in the presence of disturbances. The proposed controller is divided into three parts: feedforward, adaptive, and optimal. The unknown dynamics of the system are estimated by a joint neural network and concurrent learning adaptation mechanism (NNCL) to inform the adaptive term, while the optimal term employs an online quadratic program (QP) to generate the optimal signal and provide system stability via a control Lyapunov function (CLF). Safety conditions are created by incorporating CBFs and control bounds (CBs) constraints into the QP structure to bound the control effort. A robust term is added to make the proposed controller more resistant to disturbances and uncancelled uncertainty. Using Lyapunov synthesis, it is proven that the proposed QP-RCLBF-NNCL controller ensures uniformly ultimately boundedness of all system signals. The controller is validated on an inverted pendulum, and simulation results demonstrate its ability to achieve good tracking performance and model identification. Safety tests are performed to show that the proposed controller can bound the control signal and velocity by their predefined values when a disturbance acts on the system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Most community detection algorithms prioritize nodes with more connections, thus linking them within the same community. However, weaker ties between different communities are equally significant as they reflect relationships between different communities, such as helpful, friendly, negative, or adverse. Despite the importance of these weaker ties, few studies examine them. This paper introduces a sign prediction model by utilizing the nodes' features in the network, which include the Jaccard similarity and the ratio of negative degrees of all nodes, and the autoencoder technology that determines its loss function via the communities' traits. This model reduces the network into a low-dimensional space, enabling weak ties to be represented by low-dimensional vectors. Experimentation on the Epinions and Slashdot datasets demonstrates that the proposed model outperforms the current state-of-the-art graph embedding techniques in predicting weak tie signs, based on accuracy and F1 score measurements.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Radiomics is a fast-growing field that involves modeling the textural information that is present in different tissues of interest for clinical decision support. However, generating radiomic images can be computationally expensive and time-consuming, particularly for higher-order features like grey-level co-occurrence matrix (GLCM). To overcome this challenge, we have developed RadSynth, a deep convolutional neural network (CNN) model that can efficiently generate radiomic images.\n\nTo test the efficiency of RadSynth, we used it on a cohort of twenty-four breast cancer patients, including ten benign, ten malignant, and four normal patients. Specifically, we computed GLCM entropy images from post-contrast DCE-MRI images. RadSynth successfully produced synthetic entropy images that were comparable to traditional GLCM entropy images. The average difference and correlation between the two techniques were 0.07 \u00b1 0.06 and 0.97, respectively.\n\nOverall, RadSynth represents a powerful new tool for quick computation and visualization of the textural information in radiological images.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes adaptive channel coding and power control for practical free-space optical (FSO) communication systems. The first part of the paper assumes perfect channel state information (CSI), and proposes adaptive transmission schemes in which coding rate is adjusted independently or jointly with transmit power based on the channel conditions. An optimization problem is developed to minimize power consumption while satisfying FSO practical constraints, such as target bit-error rate, target outage probability, and maximum transmit power. Closed-form expressions for throughput and average transmit power are derived for both adaptive schemes over Gamma-Gamma atmospheric turbulence channels. The results show that the proposed adaptive schemes outperform non-adaptive schemes, particularly under strong turbulence conditions. \n\nIn the second part of the paper, the channel is not assumed to be perfectly known to Tx/Rx and is estimated by the sequence of received signals. The effect of channel estimation inaccuracies on the performance of the proposed schemes is investigated. The accuracy of the channel estimator depends on the length of the sequence and achieves performance close to the receiver with known CSI for a sufficiently large length of the observation window. Extensive analytical derivations are provided to investigate the performance of the proposed adaptive schemes under channel estimation error. The analytical results can be used to calculate and tune the optimum length of the observation window without resorting to Monte Carlo simulations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In order to improve the accuracy rate in diagnosing multiple-parameter faults of power electronic circuits, it is crucial to solve for the overlaps in the fault characteristic signals. This paper introduces a novel approach that can adaptively adjust the weights of each fault feature parameter based on its diagnostic accuracy rate to enhance their discriminability. By using the measured signal that has been decomposed via the variational mode decomposition and wavelet packet energy entropy (VMD-WPEE) to extract feature parameters, this approach takes into account the properties of multiple parameter faults in power electronic circuits. A classifier is selected and the threshold of the diagnostic accuracy rate is set, after which the weight of each fault feature parameter is adjusted until the classifier yields the desired results. Finally, the proposed approach is applied to other classifiers and feature extraction methods. The simulation and experimental results demonstrate the high accuracy, robustness, and advantages of this multiple-parameter-fault diagnosis method.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we introduce a deep learning-based framework for multiple description coding. Our proposed framework involves adaptively learning quantizers that minimize multiple description compressive loss. The framework is built on auto-encoder networks, which include a multi-scale dilated encoder network and multiple description decoder networks. We have also incorporated two entropy estimation networks, which learn to estimate the informative amounts of quantized tensors. These networks help supervise the learning process of the multiple description encoder network to more accurately represent the input image. \n\nIn an end-to-end self-supervised manner, we have learned a pair of scalar quantizers along with two importance-indicator maps. These quantizers are automatically learned and accompanied by importance-indicator maps that determine the relevance of the encoded features. To further enhance the diversity of the generated multiple descriptions, we have imposed a multiple description structural dissimilarity distance loss on the pixel domain decoded images. This is in addition to the multiple description reconstruction loss.\n\nOur proposed method outperformed several state-of-the-art multiple description coding approaches in terms of coding efficiency, as verified through testing on two commonly used datasets. Overall, our deep multiple description coding framework demonstrates great potential for improving the efficiency and quality of multiple description coding.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The behavior of APT attacks has been a hot topic in recent network security studies. Understanding the implementation principles of APT attacks is critical in safeguarding networks. In this paper, we analyze the behavior of APT attacks in the Ngay campaign from two perspectives: network traffic and code implementation. \n\nFirstly, we use traffic analysis to establish the attack chain. Subsequently, we detail the process of vulnerability exploitation via reverse code analysis. We then illustrate the building of a backdoor in the system. Finally, we discuss the obfuscation technology utilized in APT malware samples. \n\nBy studying the Ngay campaign, we have gained great insights into APT attack techniques. The knowledge we have acquired can be applied to strengthening the cybersecurity of networks, further ensuring the safety of critical data.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Melanoma skin cancer is one of the most deadly forms of cancer, responsible for thousands of deaths each year. The manual process of diagnosing melanoma is time-consuming and difficult. Therefore, researchers have introduced several computerized methods for recognition to improve the accuracy of the diagnostic process, which is helpful for dermatologists. \n\nIn this paper, we propose an automated system for skin lesion classification using transfer learning-based deep neural network (DCNN) feature extraction and kurtosis controlled principle component (KcPCA) based optimal feature selection. We utilize pre-trained ResNet deep neural network models, such as RESNET-50 and RESNET-101, for feature extraction. Then, we fuse their information and select the best features, which we later feed to a supervised learning method, such as SVM of radial basis function (RBF), for classification. \n\nWe utilize three datasets, HAM10000, ISBI 2017, and ISBI 2016 for experimental results, achieving an accuracy of 89.8%, 95.60%, and 90.20%, respectively. The overall results show that the performance of the proposed system is reliable compared to existing techniques. \n\nIn conclusion, our proposed system enables automated classification of skin lesions, which can aid dermatologists in diagnosis, leading to earlier detection and treatment of melanoma skin cancer, ultimately reducing the number of deaths caused by this deadly disease.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Existing face hallucination methods aim to enhance low-resolution face images to their high-resolution counterparts using exemplar datasets. However, the ambiguity in mapping low-resolution facial details to high-resolution can result in distorted facial features and incorrect attributes like gender reversal and rejuvenation. To address this issue, we propose the use of additional facial attribute information in upsampling networks to reduce ambiguity in face super-resolution. We developed an attribute-embedded upsampling network comprising an autoencoder with skip-connections and a discriminative network. The autoencoder incorporates facial attribute vectors into residual features of low-resolution inputs, while the deconvolutional layers are used for image upsampling. The discriminative network verifies the presence of desired attributes in the super-resolved faces, and the loss is used to train the upsampling network. Our method can super-resolve tiny unaligned face images (16x16 pixels) with a large upscaling factor of 8x while reducing the uncertainty of one-to-many mappings remarkably. Extensive evaluations on a large-scale dataset demonstrated that our method outperforms the state-of-the-art in face hallucination results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the densification of wireless networks, there are now abundant idle computation resources available at edge helpers such as base stations and handheld computers. These resources can be used to offload heavy computation tasks from small IoT devices like sensors and wearable computing devices in proximity, thus extending their battery lives and overcoming their limitations. However, these spare resources offered by edge helpers are random and intermittent, unlike dedicated servers. Therefore, it is essential to intelligently control the user's data offloading and local computing amounts to ensure that computation tasks are completed efficiently while consuming minimum energy.\n\nIn this paper, energy-efficient control policies are designed for a computation offloading system with a random channel and a helper with a dynamically loaded CPU due to the primary service. The policy adopted by the helper determines the sizes of offloaded and locally computed data for a given task, such that the total energy consumption for transmission and local CPU is minimized under a task-deadline constraint. These policies provide an offloading user with robustness against channel-and-helper randomness while balancing offloading and local computing.\n\nBy modeling the channel and helper CPU as Markov chains, the problem of offloading control is converted into a Markov decision process. Though dynamic programming (DP) for numerically solving the problem does not yield optimal policies in closed form, this procedure is leveraged to quantify the optimal policy structure, and the results are used to design optimal or sub-optimal policies. For three cases ranging from zero, small to large helper buffers, the low complexity of the policies overcomes the \"curse of dimensionality\" in DP arising from the joint consideration of channel, helper CPU, and buffer states.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we introduce a novel data-driven steganography approach known as \"generative steganography by sampling\" (GSS). Our scheme uses a powerful generator to directly sample the stego image, unlike traditional modification-based steganography that requires an explicit cover. Both parties share a secret key for message embedding and extraction. We evaluate the security of generative steganography using the Jensen-Shannon divergence. \n\nWe propose a practical generative steganography method that employs semantic image inpainting. We write the message in advance to an uncorrupted region that needs to be retained in the corrupted image. The corrupted image with the secret message is then fed into a generator trained by a generative adversarial network (GAN) for semantic completion. To penalize message extraction errors and unrealistic stego images, we introduce message loss and prior loss terms. \n\nOur design involves training a generator to generate new data samples from the same distribution as the existing training data. Backpropagation to the message and prior loss is then used to optimize the input noise data coding of the generator. We conducted experiments to evaluate the potential of the proposed framework based on qualitative and quantitative evaluations of the generated stego images. Our results indicate that GSS is a promising steganography approach that can be used to conceal data in images.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Local binary descriptors are powerful visual cues for representing features, providing discriminative information about small appearance details in local neighbourhoods. This makes them robust to local changes in databases such as illumination, identity, and expression. However, existing local descriptors are not discriminative enough to estimate the relationship between two individuals due to learning feature codes individually and the requirement of previous knowledge for hand-crafted features. \n\nTo address this issue, we propose an effective method called Context-Aware Local Binary Feature Learning (CA-LBFL) for kinship verification. This approach learns contextual features directly from raw pixels, eliminating the need for hand-crafted features. Our experimental results demonstrate that the proposed method achieves competitive results compared to other state-of-the-art methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: When attempting to identify a specific module in a dynamic network, one must typically formulate a multi-input single-output (MISO) identification problem. This involves identifying all modules in the MISO structure and determining their model order. However, this can be computationally challenging in large networks, and can result in estimating a large number of parameters that are of no interest. \n\nTo address these challenges and increase the accuracy of identified modules, a regularized kernel-based method was employed. By keeping a parametric model for the module of interest and modeling the impulse response of remaining modules as zero mean Gaussian vectors with a covariance matrix (kernel) given by the first-order stable spline kernel, noise affecting the target model output is accounted for. \n\nUsing an Empirical Bayes (EB) approach, the target-module parameters can be estimated by maximizing the marginal likelihood of the module output. This approach is solved using the Expectation-Maximization (EM) algorithm. Numerical experiments demonstrate that this method has great potential in comparison to other state-of-the-art techniques for local identification.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Spell Checker is a crucial tool that forms an integral part of language-specific text processing applications. Traditional methods for detecting errors, such as dictionary lookup techniques and minimum edit distance, have been utilized by various available spell checkers. However, there is still room for improvement in terms of both performance and accuracy. \n\nTo address these limitations, we propose a novel hybrid approach for Punjabi language spell checker. In this paper, we present the use of trie data structure to store Punjabi words dictionary, which allows for efficient error detection. The tree-based algorithm, combined with n-gram analysis, is then utilized to detect misspelled words. \n\nTo correct these misspelled words, we adopt the Long-Short Term Memory (LSTM) recurrent neural network along with a rule-based approach and minimum edit distance. This allows for a more accurate selection of the best possible suggestion to replace the misspelled word. \n\nIn addition to these approaches, our proposed spell checker also incorporates handcrafted rules, language syntax rules, and rules regarding tokenization, further improving its accuracy. \n\nOverall, our hybrid approach for Punjabi language spell checker offers a significant improvement over existing spell checkers, both in terms of performance and accuracy. By using trie-based dictionary for error detection and LSTM-based approach for error correction, we have created a spell checker that is both efficient and effective.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is widely acknowledged that many companies and individual users are hesitant to adopt cloud computing due to concerns around security and privacy. The nature of cloud services and environments creates larger attack surfaces, making it vital to implement effective and flexible access control methods that take into account the contextual elements involved in data access requests. To address this issue, this study proposes the use of PaaSword, a unique holistic access control framework that functions as a PaaS offering. This framework extends the popular XACML standard while providing semantic reasoning capabilities that support the federation of context-aware access control policies with minimal effort. To assess its efficiency, a comparative evaluation test was conducted against the WSO2 Balana engine, a well-known reference implementation of the XACML standard, and the results are discussed.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Gateway selection is a critical issue for hybrid mobile ad hoc networks (MANETs). Currently, independent and random selections, without supporting a routing negotiation protocol, may lead to overloaded links or gateways when selected by multiple nodes simultaneously. Therefore, the network's performance may not be optimal. In addition, the ad hoc nature of MANETs makes topology changes dynamically, making the gateway selection even more challenging. \n\nTo address this issue, this paper presents a mathematical model for gateway capability and a novel approach called bio-inspired gateway selection. This approach selects gateways based on the network's status and is associated with a cooperative mechanism for optimization. The novelty includes the use of an attractor selection model, self-adaptability, and autonomy of the biological system.\n\nThe proposed approach's performance is evaluated through simulations with varying scenarios, and its results are compared with conventional approaches currently used in hybrid MANETs. The numerical results illustrate the proposed approach's performance in terms of packet delivery ratio, average delivery latency, normalized routing overhead, and gateway load balance under different network conditions. Furthermore, the numerical results demonstrate significant performance gain compared to conventional gateway selection approaches. \n\nOverall, the bio-inspired gateway selection approach offers a promising solution to the challenge of gateway selection in MANETs. The use of biological systems' characteristics, such as self-adaptability and autonomy, could lead to robust and efficient network performance, even under dynamic topology changes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The task of CCR is to identify potential citations for a set of target entities with priorities from a stream corpus that is temporally-ordered. Earlier efforts in CCR that created an individual relevance model for each entity were unable to address new entities without annotation. Another attempt was to build a global entity-unspecific model that failed to consider relationship information among entities, thus resulting in inadequate performance. Additionally, previous methods were unable to effectively utilize prior knowledge in entities or documents due to indifferently considering all features. In this paper, a new entity and document class-dependent discriminative mixture model is proposed by introducing an intermediate layer to model the correlation between entity-document pairs and hybrid latent entity-document classes. This model provides better adaptation to different types of entities and documents and achieves superior performance in dealing with a broad range of entity and document classes. Experiments conducted on two official datasets demonstrate that the proposed model achieves the state-of-the-art performance.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the semiconductor industry, cost reductions and yield enhancement have become important areas of focus. Advanced machines collect a large amount of sensor data during the manufacturing process, known as status variables identification (SVID), which can be used for data-driven automatic fault detection and diagnosis. However, the variable processing times of wafers result in variable-length SVID data, making it challenging to detect and diagnose faults accurately.\n\nTraditional approaches to this problem involve using condensed fault detection and classification (FDC) data obtained through manual feature extraction or the assumption that all wafers have the same processing time, which is not accurate. To address this issue, we propose a self-attentive convolutional neural network (CNN) that can detect and diagnose faults directly from variable-length SVID data.\n\nWe conducted experiments with real-world data from a semiconductor manufacturer to evaluate our proposed model's performance. Our model outperformed other deep learning models with less training time and demonstrated robustness at different sequence lengths. Compared to FDC data, SVID data showed better fault detection performance, enabling the identification of abnormal sensor value patterns without requiring manual investigation of lengthy sensor signals.\n\nIn summary, our self-attentive CNN model has demonstrated promising results in detecting and diagnosing faults directly from variable-length SVID data in the semiconductor industry. It represents a significant step towards maintaining competitive advantages through data-driven automatic fault detection and diagnosis.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The prediction of protein subcellular location has emerged as a significant area of interest in recent years because of its ability to aid in comprehending disease mechanisms and drug design. The field has witnessed substantial growth due to the advent of automated microscopic imaging technology, which enables proteins' distribution to be represented vividly and in detail.\n\nThis study proposes a novel method for predicting protein subcellular location based on multi-view image features. The features were extracted from three different views, including the four texture features of the original image, the global and local features of the protein extracted from the protein channel images after color segmentation, and the global features of DNA extracted from the DNA channel image. The extracted features were then combined to enhance the performance of subcellular localization prediction.\n\nTo determine the best ensemble features, we compared the performance of different combination features under the same classifier. The results demonstrated that the proposed method based on Stacked Auto-encoders and the random forest classifier outperformed other methods. The deep network was combined with traditional statistical classification methods to improve the prediction results significantly.\n\nThe efficacy of the proposed method was evaluated using stringent cross-validation and independent validation tests on the benchmark dataset. The results confirmed the efficacy of the proposed method, demonstrating its potential application in the prediction of protein subcellular location.\n\nIn conclusion, the proposed multi-view feature extraction method based on deep network and random forest classifier provides a significant contribution to the field of protein subcellular location prediction. The method can be used to enhance and refine the identification of the subcellular location of proteins in microscopic images, which has important implications for understanding disease mechanisms and designing novel drugs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose an approach to optimize both hardware and software parameters for efficient implementation of dense stencil computations on programmable general-purpose computing on graphics processing units (GPUs). Firstly, we introduce an analytical model for accelerator architecture's silicon area usage and workload characterization of stencil computations. We then combine this characterization with a parametric execution-time model and formulate a mathematical optimization problem to maximize the objective function of all hardware and software parameters. By solving the codesign problem, we propose architectural variants of NVIDIA Maxwell GTX-980 and Titan X, specifically tuned to 2-D stencils (Heat, Jacobi, Laplacian, and Gradient) and 3-D ones (Heat and Laplacian), respectively. Our model predicts a potential performance improvement of up to 28% (respectively, 33%) by tweaking hardware parameters such as the number of streaming multiprocessors, compute cores, and shared memory size. Furthermore, we develop insights into the optimal regions of the design landscape.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this article, we explore the use of an adaptive neural output consensus control approach for stochastic nonlinear strict-feedback multi-agent systems (MASs). The proposed method employs a combination of the traditional backstepping framework, graph theory, and neural networks (NNs) technology. The NNs are utilized for the purpose of approximating unknown functions, while It\u00f4\u2019s lemma is used to deal with stochastic dynamics of the system. \n\nIt is demonstrated that by employing this methodology, all signals remain bounded in probability, and the tracking errors of all followers converge to a small neighborhood of the origin in the sense of mean quartic value through the appropriate choice of parameters. \n\nTo validate the effectiveness of the approach, a simulation example is provided. The results demonstrate that the proposed method is capable of effectively controlling the stochastic nonlinear strict-feedback MASs, with the tracking errors for all followers achieving convergence to a small neighborhood around the origin. \n\nOverall, the presented technique provides a powerful approach to address complex control issues in stochastic nonlinear strict-feedback multi-agent systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the proliferation of mobile devices equipped with cameras, the amount of images has seen an exponential growth. Deep learning with convolutional neural networks (CNN) has become an effective way to process these images. However, deploying the CNN model on mobile sensors, which are resource-constrained, is challenging. Cloud computing, while popular, has issues with data security and response latency. Therefore, this paper proposes a novel lightweight framework for privacy-preserving CNN feature extraction based on edge computing. Secure interaction protocols and two edge servers are utilized for collaborative CNN feature extraction. The scheme significantly reduces latency and end device overhead while preserving privacy. Security, effectiveness, and efficiency of the proposed scheme are demonstrated through theoretical analysis and empirical experiments.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Power-over-fibre (POF) has demonstrated its versatility in powering microelectronic devices in hazardous environments, telecommunication and smart grid applications, as well as in reducing the vulnerability of surveillance cameras to tampering. The technique also holds great potential in the development of all-optical sensor networks. However, the transmission of power via optical fibre is limited by various processes such as stimulated Brillouin scattering (SBS). Therefore, this study aims to optimise the power delivery to different parts of the optical fibre network, thus minimising the impact of processes like SBS. This can be achieved by enhancing the performance of the photovoltaic microcell or photovoltaic power converter (PPC) and tailoring it to meet the specific power requirements of sensors or actuators distributed throughout the network.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Telecommunications Engineering program at Macquarie University is currently undergoing a renewal. The School of Engineering is transforming its pedagogy while Macquarie University is restructuring its curriculum. This study aims to examine the impact of the changes in Telecommunications Engineering education at the university.\n\nThe changes include an update in technical content using a project-based learning (PBL) approach, project ownership, the replacement of traditional lectures, virtual laboratories, and an emphasis on software tools and programming skills. This means students will learn by working on real-world problems, taking ownership of projects, and developing the skills they need to succeed in the industry.\n\nThe transformation of the pedagogy is expected to bring a positive impact on students' learning experience. The PBL approach will enable students to gain hands-on experience in addressing complex problems. Students will become actively engaged in learning as they work to solve practical problems, promoting the development of analytical and problem-solving skills.\n\nMoreover, the use of virtual laboratories and software tools should enhance the students' learning experience. Instead of learning through simulations, students will have the opportunity to work with real-world problems, use real tools, and gain industry experience. The focus on programming skills will also give students a valuable set of skills in the job market.\n\nIn conclusion, the renewal of Telecommunications Engineering education at Macquarie University is a positive move. The changes in pedagogy and curriculum structure will provide students with a more meaningful learning experience, preparing them for success in the industry. The use of project-based learning, virtual laboratories, and software tools will enhance the students' ability to solve problems, while also improving their programming skills.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the Internet of Things (IoT), billions of physical devices are interconnected to provide a real-time view of the world. To leverage the capabilities of these devices, service-oriented computing enables the abstraction of their functionality as IoT services - delivered on-demand to users. This concept improves the competitive edge of service providers who can tailor their services to match users' requests through a negotiation process, with the specified provisioning described in a service level agreement (SLA). An SLA also ensures that the quality of service (QoS) is monitored and guaranteed. \n\nHowever, SLA negotiation in the IoT faces challenges such as the scale and dynamics of the environment, which are not considered in existing SLA negotiation approaches that are focused on cloud computing. Therefore, this paper proposes a context-based negotiation strategy to evaluate offers and generate counteroffers. This strategy is integrated with the WS-Agreement Negotiation standard and has been designed to consider the domain-specific properties of IoT services. \n\nEvaluation results show that the proposed negotiation strategy produces a higher utility and maintains a good success rate compared to other negotiation strategies. Hence, this study effectively addresses the challenges in SLA negotiation in the IoT, providing a scalable and dynamic approach to provision IoT services while ensuring QoS.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The ever-increasing amount of data generated by smart devices has led to a high demand for an efficient and reliable platform capable of handling enormous data traffic. To address the challenges posed by the heavy load on the cloud infrastructure and the need for real-time applications with low latency, fog/edge computing has been introduced. However, the capability of edge devices is not on par with the cloud, thereby creating performance and reliability issues that require the implementation of a management platform to overcome these challenges and improve the overall service availability.\n\nIn this paper, we propose the design of a distributed edge computing platform that provides a simple management solution for remote distributed edge nodes. Our platform includes vital features such as service auto-discovery, lightweight container provisioning, GPU container support for advanced deep learning applications, secure remote deployment of edge analytics applications, joint edge/cloud data communication and processing, and management of all active end-nodes without fixed IP addresses.\n\nTo guarantee high availability of edge services and minimize service downtime, we propose a real-time internal and external container migration scheme. This scheme ensures cooperative processing, data backup, load balancing, and service failover. Our solution provides lightweight service management, low overhead, secure and flexible networking, and a significant reduction in latency compared to existing edge platforms.\n\nOverall, our proposed distributed edge computing platform is an effective solution for managing and improving the performance and reliability of edge nodes. It enables the realization of real-time applications with low latency and ensures high availability of edge services.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The requirements for the Certified Accountant (CC) profession are legally defined and the profession is recognized as being of public utility. However, the increasing demands on these professionals have forced them to acquire new skills and undergo comprehensive training to meet the expectations of the industry. As a result, Higher Education Institutions (HEI) have updated their training programs to cater to these requirements.\n\nTo address the evolving needs of accounting students, the SimEmp - Contabilidade web application has been introduced. This application is designed to encourage student-centered learning, with a focus on \u201clearning by doing\u201d approach. The web-based application makes use of active teaching methodologies based on Project Based Learning (PBL), enabling students to simulate a virtual business environment that is geared towards equipping them with the requisite skills and competencies required in the accounting, financial, and tax fields. \n\nIn addition, the web application puts other essential abilities of the students through the test, such as their ability to manage stress, time, and work collaboratively in a team. This enables students to prepare for real-world scenarios where they may have to multi-task and work in a high-pressure environment.\n\nIn conclusion, the SimEmp - Contabilidade web application is a valuable tool in enhancing the training and development of accounting students. By promoting a student-centered approach to learning, the application is setting the foundation for the next generation of highly-skilled and competent certified accountants.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Managing the increasing amount of electricity information provided by smart meters is a significant challenge for the modern era, especially in the residential sector, where maintaining records of consumer consumption patterns is crucial. Retailers and utilities must provide consumers with more effective demand response programs to handle uncertainties in their consumption patterns. To manage consumer behavior and their unprecedented high volume of data, a deep neuro-fuzzy optimizer has been introduced to optimize load and cost effectively. The optimizer uses three premise parameters: energy consumption, price, and time of the day, and two consequent parameters: peak and cost reduction. The dataset used for optimization is obtained from the Pecan Street Incorporation site, and Takagi Sugeno fuzzy inference system is used to evaluate the rules developed from the membership functions of the parameters. Gaussian membership functions are chosen for continuously monitoring consumer behavior. Simulations validate the proposed energy optimizer's robustness in cost optimization and energy efficiency.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The problem of achieving leader-following attitude consensus among multiple rigid body systems over switching networks has been successfully addressed using the distributed observer approach. However, to implement this control law, every rigid body system must be aware of the exosystem dynamics that generates the desired angular velocity. This paper aims to propose a solution to the same problem through the adaptive distributed observer approach. Unlike the previous approach, the adaptive observer approach is capable of delivering an estimation of the exosystem dynamics to each rigid body system. This yields a fully distributed control law that can address the leader-following attitude consensus problem of multiple rigid body systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As part of ongoing research on agarwood oil quality classification, this paper presents a non-linear SVM model using polynomial as the kernel parameter. The study involved analyzing 96 agarwood oil samples of different high qualities. The input for the SVM model was the abundances (%) of volatile and the output was the classification of agarwood oil qualities as low or high. The experimental work was performed using MATLAB software version R2016a. \n\nThe results indicate that the polynomial tuning kernel parameter is effective in classifying agarwood oil volatile to high and low qualities. The study achieved 100% accuracy, as supported by the confusion matrix, sensitivity, precision, and specificity. This finding is significant and can benefit future research in the agarwood oil field.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Smart meters are primarily designed to provide energy readings for billing purposes. However, the potential of the data collected by these devices could be leveraged to manage future distribution grids effectively. In this paper, a multilevel state estimator is presented that utilizes smart meter measurements for monitoring low and medium voltage grids. The primary aim of this paper is to introduce an architecture that can integrate smart meter measurements efficiently and demonstrate the achievable accuracy performance when utilizing realtime smart meter measurements for state estimation purposes. The state estimator design utilizes the uncertainty propagation theory to integrate data at different hierarchical levels. A cloud-based infrastructure coordinates the estimation levels, provides the interface to auxiliary functions, and enables access to the estimation results for other distribution grid management applications. Mathematical analysis characterizes the accuracy of the estimation algorithm and demonstrates the possible performance at different levels of the distribution grid when using the smart meter data. Simulation results validate the analytical results and demonstrate the operation of the multilevel estimator coordinated with the cloud-based platform.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Dynamic hand gesture recognition is a highly researched topic that involves the use of various sensors to capture hand movement patterns. However, the initial data collected is in the form of raw data, which requires advanced processing techniques to obtain features that can be trained using different classifiers. Without feature selection techniques, the accuracy of pattern recognition diminishes during classification.\n\nTo address these shortcomings, this study presents initial processing examples that produce relatively good features for the data training process. The study utilizes the Leap-motion sensor to record dynamic hand movements and uses a Gaussian mixture model along with predictor selection for the classification process. The study uses three dynamic hand gestures and 4609 coordinates, spread across 30 features.\n\nComparing the results obtained by the k-NN classifiers with Euclidean distance metric, the study observes that the feature selection process demonstrates better accuracy in recognizing hand gestures. The results of the study reveal that the lowest prediction, with and without a feature prediction process, has a range slightly between 99.7% to 100%.\n\nIn conclusion, this study proposes a method for improving hand gesture recognition accuracy using robust feature selection techniques, which is essential for the successful classification of hand movement patterns.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper aims to address a scheduling problem involving two competing agents on a single machine. The objective of the first agent is to minimize mean lateness, while the second agent aims to minimize the number of tardy jobs. As this problem is known to be computationally difficult, an efficient algorithm known as SPT-M is proposed to generate non-dominated solutions of the Pareto set. Our computational results indicate that the SPT-M algorithm can generate an efficient Pareto frontier in a short amount of computing time. The findings of this study could assist practitioners in determining optimal tradeoffs between jobs of two competing agents vying for a shared resource.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the advent of RGB-D sensors, obtaining high-quality color point clouds has become more convenient. Color information can be used to complement geometrical data and improve point cloud registration accuracy. This paper presents a registration method that adaptively combines color moment information to improve accuracy. Using central moments, we characterize the color distribution of two point clouds, then build correspondences by combining both geometric and color moment features of each point. The resulting corresponding points satisfy both geometric and color similarity. Finally, we use the trimmed ICP algorithm framework to calculate the rigid transformation for partial registration problems. Experimental results show that our algorithm is more robust and accurate in dealing with geometry defects, missing data, and poor initial position.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The integration of vehicular network components with the Internet has recently emerged as a solution to meet the ever-growing demand for safe and comfortable driving experiences. Road Side Unit (RSU) clouds have been developed to serve requests from vehicles, but they can be limited by available resources. Additionally, it is necessary to decrease the overhead cost of Virtual Machine (VM) migrations caused by vehicle mobility. Request serving time must also adhere to delay constraints. In this research, a novel algorithm is proposed to schedule requests at RSU clouds. This algorithm efficiently schedules service requests from vehicles while considering both delay constraints and VM migrations. Simulation results indicate that the proposed scheduling technique serves a greater number of requests within their required delay constraints with fewer VM migrations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Improving Communication Efficiency in Networked Control Systems through Transmission Scheduling\n\nIn systems with communication constraints, scheduling the transmission of updated control values can lead to a significant enhancement in the trade-off between control performance and communication effort. This letter presents an analysis of a dynamical communication network coupled with a predictive controller that has explicit knowledge of the network. \n\nThe predictive controller performs a rollout strategy, where it schedules transmissions and computes corresponding control values. Using model predictive control tools, the stability of the network is established for nonlinear, constrained plants. Additionally, the special case of linear plants is scrutinized to further verify the results. \n\nFurthermore, the effectiveness of the proposed approach is demonstrated through a numerical example. The approach establishes a clear performance advantage over a feasible baseline control method, especially for unconstrained plants. These results provide more evidence for the need for careful scheduling of transmissions in networked control systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Efforts have been made to identify biomarkers for vascular dementia (VaD), but the current findings are mainly based on statistical tests of group-level differences. However, individual-level inferences, such as determining if a subject is suffering from VaD, cannot be resolved with statistical analysis. This study aimed to develop a method to effectively distinguish early VaD patients from normal controls by combining EEG features with machine learning techniques.\n\nEEG signals were recorded from a total of 15 VaD patients and 21 controls during a visual oddball task. Interregional directed connectivity was derived from directed transfer function (DTF) analysis and used as features in classification. Three machine learning methods, including linear discriminant analysis (LDA), error back-propagation (BP) neural network, and support vector machine (SVM), were used as classifiers, and their classification performance was compared.\n\nThe study found that BP and SVM classifiers effectively identified VaD patients with high accuracy. In particular, when the SVM classifier was combined with feature selection by Fisher score, it reached an accuracy of 86.11%, sensitivity of 86.67%, and specificity of 85.71%. The area under the curve (AUC, 0.854) indicates good identification of VaD patients from normal controls.\n\nAs EEG is non-invasive, inexpensive, and widely available, this study presents a novel clinical application of machine learning methods that could facilitate automatic screening and early diagnosis of VaD patients in the future.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a study on sliding mode control (SMC) via finite-time stabilization (FTS) for a specific type of nonlinear semi-Markovian jumping systems (SMJSs) characterized by conic structures. Unlike classical Markovian jumping systems, SMJSs have transition rates that are dependent on random sojourn-time g. The objective of this study is to propose an appropriate SMC law that will effectively drive the state trajectories to the assigned sliding surface within a finite-time interval. \n\nTo achieve this objective, the researchers conducted a thorough investigation, which includes establishing a mathematically rigorous proof of the FTS over-reaching phase and sliding motion phase. The FTS is designed to ensure the stability of the whole SMJSs. The proposed method's effectiveness is demonstrated via a time-delayed Chua's circuit simulation. \n\nIn summary, this paper presents a robust SMC approach for SMJSs, which is efficient in driving state trajectories to a sliding surface within a finite-time interval. The FTS over-reaching phase and sliding motion phase have been given a strict mathematical proof, and the simulation results show that the proposed method is effective.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Human activity recognition is a valuable tool that can be used in various smart assistive technologies. However, there are several challenges in the process of collecting, evaluating, and analyzing data to recognize human activities, particularly in healthcare applications. This paper aims to address the key concerns regarding the human activity recognition process in healthcare.\n\nThrough a thorough review of the approaches utilized to implement each stage of a specific activity recognition process, this study found that the major issues associated with human activity recognition mostly occur during the data collection stage. Therefore, the applicability of various approaches, including wearables, smartphones, and non-contact sensing, was analyzed to mitigate these issues.\n\nMoreover, this paper identifies the underlying issues that need to be addressed in future research activities. By addressing these issues, the accuracy and efficiency of the human activity recognition process in healthcare can be enhanced, which in turn can result in better patient care and encourage the development of more sophisticated and effective assistive technologies.\n\nIn conclusion, the human activity recognition process holds great potential to revolutionize healthcare. However, it is imperative to acknowledge and address the challenges associated with data collection in order to achieve the best possible outcomes. Through ongoing research and development, it is possible to enhance the accuracy and effectiveness of human activity recognition, thereby facilitating better healthcare outcomes for all.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This study presents a novel approach to improve the accuracy and success rate of side channel analysis by combining mutual information analysis and correlation power analysis. The k-nearest-neighborhood algorithm is utilized to classify potential keys into two categories - correct and wrong keys. The advantage of this combination is two-fold, as it enhances the accuracy of estimation and reduces the number of required measurements to reveal the correct key. Extensive simulations validate the effectiveness of this combined approach, which improves the success rate of a side channel attack by 20% and 49% compared to individual correlation power analysis and mutual information analysis, respectively, while requiring fewer measurements.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In energy-from-waste plants, combustible waste is transported in by garbage trucks and deposited into a large waste pit. From there, a waste crane is used to transport the waste into the incinerator. To ensure the stability of the combustion process, it is crucial to maintain a consistent waste composition by breaking up plastic containers and thoroughly mixing the garbage. However, operating the crane is a complex task due to limited space in the waste pit, time, and energy conservation constraints.\n\nTo address this issue, a fully automatic waste crane scheduler utilizing evolutionary computation has been developed. The proposed scheduler has been successfully implemented in an actual energy-from-waste plant, demonstrating improved mixing of the waste in the incinerator. With this new technology, the process of waste incineration can become more efficient and effective, leading to greater benefits for the environment and the economy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents efficient distributed algorithms for weighted average consensus of input data. For acyclic graphs, the proposed algorithm achieves exact weighted average consensus in a finite number of iterations. The number of iterations is equivalent to the graph diameter. To handle loopy graphs, the authors provide two solutions. The first approach converts the loopy graph into a spanning tree, enabling the use of the average consensus algorithm. The second solution modifies the average consensus problem to produce an optimal solution that approximates the consensus solution with arbitrary precision. The corresponding modified average consensus algorithm guarantees exponential convergence to the optimal solution. These proposed algorithms are highly efficient, have low complexity, and are resilient to transmission adversaries. They are also suited for asynchronous implementation. Unlike the graph Laplacian approach that is widely used, the proposed algorithms are conceptually different and converge more rapidly.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Vision-to-language tasks have become a popular area of research that aims to combine computer vision and natural language processing. However, typical approaches often fail to capture high-level semantic concepts and subtle relationships between image regions and natural language elements. To address this issue, this paper proposes a new method that utilizes text-guided attention and semantic-guided attention (SA) to reduce the semantic gap between vision and language. \n\nThis method includes two-level attention networks: text-guided attention network and SA network. The text-guided attention network selects the image regions that are related to the given text, while the SA network highlights the concept-related regions and the region-related concepts. These networks work together to incorporate all the relevant information and generate accurate captions or answers. \n\nThe proposed approach was evaluated through image captioning and visual question answering experiments, which demonstrated its excellent performance in capturing the semantic concepts and relationships between different elements. In conclusion, our method presents a promising solution to integrating computer vision and natural language processing and can potentially benefit a wide range of applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Despite the impressive performance of convolutional neural networks in image search, most existing methods are designed for natural images captured using normal lenses without anamorphic distortion. However, circular fisheye lenses are widely used to capture images with a large field-of-view, particularly in the field of natural science. This paper introduces a novel image search method for circular fisheye images, particularly aurora images used in solar-terrestrial space research. The proposed method combines the advanced Mask R-CNN framework with saliency deep features to extract semantic features from the images. Additionally, a saliency proposal network (SPN) is implemented within the region proposal network (RPN) to ensure the anchoring boxes are spherical and follow the physical magnetic meridian, covering a minimum area that includes the auroral structures. Experiments conducted on a large aurora image dataset proved the superiority of the proposed method over other state-of-the-art methods, in terms of search accuracy and efficiency.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Clustering is a crucial technique used to maintain the balance of energy consumption in wireless sensor networks. This method enhances the sustainability and scalability of the network. When dealing with homogeneous WSNs, energy-efficient clustering algorithms must be developed to optimize their performance. \n\nTo address this issue, we propose a new centralized energy-efficient clustering protocol called Distance Energy Evaluated (DEE) for homogenous WSNs. DEE elects cluster heads (CHs) based on a probability ratio between the distance and residual energy of each node. This probability considers both the initial and residual energy levels of each node to determine its likelihood of becoming a CH. \n\nThe simulation results demonstrated that DEE outperforms existing important clustering protocols in homogeneous environments in terms of effective message transmission and longer lifetime. Therefore, DEE presents a promising solution to optimize the energy consumption and scalability of homogeneous WSNs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we explore the application of data mining techniques including statistical and spectral analysis as well as machine learning to analyze data collected from wearable devices. Specifically, we focus on their potential use in detecting tremors associated with Parkinson's disease. We provide a rationale for utilizing these methods and describe how they interact with one another. Our study involved 8 patients with Parkinson's and we compared our algorithm, as well as some modified versions of it, to the best existing method for detecting tremors. Our results show that our method has an impressive accuracy of 0.98, making it a viable option in clinical settings.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Advertisements are an integral part of internet economics and culture, and video ads have become the most popular form of advertising in recent years. However, they can be expensive to create and are not always effective. This poses a problem for creators, advertisers, and ad platforms, who would like to know the effectiveness of a video ad before deployment.\n\nTo address this problem, we propose a novel algorithm that provides feedback before an ad is placed based on historical data about the effectiveness of other video ads. Our approach leverages a multi-modal mixture based algorithm to predict the effectiveness of the video automatically. We use both textual and visual information to learn a finite mixture model, which can provide more accurate predictions.\n\nIn our experiments, we tested our approach on a publicly available dataset and found that our algorithm outperformed other baseline approaches. By providing feedback on the potential effectiveness of a video ad before deployment, our approach can help creators, advertisers, and ad platforms make better-informed decisions about their advertising strategies.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a study on the stabilization of an unstable time fractional hyperbolic partial differential equation through the use of boundary control and boundary measurement. The investigation is conducted using the backstepping method, the fractional Lyapunov method, and the semigroup theory. A new state feedback control that uses the Dirichlet boundary is designed to stabilize the controlled system. An observer is then constructed based on the output signal to recover the original system's state. An observer-based stabilizing control law is proposed, which ensures that the closed-loop system has a unique solution and is Mittag-Leffler stable. The proposed theory is tested using a benchmark example.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Individual recognition of communication emitters can be achieved by separating and intercepting specific signals to obtain their working mechanism, system parameters, and usage model. To achieve this, a method using wavelet packet feature extraction and attribute reduction through granular computing has been proposed. This method utilizes hierarchical and granularity to represent the subtle feature information of different emitter signals after wavelet packet decomposition. By mapping feature attributes efficiently to different levels of granular structure, an optimal reduced attribute operation can be identified for signal recognition. The reduced attributes are then used to correctly distinguish emitter signals using the label propagation method. Results from experiments show that this proposed method has good classification performance in individual recognition of communication emitters.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel approach for tackling the finite-time control issue for uncertain nonlinearly parameterized systems with arbitrary switching. The proposed method incorporates the adding a power integrator technique (APIT) and introduces a new adaptive controller with a tuning parameter. Unlike previous studies, this research develops a new common Lyapunov function (CLF) with a tuning parameter to achieve global finite-time stability (GFTS) for closed-loop switched systems. The method is proved to ensure that all states of the switched systems converge to an equilibrium state in a finite time. Several simulations are presented to demonstrate the feasibility and advantages of the proposed approach.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As Integrated Circuits (ICs) become more complex and Intellectual Property (IP) based hardware design becomes the trend, Hardware Trojans have become an inevitable threat. These Trojans can be inserted by untrustworthy third parties during the design or fabrication phase of the IC design cycle. Once implanted, they can cause unintended modifications in system functionality, leading to significant performance penalties or even system crashes. Additionally, malicious Trojans may access highly sensitive data, increasing system vulnerability. \n\nA Trojan can continue to inject memory transactions through the system interconnect, resulting in unnecessary invalidation broadcasts and the eviction of valid cache lines, causing a significant degradation in system performance. In extreme cases, a malicious Trojan can even cause Denial-of-Service (DoS) attacks, greatly increasing lower level memory traffic. \n\nThis study evaluates the effects of such stealthy Trojans on the performance of a real-time many-core system. Machine learning based techniques are used for run-time detection of such Trojans, with simulation results showing more than 95% accuracy in detection.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Many mobile systems and wearable devices, such as Virtual Reality (VR) or Augmented Reality (AR) headsets, lack a keyboard or touchscreen for typing ID and Password to sign into a virtual website. However, these devices are usually equipped with gesture capture interfaces that allow users to interact with the system through hand gestures. While gesture-based authentication is a well-studied problem, gesture-based user identification is often overlooked. This problem is also an input method of account ID and an efficient searching and indexing method of a database of gesture signals. To solve this problem, we propose FMHash (Finger Motion Hash), a user identification framework that creates a compact binary hash code from user in-air-handwriting gesture of an ID string. The hash code enables indexing and fast search of a vast account database using the in-air-handwriting by a hash table. We implemented a prototype and achieved \u226599.5% precision and \u226592.6% recall with exact hash code match on a dataset of 200 accounts collected by us. The framework's ability to create binary codes from in-air-handwriting gestures aligns with the future demand for convenient sign-in and sign-up with in-air-handwriting gesture ID on mobile and wearable devices connected to the internet.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In addition to regulating the amount of light that reaches the retina, the pupil also responds to cognitive and emotional processing. As a result, dilation of the pupil can serve as an indicator of cognitive effort and emotional arousal, providing valuable insights into these psychological conditions. \n\nTo better understand this phenomenon, an extended version of a computational model of pupil dilation was developed, which could account for the \"contagion effect\" where one's own pupil dilates upon observing another person with dilated pupils. This extends previous models by accounting for the potential influence of social cues on pupil dilation.\n\nThe model was also able to reproduce the effects of cognitive effort in a math exercise, demonstrating its efficacy in accounting for different components of pupil dilation. Moreover, the model was able to probe the abnormal pupil response seen in some individuals with or at risk for autism spectrum disorder, highlighting its potential as a tool for understanding complex cognitive and emotional phenomena.\n\nOverall, these computer simulations demonstrate the utility of system-level models of the brain in understanding a range of psychological phenomena. By providing a better understanding of the mechanisms underlying pupillary responses, these models offer insights into the workings of the mind and may inform treatment and support for various conditions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the widespread adoption of large-scale distributed generators (DGs) in active distribution networks (ADNs), conventional load flow convergence failures caused by heavy power transmission have become a common issue. To combat this challenge, the Holomorphic Embedding Load Flow Method (HELM) has emerged as a more robust solution when compared to the traditional Newton-Raphson method. Furthermore, it also offers lower sensitivity to initial points. It is important to note that HELM was primarily designed for balanced transmission networks. Hence, our study aimed to develop a three-phase HELM model that caters to ADNs with DGs, delta connection loads, and ZIP loads. Using modified unbalanced IEEE 13, 34, 37, and 123 test feeders, our proposed method was demonstrated to provide better performance and effectiveness, especially under heavy load situations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In aerial mapping, transforming different sets of images into one coordinate system is a crucial step. It is a critical capability of unmanned aerial vehicles (UAVs), but the quality of captured images strongly affects the results of the image registration system. Choosing the images that will be processed efficiently is a challenge because ground truth in the mapping process is not known before the UAV flies and takes the images. Moreover, UAVs generally fly and capture images in sequence regardless of their quality, resulting in poor mapping results and high computational costs. To address these issues, we propose an inclined image recognition system that excludes images captured by UAVs not perpendicular to the ground. Our system utilizes a deep learning approach to recognize these images without the use of a gyroscope sensor. We tested the proposed system with UAV-captured images and achieved an accuracy rate of 86.4%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Full-Duplex (FD) and Device-to-Device (D2D) communications have emerged as a result of spectrum scarcity in 5G networks. Advancements in self-interference-to-power-ratio (SIPR) reduction have opened up opportunities for FD to double data rates and reduce latency. This advantage can now be leveraged to optimize dynamic spectrum sharing among different radio access technologies in cognitive networks. However, protecting primary user communication has been a challenging problem in such coexistence. In this paper, we analyze the protection of primary users reception based on secondary users' FD-enabled communication from an abstract level. We propose an optimal mode selection (Half-duplex, Full-duplex, or silent) for secondary D2D users, depending on its impact on primary users. Our analysis presents the significant advantage of D2D mode selection in terms of efficient spectrum utilization while protecting the primary user transmission, leading the way for FD-enabled D2D setup. We characterize the interference from D2D links and derive the probability for successful D2D users for half-duplex and full-duplex modes based on the location and transmit power of D2D users. The analyses are supported by theoretical and extensive simulation results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A fast and efficient method for estimating the time-delay in a linear frequency modulation signal has been proposed, called the frFt-TELS estimator. This approach involves searching for the maximal bin of the fractional Fourier transform (frFt) as a rough estimation, followed by applying Taylor expansion to the theoretical frFt power spectrum around the coarse estimation. The fine estimation is then obtained by using least squares approximation between the theoretical frFt power spectrum and the frFt power spectrum of the given sample data. The simulations conducted have demonstrated that frFt-TELS achieves comparable performance to traditional estimators, while being computationally less complex.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We are proposing a new approach to enhancing classification networks, which is Three-Player Generative Adversarial Network. In this approach, we introduce a competition between the generator and the classifier, in addition to the game that is played between the discriminator and the generator. The main objective of the generator is to produce synthetic samples that are both realistic and challenging to label for the classifier. Although we do not make any assumptions regarding the type of augmentations to learn, this method enables us to synthesize examples that look remarkably realistic while being difficult for the classification model.\n\nMoreover, we discovered that our classifier becomes increasingly immune to difficult samples after being trained on them. The efficacy of this approach was evaluated on a publicly available dataset for traffic sign identification.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There are numerous onomatopoeias in the Japanese language. These serve as a direct means for Japanese people to convey their own senses and images to others. Onomatopoeias are characterized by sound symbolism, allowing those familiar with Japanese to metaphorically associate images with even new coined words. However, for those with different linguistic backgrounds, it can be difficult to grasp the semantic usage of Japanese onomatopoeias in a metaphorical sense. To address this issue, research has focused on the phonological and acoustic features of onomatopoeias. Attempts have been made to automatically classify onomatopoeias into appropriate semantic usage categories using these features. This paper employed mimetic words associated with \"human action\" as an example for automatic classification. Through investigation, we have identified useful features among the phonological and acoustic features for classifying mimetic words.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Person re-identification is a technique used for searching a target within a video surveillance network. This technique has become a research hotspot in computer vision and is widely used in security and surveillance systems. However, person re-identification has also been a challenging task as it involves dealing with multiple cameras in the network, variable camera angles, illumination, occlusions and poses.\n\nIn this paper, we propose a novel approach to person re-identification that is capable of handling occlusions and variations effectively. Our approach is based on a human pose guided convolution neural network framework with joint loss functions. We first extract local features from body parts localized by landmarks, followed by merging it with global features to learn the similarity metric. We employ both identification loss and pose-constrained triplet loss function to train the model.\n\nOur approach outperforms most of the state-of-the-art methods on three large-scale datasets, the Cuhk03, Market1501 and Duke MTMC-reID, achieving an accuracy of 83.31%, 86.1% and 72.6% respectively. The proposed method also significantly outperforms existing approaches in terms of robustness to occlusions and variable poses. Overall, this approach demonstrates significant potential for practical applications in real-world surveillance systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A key aspect of smart cities is the prevalence of numerous mobile multimedia terminals. However, accommodating these devices via device-to-device (D2D) communication can lead to serious interference and energy consumption when D2D pairs reuse cellular users' channels. Thus, this paper aims to maximize the energy efficiency of D2D communication while considering the constraints of D2D pairs and cellular users' quality of service.\n\nSince the resource allocation problem is often difficult to solve, the problem is divided into power control and channel allocation sub-problems. Specifically, a power control algorithm based on the Lambert W function is proposed to maximize a single D2D pair's energy efficiency. Then, preference values for D2D pairs and cellular users are calculated using the power control results. A channel allocation scheme based on the Gale-Shapley algorithm is then employed, which uses the preference values to match the two sides, aiming to maximize the signal-to-interference plus noise ratio of cellular users and the energy efficiency of D2D pairs.\n\nSimulation results demonstrate that this proposed algorithm not only ensures the transmission rate of cellular users but also improves the energy efficiency of the system and D2D pairs. Overall, this approach provides a promising solution to the energy efficiency maximization problem in D2D communication.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The landscape of super-computing is rapidly evolving towards high and heterogeneous core count designs, driven by power and complexity trends in processor and system architecture. As the number of cores continues to increase, traditional models for large-scale computations such as OpenMP & MPI will need to evolve to overcome challenges in expressing asynchrony in programs.\n\nTo address these challenges, the codelet model was developed as a fine-grained, event-driven asynchronous program execution model. While initially designed to exploit instruction level parallelism, the codelet model is rife with opportunities for performance enhancements with respect to both data and control regular applications.\n\nThis paper proposes hardware assisted extensions to the codelet execution model in order to implement efficient dataflow software pipelining and extend the model's capabilities. A hardware-software co-design approach is taken, leveraging proposed optimizations such as FIFO ring buffers, multiple-head FIFO buffers, and single owner FIFO buffers to exploit the advantages of dataflow software pipelining.\n\nBy identifying key applications and benchmarking performance, we anticipate that a wide range of scientific, machine learning, and streaming applications can take advantage of the techniques proposed in this paper.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The first generation of IoT was developed with the aim of connecting devices with common functionalities. However, the IoT ecosystem now requires smarter, faster and more reliable solutions, especially in dynamic situations. Contextual AI technologies on fog platforms offer a promising solution for the next generation of IoT services. This led to the development of a flexible fog computing framework known as EiF. It runs on IoT gateways with adaptive AI services on the cloud. EiF is an integration of three emerging technologies, namely IoT, fog, and AI. The framework virtualizes the IoT service layer platform and provides management and orchestration functions for various fog nodes. AI services are then fostered within both the federated cloud and distributed edge side before they are deployed on the fog nodes. The feasibility of EiF is demonstrated through an example of intelligent traffic flow monitoring and management.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet of Things (IoT) is dedicated to solving critical challenges faced by the environment, industries, cities, homes, and society. It does this by leveraging data from numerous sensors and internet-connected devices. In this paper, we propose an innovative IoT-based solution aimed at detecting hydrocarbon pollution generated by retail fuel outlets. This solution incorporates a low-cost, yet extremely accurate, fibre optic sensor that can identify hydrocarbons in groundwater, and which can be easily deployed into existing monitoring wells. \n\nThese hydrocarbon sensors are an integral component of an IoT sensor data collection and analysis platform. We are utilizing commercially available sensor nodes to communicate via low power networks (such as LORA) to collect and analyze hydrocarbon pollution data in the cloud. This IoT platform is breaking new ground by combining hydrocarbon sensing with cloud-based data analysis, enabling the production of continually updated hydrocarbon pollution maps and alerts. \n\nThis solution can detect and report hydrocarbon pollution in real-time, potentially combining hydrocarbon pollution levels from millions of such sensors deployed in thousands of service stations across the globe. The platform will automatically analyze this data to produce instantaneously updated hydrocarbon pollution maps and alerts for individual service stations, corporate chains, and environmental monitoring agencies.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With von-Neumann computing struggling to match the energy-efficiency of biological systems, there is a pressing need to explore alternative computing models. Recent experimental studies have revealed that resistive random-access memory (RRAM) is a promising alternative for DRAM. Resistive crossbar arrays possess many promising features that can not only enable high-density and low-power storage but also non von-Neumann compute models. However, most recent works focus on dot product operation with RRAM crossbar arrays and are, therefore, not flexible to implement various logical functions. \n\nTo address this limitation, we propose a low-power dynamic computing-in-memory system that can implement various functions in Sum-of-Product (SOP) form in RRAM crossbar array architecture. To evaluate our proposed technique, we conducted simulations over a wide range of MCNC benchmarks. The simulation results showed a 1.42X and 20X latency improvement, as well as 2.6X and 12.6X power saving, compared to static and MAGIC computing-in-memory methods. \n\nIn conclusion, our proposed low-power dynamic computing-in-memory system using RRAM crossbar arrays shows promising results for implementing a variety of logical functions in an energy-efficient manner. This could potentially pave the way for future computing models that utilize RRAM technology to address the energy-efficiency challenges faced by traditional von-Neumann computing models.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distribution system state estimation (DSSE) has become increasingly popular in practical distribution networks. However, distribution systems have unique challenges, such as unsymmetrical configurations and limited real-time measurements, making it difficult to apply mature state estimation methods for transmission systems. This paper proposes a new weighted least square-based DSSE approach for three-phase four-conductor configured unsymmetrical medium-voltage distribution systems.\n\nIn this approach, voltages are chosen as state variables, and load pseudo measurements of low-voltage distribution systems are considered to compensate for insufficient real-time measurements in medium-voltage distribution systems. Both rectangular and polar coordinates are studied, and voltage variables of neutrals and zero-injection phases are eliminated to reduce the scale of the DSSE problem. To improve load estimation accuracy, a clustering and partial least square regression-based load estimation model is proposed to leverage the real-time communication ability of smart meters.\n\nThe effectiveness of the proposed approaches is demonstrated through case studies on a modified IEEE 123-bus distribution system with actual smart meter data. The results show that the proposed DSSE approach can accurately estimate the states of distribution systems, even with limited real-time measurements. Additionally, the load estimation model can improve the accuracy of load estimation and reduce the errors caused by the lack of real-time data.\n\nOverall, the proposed approaches provide a promising solution to the challenges faced in distribution networks and demonstrate the potential for future research in this area.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recognizing dimensional variation patterns accurately is crucial to produce high-quality body-in-white (BIW) in automotive manufacturing. Optical coordinate measurement machines (OCMM) have been widely used in vehicle factories and generate massive online dimensional data necessary for variation pattern recognition. However, the sheer volume of serially correlated or autocorrelated and 100% measurement data from OCMM presents a challenge to traditional statistical process control (SPC) technology and common variation recognition approaches. \n\nTo address this challenge, this paper introduces a novel deep-learning method called long-short term memory neural network (LSTM NN), designed to recognize the variation pattern of BIW OCMM online measurement data. A comparative study between the backpropagation neural network (BP NN) and the LSTM NN was conducted to demonstrate the feasibility of the proposed intelligent method. The results of the case study indicate that the LSTM NN, which effectively uses time series information, shows outstanding performance in variation patterns' recognition and high practicability in improving the quality of the BIW.\n\nIn conclusion, utilizing the LSTM NN to recognize dimensional variation patterns in BIW OCMM online measurement data presents a promising approach to overcome the limitations of traditional SPC technology and common variation recognition approaches. This innovation could significantly improve the quality of BIW and enhance the competitiveness of the automotive industry.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper addresses the problem of face sketch synthesis in the wild, which involves transforming a face photo into a face sketch. While face sketch synthesis finds application in several fields such as law enforcement and digital entertainment, existing methods either rely on hand-crafted techniques that require prior human experience or adopt deep learning techniques as an end-to-end framework that may not represent facial details well. In this paper, we propose a novel approach named DeepPGM, which uses a deep patch representation-based probabilistic graphical model. We construct a Siamese network that extracts deep patch representation from a raw facial patch which can be used for robust face sketch synthesis. We then optimally combine the generated deep patch representation and facial image patches through a probabilistic graphical model. Our proposed DeepPGM approach outperforms state-of-the-art methods on public face sketch datasets and can cope with varying lightings, poses, occlusions, skin colors, and ethnic origins encountered in forensic photos in the wild conditions. We demonstrate the superiority of our method through extensive experiments on two public face sketch datasets and real-world forensic photos.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With autoscaling, clouds are now considered as self-adaptive systems. However, the reactive nature of the existing autoscaling solutions limits the ability of cloud applications for self-adaptation. This is mainly due to the necessity for manual configuration of autoscaling rules. Monitoring systems have evolved to enable data-driven approaches to derive the parameters of scaling rules, leading to predictive autoscaling. By forecasting the amount of requests to cloud applications, it is possible to estimate the upcoming demand to replicate microservices in advance. An industry partner Instana provided anonymized time series data for 261 microservices, allowing the authors of this paper to test various extrapolation models such as seasonal ARIMA models, exponential smoothing models, and support vector regression. The study aims to provide an approach to evaluate the quality of forecasting models for self-adapting cloud applications and virtual infrastructure. Comparative results are provided based on the interval accuracy score and model fitting time.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: When using standard learning techniques to approximate uncertainty, it is essential to have persistently exciting inputs to achieve optimal parameter estimation. However, there has been progress in recent years by utilizing concurrent learning (CL), which provides better learning outcomes without the need for input persistency. To determine the effectiveness of the parameter estimation, it is essential to analyze how accurately the estimated parameters can reconstruct the uncertainty. While there have been several studies with the CT framework, there is a gap in research when it comes to discrete-time (DT) structured uncertainties. In an earlier study, we demonstrated that the concept of CL can aid in solving the identification problem for DT structured uncertainties without requiring persistency of excitation. In this paper, we elaborate on this idea and compare it with the traditional gradient descent technique. Our primary contribution is presenting a generalized gradient-based DT algorithm for online approximation of both DT structured and unstructured uncertainties. Through formal derivations, we demonstrate how the algorithm utilizes memory more efficiently, which results in better learning. Numerical simulations reinforce our findings and showcase the algorithm's effectiveness in achieving optimal parameter estimation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep neural networks (DNNs) are known to be memory-intensive, and compressing them with weight sharing is an effective way to deploy them on embedded systems with limited on-chip memory capacity while saving energy consumption. However, extreme compression sacrifices the accuracy of DNNs. To address this issue, this work proposes two optimization problems and solutions for DNNs with \u201cunfitted compression\u201d to maintain accuracy, where all distinct weights of the compressed DNNs cannot be entirely contained in on-chip memory. The first problem is to arrange weights in off-chip memory to minimize the number of memory accesses and energy consumption. The second problem is to devise a strategy for selecting a weight block in on-chip memory for replacement when a block miss occurs to minimize energy consumption and scanning overhead. Experiments with compressed AlexNet show that our solutions reduce energy consumption of off-chip memory accesses by 34.2% on average compared to unoptimized memory layout and LRU replacement scheme.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurate lane information is imperative for ensuring safe autonomous driving. This article presents a novel multisensor fusion framework for processing ego and adjacent lanes. The framework adopts a Dempster-Shafer theory-based fusion approach for road marking lines and uses a particle filter for lane tracking. Furthermore, a fusion quality measure is computed for each line, considering sensor coherence, availability, and temporal continuity. This measure is critical for deploying various lane management strategies and avoiding incorrect data integration. The proposed framework was assessed in a lateral control architecture for autonomous driving on open roads and demonstrated its reliability and availability.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Proof-of-Work (PoW) is a popular consensus algorithm used in many Blockchains (BCs). However, it has been criticized for its high power consumption and low transaction rates. This paper introduces a PoS-based BC called Bazo, specifically designed for IoT data streams. Bazo offers improved energy consumption and transaction processing compared to PoW-based BCs. Furthermore, this work employs sharding and transaction aggregation methods to further enhance the performance of Bazo. A modular and layered architecture is also proposed, which includes IoT-BC adaptation helpers, that allows wireless devices to submit data into the BC. This architecture is designed to support multiple hardware and software platforms, as well as various network technologies.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Uniform Opposition-Based Particle Swarm Optimization (NOPSO) method was developed to address the limitations of the opposition-based particle swarm optimization, which often experiences slow convergence speed and falls into local optimization. Two main mechanisms were implemented to balance the trade-off between exploration and exploitation during the search process. The first mechanism involved the creation of a new particle's position update rule, whereby the uniform term was used to replace the inertia term, accelerating the convergence rate. The second mechanism involved an adaptive elite mutation strategy (AEM), which prevented the method from getting trapped in local optima. Empirical findings demonstrate that NOPSO substantially outperforms current state-of-the-art PSOs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Automated Tongue Diagnosis (ATD) has gained increasing attention in recent years due to the growing demand for personalized healthcare worldwide. One of the most critical technologies for automated tongue diagnosis is automated tongue segmentation. Developing an efficient and accurate tongue segmentation algorithm for mobile and embedded devices is of paramount importance. However, research in this field is still limited. Existing methods suffer from either poor efficiency or insufficient accuracy. \nTo address these challenges, our paper introduces a new class of efficient cascaded CNN models for tongue detection and segmentation, which are both lightweight and highly accurate compared to other state-of-the-art algorithms. In addition, we propose a new annotation method to reduce the workload of establishing a tongue segmentation database. \nOverall, our study offers a promising solution for improved automated tongue diagnosis, which can help to enhance personalized healthcare by providing reliable, efficient, and accurate tongue segmentation technology.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud data centers have experienced unprecedented growth in recent years due to the increasing demand for real-time video streaming, on-demand gaming, e-commerce services, and highly connected social networks. To meet these demands, cost-effective service models, adaptive resource provisioning, and upfront application availability are critical. However, there are several challenges to be addressed to ensure the effective functioning of cloud computing paradigms, including optimal resource management, instant response time, and interoperability among a diverse set of emerging technologies and innovative applications.\n\nFortunately, the softwarization of networks offers immense opportunities to better utilize network resources by programmable abstractions with efficient control and management techniques, enabled by NFV and SDN principles. Furthermore, machine learning-based solutions are gaining prominence in optimizing resources and autonomous systems. In this paper, we aim to address these challenges by proposing DeepSDN, an SDN-based solution that leverages existing machine learning techniques to move closer to self-driving networks. Our experimental testbed results corroborate the effectiveness of our approach and suggest a path towards autonomous network management.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Automated whole breast ultrasound (AWBUS) image segmentation poses several challenges, such as a shadow effect, blurred boundaries, low contrast, and the presence of large targets. In order to address these challenges, a novel framework called AttentionNet has been developed, which incorporates self-attention mechanisms during both feature extraction and up-sampling phases.\n\nThe feature extraction process initially involves the use of ResNeXt-50 for exploring intra-channel information. To ensure effective feature extraction and channel utilization, a module called spatial attention refinement (SAR) has been created using the basic ResNeXt-50 module, also known as ResNeXt-SAR. During the up-sampling phase, a weighted up-sampling block (WUB) module is introduced, which is designed to provide precise pixel localization by incorporating high-level semantic concepts. This module plays an essential role in guiding low-level features based on category information.\n\nExtensive experiments have been conducted on AWBUS images for multi-class image segmentation. The AttentionNet proposed in this study has achieved superior results compared to state-of-the-art approaches, and can potentially aid in the calculation of breast density.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Analog-to-Information Converter (AIC) is a physical implementation of Compressed Sensing (CS) systems which allows signals to be digitized directly from the analog domain. However, due to non-idealities of components, AIC hardware requires a calibration step to adjust parameters before being utilized. In this study, we investigate a theoretical linear model of an AIC architecture composed of parallel channels that may suffer from synchronization problems between the input signal and the internal signals used for measurement. To solve this issue, we propose a method inspired by radar techniques to estimate the delay between these signals. The results show that the proposed method is accurate and has the potential to be used in AIC calibration techniques.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Endoscopy is a common medical imaging technique that is used for both diagnosis and minimally invasive surgical treatment. Despite the wealth of information contained in endoscopy videos, the existing tools for capturing this information for clinical reporting are rather inadequate. Currently, endoscopists do not have access to tools that enable them to navigate the video data effectively and efficiently. Reliable and efficient video retrieval methods could allow them to review data from previous exams and improve their ability to monitor disease progression.\n\nDeep learning is a promising approach for compressing and indexing video data efficiently, and in this study, we propose using an autoencoder for this purpose. Our autoencoder-based approach allows for fast and efficient video compression and retrieval of video images. To address variability in the data, such as multi-modality and view-point changes, we integrate a Siamese network to enhance the accuracy of the video image retrieval. \n\nOur approach is competitive in retrieving images from large-scale videos of three different patients against the query samples of their previous diagnoses. Quantitative validation shows that the combined approach yields an overall improvement of 5% and 8% over classical and variational autoencoders, respectively.\n\nIn summary, our proposed method provides a reliable and efficient way to navigate endoscopy video data, which could improve diagnosis and disease monitoring capabilities.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A new approach to online fault configuration in network simulation has been proposed in this paper. The programmable dynamic simulation interface (PDSI) allows for dynamic setting of link and route faults during joint power grid simulation. The approach includes a graphical user interface (GUI), which enables the user to configure fault situations in real-time. Based on the configuration messages received from the GUI, QualNet will push corresponding events into the event queue, leading to the closing of link ports or rewriting of routing tables to formulate link or route faults. The proposed approach was tested experimentally and showed successful fault configuration of communication networks with an acceptable compromise at the interface interaction delay.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, a class of cyber defense dynamics has been proven to be globally convergent. This means that the dynamics always converges to a unique equilibrium point, and the location of this point depends only on the model parameters, not the initial state of the dynamics. In this paper, we unify this class of preventive and reactive cyber defense dynamics with the closely related class of N-intertwined epidemic models under a single framework. We prove that the unified dynamics is still globally convergent under mild conditions, which are naturally satisfied by these two classes of dynamics models and are necessary when examining more general frameworks. Additionally, we characterize the convergence speed of the unified dynamics, which provides a full characterization on the convergence speed of the N-intertwined epidemic model and its extension, a topic that has only been partially addressed in previous literature.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet's topology has shifted over the years, from one with symmetric traffic flows and a hierarchical topology to one with significantly asymmetric traffic flows and a flatter topology. Today's Internet comprises of three main types of networks: content providers, user access providers, and transit providers. However, the best-effort routing of centrally stored content using distributed protocols has proven inadequate in providing the necessary level of service to end-users. Two significant developments have arisen to mitigate this gap - the proliferation of content distribution networks and direct peering between content networks and ISPs.\n\nIn this paper, we will analyse the economics of such interconnections. Using microeconomic models from the industrial organisation literature, we will first establish the conditions for a content provider to connect directly to a service provider. Once that is established, we will analyse the quality of the link compared to using a transit service. We will examine the content provider market coverage by content distribution networks, and finally, we will discuss the implications of these results on the objectives sought by net neutrality regimes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Visible light communication (VLC) networks have recently emerged as a promising alternative for indoor data access due to their high data rate, low implementation cost, and immunity to radio frequency (RF) interference. However, the co-existence of VLC with RF access points and the dependence of VLC on room illumination necessitate the formation of a hybrid VLC/RF network to offer the advantages of both technologies, such as increased capacity and ubiquitous coverage.\n\nNon-orthogonal multiple access (NOMA) is a very promising technique for the next generation of wireless networks due to its increased spectrum efficiency compared to orthogonal access schemes. However, the optimal user grouping in NOMA presents a combinatorial NP-complete problem, which requires low complexity techniques. In this paper, we propose the use of coalitional game theory, where users served by the same access point form a single coalition, and can switch through coalitions based on their payoff. \n\nWe also propose a novel utility function that takes into account the peculiarities of the NOMA hybrid VLC/RF network, and present a coalition formation algorithm as well as an efficient power allocation policy. Our computer simulations validate the presented analysis and reveal the effectiveness of our proposed user grouping scheme compared to an opportunistic approach.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Voice-over IP (VoIP) technology is a digital transmission technology that relies on IP network. Its primary usage is to enable voice service through steganographic carrier for secure transmission. However, the traditional steganographic codes have limitations such as low embedding efficiency and weak concealment, which fail to meet the security requirements for VoIP streaming media information hiding. To address these challenges, a steganographic algorithm that combines F5 and simplified wet paper code (SWPC) algorithm is proposed. The algorithm involves embedding secret messages in each row of the carrier matrix using the F5 algorithm, and then using the SWPC algorithm to embed the columns, based on the wet and dry characteristics of wet paper code, without impacting the results of row embedding. The VoIP streams encoded by the ITU-T G.729a codec are used as a carrier to verify the proposed scheme. The experimental results demonstrate that the proposed scheme can achieve better IP speech data steganographic transparency and outperforms F5-WPC and SWPC approaches.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Integrated circuit (IC) camouflaging is a promising technique to protect against IC extraction attacks, which aim to reverse engineer the netlist of a packaged IC using delayering and imaging techniques. Camouflaging involves hiding the Boolean functionality of selected gates in the netlist, which makes it more difficult for attackers to infer the exact Boolean functionality of the netlist. However, camouflaging comes at the cost of increased gate area and power.\n\nThis paper describes a new class of attacks on IC camouflaging referred to as SAT attacks. These attacks use the input/output (I/O) behavior of a functional camouflaged IC, along with Boolean satisfiability (SAT)-based inference, to reverse engineer the Boolean functionalities of camouflaged gates. The SAT attack is rooted in a foundational complexity theory mindset and has been shown to defeat defenses that previously claimed to be secure against even the most determined adversaries.\n\nThe impact of the SAT attack has led to the emergence of new, SAT-resilient defenses. However, these defenses are still vulnerable to enhancements of the SAT attack, and the implications of the attack on provably secure defense mechanisms must be carefully considered.\n\nIn summary, IC camouflaging is a promising defense against IC extraction attacks, but SAT attacks represent a powerful new class of attacks that must be taken into account when designing IC security defenses.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The prevalence of High Definition (HD) and UltraHD (UHD) videos in recent years has led to an increase in video display across a range of devices, including televisions, tablets, and smartphones. To provide users with high-quality video content, it is essential to evaluate the visual quality of these videos. Video quality assessment (VQA) has therefore become a major focus for researchers. In this paper, we not only consider the media format and pixel quality, but also take into account the video contents. Our method employs four measures, including picture resolution, bitrates, Spatial Information (SI), and Temporal Information (TI), to represent visual quality across four distinct dimensions. We also construct a subjective database to train a neural network, which can scale video qualities that are content-aware. Our experiments demonstrate that our approach outperforms human observers in terms of correlations with Mean Opinion Score (MOS) values.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This research paper explores a grant-free non-orthogonal multiple access (NOMA) system, which enables a large number of users to transmit data without a grant-based initial access procedure. In this system, the base station (BS) lacks scheduling grant information, which leads to the challenge of determining which user is using each resource. Furthermore, due to the absence of uplink (UL) timing control, multiple user signals are received asynchronously, causing asynchronous interference, or multi-user inter-carrier and inter-symbol interference. \n\nTwo solutions to address these problems are proposed in this paper. The first solution is an auxiliary preamble structure that can detect user activity efficiently, even with a massive number of users. The second solution involves a modification to the interleave-division multiple access (IDMA) receiver that mitigates asynchronous interference. The simulation results demonstrate that these proposed schemes significantly improve preamble detection performance and bit error rate (BER) performance in comparison to conventional schemes. Additionally, the results indicate that the proposed grant-free NOMA system delivers superior performance in terms of transmission time and signaling overhead when compared to the grant-based NOMA system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The development of Vehicular Ad-hoc NETwork (VANET) has revolutionized the way people travel, but it has also raised concerns about security. Traditional solutions to the security concerns have been based on a centralized approach that depends on a single, trusted entity, which can be vulnerable to a single point of failure attack. Furthermore, there is a lack of technical-level approaches to ensure the security of data in VANET.\n\nTo address these issues, this paper proposes a VANET security architecture based on blockchain technology and mobile edge computing. The architecture consists of three layers: the perception layer, the edge computing layer, and the service layer. The perception layer provides security for VANET data during transmission using blockchain technology. The edge computing layer offers computing resources and edge cloud services to the perception layer, while the service layer combines traditional cloud storage and blockchain to enhance data security.\n\nWith this architecture, VANET can enjoy the benefits of blockchain technology and mobile edge computing while ensuring data security. The proposed architecture can also minimize the risk of a single point of failure attack and provides a technical-level approach to data security in VANET.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This article showcases a specially-designed system that aims to control the temperature in the weld area during lap welding automatically. The system\u2019s primary goal is to maintain a consistent temperature while joining two plastics with varying optical properties. This is done by using a National Instruments CompactRio controller with real-time reconfigurable features, as well as input-output modules. The controller application makes use of fuzzy logic control, and it was created through the use of LabVIEW software. The system\u2019s sampling rate used was 1kHz, while the feedback signal was obtained through an optical pyrometer that measures the temperature in the fusion zone. \n\nThrough empirical testing, the optimal settings for the system were determined, and the controller\u2019s performance was evaluated. The article also includes waveform data that shows the process variable, the control signal, and the error signal.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Clustering protocols have become increasingly important in heterogeneous wireless sensor networks (HWSNs) due to their ability to handle critical applications. In this study, we propose an extension of MICHB (Modified Intelligent cluster head election based on Bacterial foraging optimization) for three-level HWSNs. To achieve this, we apply the MICHB algorithm to the existing protocols (ESEP and EDEEC), and propose MIESEP and MIEDEEC protocols that search for sensor nodes (SNs) with high residual energies for cluster head (CH) elections in distributed HWSNs. As a result, the proposed protocols minimize energy consumption, prolong the stable period, and ensure proper load distribution in the network. Simulation results confirm that our proposed MIESEP and MIEDEEC protocols significantly improve the overall network performance in distributed HWSNs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Currently, SDN controllers are lacking in cognitive capabilities. In response to this challenge, a new architecture for an SDN controller has been proposed, focusing on enabling intelligence through the use of Multi-agent systems.\n\nTo demonstrate this proposed architecture, a prototype MAS-SDN controller has been developed utilizing agent programming language GOAL. This new architecture seeks to address the limitations of existing SDN controllers by enhancing their cognitive abilities.\n\nThe motivation behind this new architecture is to enable SDN controllers to effectively handle complex network environments through enhanced intelligence. The Multi-agent system provides a highly flexible and adaptable framework, which can be used to develop new capabilities and services which traditional SDN controllers lack.\n\nInitial results show promising outcomes, indicating that the MAS-SDN controller can adapt to network changes in real-time, and is capable of using intelligence to improve network performance. Overall, this new architecture offers an important step forward in the development of more advanced and intelligent SDN controllers.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Brain-Computer Interface (BCI) technology integrated with assistive robots has recently emerged as a promising approach for stroke rehabilitation. However, the current studies in this field mostly involve complex system setups, expensive and bulky devices. In this study, we developed a wearable Electroencephalography (EEG)-based BCI system to improve hand function in stroke patients. Our system comprises of a customized EEG cap, a small-sized commercial amplifier, and a lightweight hand exoskeleton. Additionally, we designed a visual interface for easy user interaction. To validate the safety and efficacy of our proposed system, we recruited six healthy subjects and two stroke patients. Our study achieved an online BCI classification accuracy of up to 79.38%, providing proof of concept for future clinical applications in outpatient settings.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Annotation of perceived emotions in music pieces is necessary for the implementation of an automatic music emotion recognition system. However, most existing music emotion datasets are focused on Western pop songs, which presents a significant challenge for the algorithm to recognize emotions in non-Western pop songs due to cultural differences in emotion perception and acoustic characteristics. Previous cross-dataset and cross-cultural analyses have confirmed this issue, yet there is a lack of research on how to adapt pre-trained emotion recognition models to non-Western music genres.\n\nTo address this challenge, we propose an unsupervised adversarial domain adaptation approach that utilizes neural network models for making the target music genre indistinguishable from the source music genre in a learned feature representation space. Since emotion perception is multifaceted, we consider three types of input feature representations related to timbre, pitch, and rhythm for the evaluation of our method's performance. The experimental results demonstrate that our method effectively enhances the prediction accuracy of valence in Chinese pop songs using a model pre-trained on Western pop songs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Advancements in automated face analysis have made possible the assessment of viewer emotion during the presentation of commercials and other video content through the use of webcams. However, the key assumption that viewer emotion is solely in response to the media being presented may not be warranted. The reason for this is that emotional responses could stem from other sources, such as talking to a friend, enjoying a meal, or tending to a pet, particularly since viewer attention is rarely assessed. In response to this, a CNN-LSTM approach was developed that detects both attention and non-attention to commercials through the use of webcams and mobile devices in a variety of settings. To account for cultural variations in viewer response, participants were selected from both Western and Eastern countries. The study involved 28,911 adult participants aged 18 to 69 years across Europe, the USA, Russia, and China, and analysed 15,543 sessions (equating to approximately 6.5 million video frames). The accuracy of the approach was quantified and compared to that of human annotators, and the results showed that it outperformed baseline metrics, achieving moderate to high accuracy that approached that of human annotators.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent studies have shown great interest in identifying and classifying Power Quality (PQ) events, especially in Distributed Generation Systems (DGs), in order to improve their overall performance. In order to achieve this goal, researchers utilize a set of signal processing techniques to perform a detailed analysis of real-time electrical signals, primarily focusing on two major parameters: PQ distortions caused by environmental factors such as changes in wind speed and solar irradiation. Signal features are extracted using the S-Transform technique while signal classification is conducted using the Least Square Support Vector Machine (LS-SVM) technique.\n\nTo test these methods, a 17-bus test system was modeled using the Open Distribution System Simulator (OpenDSS) software while LS-SVM and S-Transform were implemented using MatLab. The signal classifier provides input for smart converter control, which ensures proper grid-support functions are initiated based on the type of PQ event detected.\n\nOverall, this approach has shown promising results in identifying and classifying PQ events in real-time, and can be utilized to initiate suitable control actions to achieve improved performance of DGs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present an improved version of the Taylor model for opinion dynamics in social networks. Drawing inspiration from the concept of bounded confidence models, we have introduced an extended Taylor model in which the influence graph is constructed based on individuals' confidence levels. In this way, the model can better capture the impact of social influence on opinions.\n\nTo further examine the dynamics of this model, we have established a relationship between its structural controllability and the containment of individuals' opinions by the sources. This has allowed us to develop an algorithm that can determine the minimum values of confidence levels required to achieve both structural controllability and containment.\n\nFinally, we validate our proposed scheme with an illustrative example, demonstrating the usefulness of our extension to the Taylor model. Ultimately, this work provides a valuable addition to the field of opinion dynamics in social networks, offering a more accurate and nuanced understanding of how opinions are formed and influenced.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Image stitching is a crucial aspect of computer vision, and there is a growing interest in finding more efficient and high-quality methods for this process. In this paper, the authors introduce a new approach known as TMGA, which utilizes Genetic Algorithm to calculate the Transform Matrix. The TMGA method takes into account not only the number of interior points but also considers the standard error and degree of dispersion, distinguishing it from the traditional approaches. The results of the study validate the effectiveness of TMGA as it produces a higher quality transform matrix leading to better stitching outcomes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Falls are a common accident that elderly people face when living independently. To reduce injuries and prevent loss of life, the timely and accurate detection of falls is critical. This paper aims to enhance the current smartphone-based fall detection systems by investigating the triaxial acceleration values acquired from the built-in accelerometers of a smartphone. Crucial thresholds between fall and non-fall events are identified, and an enhanced threshold-based fall detection approach is proposed. This approach can distinguish fall events from daily activities such as walking, running, and sitting down and support falls in four directions (forward, backward, left lateral, and right lateral). Furthermore, when a falling accident is detected, the user's position is immediately transmitted to an emergency center for timely medical assistance. Experimental results have demonstrated the feasibility of the enhanced approach with 99.38% accuracy and 96% detection rates. Our approach was tested against a set of 650 daily activities, including 11 different kinds of activities.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Introduction: This paper presents a framework for visualizing and modeling internal fixation plates in computer-aided personalized curved bone fracture surgery. This approach aims to improve the accuracy and precision of surgeries while minimizing invasiveness.\n\nMethods: The method involves a personalized reverse reconstruction of the bone fracture plate using 3-D mesh models obtained from a 3-D optical scanner. First, the Bezier curve (ridge line) of broken bones is reconstructed using principal component analysis and the K-means method. Next, a capsule projection model of the broken bones is created to obtain feature information of the broken bone sections. The ordering points to identify the clustering structure (OPTICS) method is then used for rough registration. A regional self-growth strategy is designed to extract the cross-section points, followed by the application of the iterative closest point method for accurate registration of the fracture surface models. Finally, a personalized internal fixation plate model is reconstructed based on several user points.\n\nResults: The framework successfully reconstructs a personalized internal fixation plate model according to the patient's bone parameters. This allows clinicians to obtain accurate representations of broken bones, which can be fixed into the target area through a small incision via X-ray navigation.\n\nConclusion: This framework enables clinicians to obtain personalized and precise internal fixation plate models that effectively represent a patient's broken bones. It offers a practical technical approach for computer-aided minimally invasive curved bone fracture surgery.\n\nSignificance: The framework described in this paper provides a reasonable and practicable approach for improving patient outcomes in curved bone fracture surgeries. It has the potential to significantly reduce the invasiveness of surgeries while improving accuracy and precision.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We present a distributed framework designed to address resource sharing problems in communications, microeconomics and networking applications. Our focus is on network utility maximization (NUM), and we propose a hierarchical multi-layer decomposition approach where functionalities are assigned to different layers. Our proposed methodology results in solutions that have central management and distributed computations, specifically tailored to respond to the dynamics of the network by decreasing communication costs and shifting more computational load to network edges.\n\nThe main contribution of our work is a detailed analysis of our framework, under the assumption that network changes occur at the same time-scale as the convergence time of the algorithms used for local computations. Assuming strong concavity and smoothness of user objective functions, we present convergence rates for each layer. This approach provides an innovative and effective solution to problems related to resource sharing in complex networks, addressing the challenges of distributed communication and computation in a dynamic environment.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing is a service that provides shared virtual resources and data for computation. To ensure that the servers have enough resources when a request comes in and to save server resources, we propose a prediction-based server capacity planning and dynamic scheduling algorithm. This algorithm has three main steps. First, the data is analyzed and a model is developed to effectively predict future demands. Second, workload is generated based on predicted demands to make capacity planning for the servers. The accuracy of the prediction plays a crucial role in the effectiveness of capacity planning. Finally, a demand prediction-based strategy for workload allocation is developed. This strategy incorporates dynamic resource allocation to ensure quality of service while minimizing energy consumption. Experiments show that server numbers decreased by 33% after applying the prediction-based capacity planning algorithm to server scheduling.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the field of Lithium-ion (Li-ion) battery technology, one of the biggest challenges is increasing battery longevity. To address this challenge, we propose the development of a health-conscious advanced battery management system that would implement monitoring and control algorithms to increase battery lifetime while maintaining performance. The key to these algorithms is real-time battery capacity estimates. To this end, we present an online capacity estimation scheme for Li-ion batteries. \n\nThe novelty of our approach lies in two key aspects: first, we leverage thermal dynamics to estimate battery capacity; and second, we develop a hierarchical estimation algorithm that has provable convergence properties. Our algorithm consists of two stages that work in cascade to provide accurate estimates of battery capacity. \n\nIn the first stage, our algorithm estimates battery core temperature and heat generation based on a two-state thermal model. In the second stage, the estimated core temperature and heat generation values are used to estimate state-of-charge and capacity. Numerical simulations and experimental data demonstrate the effectiveness of our proposed capacity estimation scheme.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This research paper delves into the use of machine learning methods in finance for the prediction of Seasoned Equity Offerings (SEO) through the analysis of a firm's 10-Q filing documents with the Securities and Exchange Commission (SEC). By utilizing the information content in the Management Discussion and Analysis section (MD&A) of these filings, five algorithms were trained, including Logistic Regression (LR), Support Vector Classification (SVC), Multinomial Na\u00efve Bayes (NB), Artificial Neural Network (ANN), and Random Forest (RF). Unigrams and phrases were taken into consideration as features, with term frequency-inverse document (TF-IDF) scores being utilized as independent variables in the models. Experimental results indicate that the phrases-only models show an improved accuracy of 0-2% for LR, NB, and RF when compared to the unigrams-only models. The phrase-only model for SVC is close in accuracy to the unigrams-only model. The best performing classifier is SVC, with an accuracy of 74.53% for the unigrams-only model. The precision of all models ranged between 60%-75%, while the recall varied from 55%-85%. Model parameters of one linear model (LR) and one non-linear model (RF) were fine-tuned to observe their impact on the models' performance. Finally, RF was utilized to determine the most important features for prediction, with \"merger\" being deemed the top feature in both unigrams-only and phrases-only models. In conclusion, analysis of SEC financial document filings through text mining can prove to be an effective tool in predicting critical corporate events like SEO.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Detecting anomalous behavior in wireless spectrum is a complex and demanding task due to the sheer number of electromagnetic spectrum uses. Anomalies can manifest in various forms, making it difficult and suboptimal to manually label them. To overcome this challenge, we present the Spectrum Anomaly Detector with Interpretable FEatures (SAIFE), an Adversarial Autoencoder (AAE) based anomaly detector for wireless spectrum anomaly detection using Power Spectral Density (PSD) data. This model achieves good anomaly detection and localization in an unsupervised setting, and also possesses the capability to learn interpretable features, such as signal bandwidth, class, and center frequency, in a semi-supervised fashion.\n\nAdditionally, the SAIFE model exhibits promising results for lossy PSD data compression up to 120X and semi-supervised signal classification accuracy close to 100% on three datasets, using only 20% labeled samples. Finally, the model has been tested on data from one of the distributed Electrosense sensors over a long term of 500 hours, showing its reliable anomaly detection capabilities.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Smart healthcare technology is an area that has been explored extensively, utilizing modern computing technologies and techniques in healthcare research. The incorporation of sensors in smart wearable devices has enabled patient-generated data to be directly transmitted to electronic devices or health records, allowing doctors and caregivers to monitor patient activity in real-time. With a high volume of medical information continually produced, it has become essential to gather, store and learn from the data to predict patient health accurately.\n\nOne particularly concerning area is the increasing number of diabetic patients. In response, a Cloud IoT-based framework for diabetes prediction has been proposed. This framework utilizes sensors in smart wearable devices as a set of connected IoT devices for continuous monitoring and collection of blood glucose data. This data is then sent for storage in a cloud environment where an ensemble model is used to predict diabetes in patients.\n\nAn experiment was conducted on ten ensemble models by pairing two out of five different machine learning methods. The best performing model consisted of a Decision Tree and Neural Network and achieved an accuracy of 94.5% when evaluated using the \"Pima Indians Diabetes\" data set.\n\nIn conclusion, the implementation of smart healthcare technology has been an essential development in healthcare research. With the incorporation of sensors in smart wearable devices and the use of cloud-based storage and ensemble models, it is possible to predict and prevent medical conditions such as diabetes more accurately.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapid advancements in natural language processing technologies, there has been an increasing number of text steganographic methods based on automatic text generation technology in recent years. These methods utilize the self-learning and feature extraction abilities of neural networks to learn the feature expression of vast amounts of normal text. Consequently, the models can generate dense steganographic texts that follow statistical distribution based on the learned statistical patterns. In this letter, we have observed that the conditional probability distribution of each word in the automatically generated steganographic texts gets distorted once embedded with hidden information. Thus, we have employed recurrent neural networks to extract these feature distribution changes and classify them into cover text and stego text categories. The experimental results demonstrate that our proposed model can achieve high detection accuracy. Moreover, our proposed model can effectively estimate the amount of hidden information embedded in the generated steganographic text by leveraging the subtle differences in the feature distribution of texts.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a cutting-edge framework that focuses on enhancing privacy and security in power trading within networked microgrids. The proposed approach is based on the blockchain-enabled Internet of Things (IoT). By utilizing this technology, the power trading system in networked microgrids is expected to reap significant benefits such as low system risks, reduced financial fraud, and decreased operational costs. \n\nTo address uncertainties in renewable energy resources and hourly load demand, a stochastic framework based on the unscented transform (UT) is employed. The proposed technique is tested on a networked microgrid consisting of a residential MG (non-crucial), commercial MG (intermediate), and hospital MG (crucial). The results show the effectiveness and high performance of the proposed framework.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent times, mobile phones have been equipped with advanced onboard sensors that make it easier to connect and compute. This has paved the way for new application paradigms such as crowd sensing and crowd computing. Crowd sensing relies on the sensing capabilities of mobile devices and their communication efficiency to collect data and transfer it to the cloud for further processing. On the other hand, mobile crowd computing involves integrating machine and human intelligence to perform distributed tasks. In our work, we have developed a novel approach that combines these two paradigms into a framework that addresses both mobile sensing and crowd computing challenges simultaneously. This approach utilizes the crowd's problem-solving abilities without cloud servers in the backend. We implemented this framework using four handheld devices for a route-finding application. These devices are connected through Bluetooth Low Energy technology. The results can be obtained both online, through machine intelligence, and offline, through human intelligence, when the devices are not connected to the internet. Additionally, the device receiving the information can itself contribute to the crowd for other route-finding queries solicited by another user in the crowd.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The characterization of collagen deposition in immunostained images is of great importance in various pathological conditions, especially in HIV infection. Achieving accurate segmentation of collagens and extracting representative features from them is crucial for achieving quantitative diagnosis of underlying diseases. While a first-order statistic derived from segmented collagens can represent pathological evolutions at different timepoints, it cannot capture morphological changes and spatial arrangements effectively. In this study, we present a comprehensive pipeline for extracting key histopathology features that represent disease progression in histopathology whole-slide images (WSIs) using deep learning and graph theory integration. We trained and utilized a convolutional neural network for histopathological WSI segmentation. Parallel processing was applied to convert 100K ~ 150K segmented collagen fibrils into a single collective attributed relational graph, and graph theory was applied to extract topological and relational information from the collagenous framework. The results are consistent with the expected pathogenicity induced by collagen deposition, demonstrating great potential in clinical applications for analyzing various meshwork structures in whole-slide histology images.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Firewalls play a crucial role in ensuring the security of private and critical systems and infrastructures. However, the effectiveness of a firewall is heavily reliant on its configuration and filtering policy. To enhance the efficiency of access control solutions, researchers have developed several generations of firewall technologies. One such innovation is the FW-TR firewall, which integrates a trust-risk assessment approach into firewall solutions.\n\nThe FW-TR firewall assesses the trust-risk associated with the filtering rules and policy. This approach has several benefits, including strengthening the quality of the firewall filtering service, identifying misconfigurations, detecting anomalies in firewall rules, and adjusting the firewall behavior in response to critical and malicious scenarios. \n\nTo incorporate the trust-risk values of policies and rules in firewall solutions, a framework has been developed for organizing thinking about FW-TR firewalls. This framework defines FW-TR as the new generation of firewalls that can provide reliable and efficient security solutions.\n\nOverall, the integration of trust-risk assessment in firewall solutions provides a novel approach to enhance the efficiency of access control and strengthen the security of private and critical systems and infrastructures.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Alzheimer's disease (AD) is the most common form of dementia, and while researchers understand that it involves a gradual loss of neuronal cells, the exact cause remains unknown. Because AD affects the brain at a systems-level, the brain's structural connectome - derived from whole-brain tractography using diffusion-weighted MRI - has the potential to help researchers study the changes associated with AD progression. However, current methods of reconstructing the connectome are limited to one single tractography algorithm and can overlook important graph structure. In this study, we proposed a new approach using multi-view structural connectomics, which showed improved power in detecting early changes associated with AD progression.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The main objective of this paper is to establish the accuracy of a method for evaluating power density in close proximity to wireless communication devices operating above 6 GHz. To achieve this, the authors focused on a near-field reconstruction technique that estimates power density in the vicinity of a wireless communication device by using measurements of the electric field at a plane which is several wavelengths away from the device.\n\nThe paper begins by validating the technique through a comparison of the results obtained using this reconstructed method and those obtained through computational simulation for a standard horn antenna. The study proceeds to assess the reconstruction errors of the technique using ten planar array antennas at frequencies ranging from 15 to 100 GHz. The authors note that the maximum spatially averaged power density at a separation distance of over 0.15\u03bb from the antennas using an averaging area of \u03bb2 or larger had reconstruction errors no larger than 0.35 dB.\n\nFinally, the authors examine the requirement for electric field measurement and suggest a combined error for the compliance assessment of power density for an actual testing scenario. These results are significant and provide valuable insight into the standardization of compliance assessment techniques for wireless communication devices operating above 6 GHz, which are anticipated to be introduced in the future. In conclusion, the authors' research supports the implementation of such techniques and raises the possibility of more accurate and reliable methods for ensuring that wireless communication devices operate within radio-frequency exposure guidelines.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the online apparel market, one of the biggest issues is the high return rate due to size mismatches. To address this problem, various virtual garment technologies have been developed, but most of them have not been suitable for online marketplaces due to network latency issues, high user intervention, and computational power requirements. However, a lightweight solution has been proposed in this paper, which allows end-users to select perfectly fitted garments based on the consideration of human body measurements and garment measurements.\n\nThe suggested fitting model includes human body and finished garment 2D patterns, which can identify the fitness of a selected garment and display it using seven predefined areas, along with user preference and distance ease values. Although this research has only focused on generating the fitting model for short-sleeve men's shirts, the procedure of generating a 2D block pattern from a finished short-sleeve shirt has been established through an experiment, and standard error values have been recognized.\n\nTo evaluate the implemented model, it was tested on 20 participants using qualitative approaches, resulting in an accuracy of 81.2%. Additionally, a survey was conducted to assess the visualization of the system output, and 76% of the responses were positive.\n\nOverall, this lightweight solution provides end-users with an effective tool to help them make informed decisions when choosing apparel in online marketplaces, ultimately reducing the size mismatch issues and the need for returns.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The \"STEAM Education of Flying and Assembly of Drone\" course is a comprehensive program that spans 12 hours of instruction. It encompasses a variety of modules, including the Drone Flight System, Simulation Flight of Drone, Innovative Applications in the Field of Drone, Assembly and Maintenance of Drone, Practical Flight of Drone, and using the \"Flying Competition to check the effectiveness of learning. Through these modules, a teaching paradigm and mechanism has been established which serves as an excellent reference for other STEAM designs.\n\nThe focus of this course is to integrate imagination, creativity, and innovation into UAV STEAM, fostering innovative thinking, practical assembly ability, and an active learning attitude among students. The Chicago Arts Partnership in Education (CAPE) method is employed to review and revise the curriculum content and learning effectiveness in a four-stage cycle that includes exploration, collaboration, documentation, and reflection.\n\nBy following this process, imagination and innovative thinking are properly integrated into the UAV STEAM curriculum, inspiring students to develop their creativity and imagination in learning. Overall, this course provides an excellent opportunity for students to gain a deeper understanding of drones, hone their practical skills, and foster a passion for innovative thinking and problem-solving.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Structured light with spatially varying amplitude, phase, and polarization has found a broad range of applications, including manipulation and communication. This invited talk reviews the latest advances in structured light communication, focusing on accessing the space domain of light waves. The talk will cover key devices, techniques, and emerging applications. We will also discuss the future challenges and prospects of structured light communication.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless sensor-actuator networks (WSANs) have become an appealing technology for use in industrial Internet of Things (IoT) applications due to their ability to operate without wired infrastructure. Battery-powered wireless modules are easy to retrofit in existing sensors and actuators in industrial facilities without the need for communication and power cabling. The WSANs that are based on IEEE 802.15.4 have the added advantage of being low-power and manufactured inexpensively, making them ideal where battery lifetime and costs are important. The WirelessHART standard has been successful in demonstrating the feasibility of using reliable graph routing and time slotted channel hopping (TSCH) to achieve reliable low-power communication in industrial facilities.\n\nAs we embrace the fourth Industrial Revolution, which is also known as the Industry 4.0 Initiative, there is an increasing demand for deploying numerous field devices in industrial facilities and connecting them through WSANs. However, the current WSAN standards have a major limitation due to their centralized routing and scheduling that enhance network predictability and visibility at the expense of scalability. In this regard, this paper presents the first Distributed Graph routing and autonomous Scheduling (DiGS) solution that decentralizes network management in WirelessHART. This solution allows field devices to compute their own graph routes and transmission schedules, resulting in significantly improved network reliability, latency, and energy efficiency under dynamics. The experimental results from two physical testbeds and a simulation study demonstrate the effectiveness of this approach.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The human footprint serves as a unique biometric system that can be utilized for individual identification. Unlike password-based authentication systems, the footprint-based system is highly secure as it verifies the validity of the person rather than just the password. This makes it an interesting candidate for user authentication in financial transactions or other security systems. To develop an effective identification system that makes use of the footprint image, this paper proposes the use of convolutional neural network training. CNNs are highly suited for deep learning and image recognition, making them an essential tool for developing robust footprint recognition algorithms. Overall, incorporating the footprints as a biometric authentication factor can significantly improve security in various domains.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The deployment of data centers and cloud resources across the globe has resulted in a rise in energy costs, communication costs, cooling costs, and carbon dioxide consumption. To address this issue, we have created a scientific workflow-based cost-effective paradigm that utilizes a rigorous mathematical model. Our approach involves the implementation of various techniques to increase the utilization rate of the system without impacting performance. One of these techniques is Dynamic Voltage and Frequency Scaling (DVFS), which scales down power consumption by calculating the optimal frequency for cloud servers. Additionally, we have leveraged cloud-based VMs to execute more scientific workflows with fewer cloud servers, thereby conserving energy costs, carbon dioxide emissions, and electricity costs. This approach is achieved by eliminating the overhead of sharing several VMs' capacity on a single server. Furthermore, our model achieves these objectives without degrading the Quality of Service (QoS) specified in Service Level Agreement (SLA). We have tested our heuristic using CloudSim and compared it with algorithms such as Rank and EARES-D. Our results show that our paradigm model is better than other heuristics, with an average energy reduction of 70%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this letter, we explore device-to-device (D2D) communication in downlink cellular networks, where the cellular user (CU) is responsible for both decoding information and harnessing energy. Unlike traditional D2D communication approaches, we have found that it is possible for the D2D pair to communicate without adversely affecting the CU's performance. Consequently, we propose D2D transmit power control schemes that maximize D2D rates while ensuring the CU's performance is maintained. These decentralized schemes are implemented at the D2D transmitter (DT) and rely on instantaneous channel state information (CSI) of only the link between the D2D pair. To analyze performance, we provide an asymptotic upper bound for the D2D rates of the proposed schemes. Numerical results demonstrate the efficacy of our proposed approach.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Traffic congestion and road obstructions are major issues in modern cities, leading to an increase in traffic accidents. As a result, traffic flow management is crucial to avoid these issues, reduce time wastage, and prevent tragic accidents. Optimizing the timing of traffic signals is one solution to this problem. This paper proposes a low-cost camera-based algorithm to manage traffic flow on roads. The algorithm involves three main steps: vehicle detection, counting, and tracking. Background subtraction is used to isolate vehicles from their surroundings, while Kalman filtering tracks the motion of the vehicles. To associate labels with the tracked vehicles, the Hungarian algorithm is employed. This algorithm is applied to both day and night videos captured by CCTV and IR cameras. The experimental results demonstrate the effectiveness of the proposed algorithm.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Traffic congestion is recognized as a significant problem in modern urban cities, and Jordan is considered one of the top countries worldwide in terms of frequency of traffic accidents. The increase in the number of vehicles on the roads has led to a rise in the number of traffic incidents, resulting in an increase in fatalities and injuries. Despite this, the current traffic signals system in Jordan is still controlled by fixed timers.\n\nTo address this issue, an Intelligent Road Traffic Management System based on Human Community Genetic Algorithm (IRTMS) has been proposed. The IRTMS was compared to the current traffic system in the Hashemite Kingdom of Jordan and found to have a minimum total time and waiting time compared to the current traffic lights system.\n\nThis automated road traffic management system employs genetic algorithms to generate optimized traffic light timings. The algorithm works by mimicking the natural selection process that is responsible for the evolution of species where the fittest individuals survive and the weaker ones perish. In the same way, the genetic algorithm selects the optimal timings that can accommodate the maximum number of vehicles passing through a particular intersection.\n\nThe IRTMS considers the real-time traffic conditions, such as the number of vehicles, their speeds, and the road network layout. This information is collected from various sources, including roadside sensors, surveillance cameras, and GPS-equipped vehicles. The system then processes all this data and calculates the optimal traffic signal timings.\n\nThe IRTMS has several advantages over the current traffic signals system. Firstly, it reduces traffic congestion by minimizing the waiting time for vehicles at intersections. Secondly, it minimizes the total time spent by vehicles at intersections, thereby reducing fuel consumption and air pollution. Thirdly, it improves traffic flow, reduces travel time, and enhances road safety by minimizing the potential for traffic accidents.\n\nIn conclusion, the Intelligent Road Traffic Management System based on Human Community Genetic Algorithm (IRTMS) is a promising approach to address traffic congestion and improve road safety in modern urban cities. By utilizing real-time traffic data and optimizing traffic signal timings, this system has the potential to reduce waiting times for vehicles, minimize total time spent at intersections, and improve traffic flow, ultimately leading to safer roads and a better quality of life for citizens.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing offers organizations the convenience of accessing IT services on a pay-per-use basis without the need to purchase hardware or software. However, cloud services are often governed by Quality of Service (QoS), user budget and deadlines, efficient utilization of cloud resources, and energy consumption. To address the issue of energy consumption, various techniques have been developed.\n\nOne such technique is Evolutionary Algorithms (EA) which include Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), and Genetic Algorithms (GA). These algorithms aim to minimize energy consumption in cloud data centers while ensuring optimal performance.\n\nPSO is a population-based algorithm that simulates the behavior of a swarm of particles to find the optimal solution. ACO is inspired by the behavior of ants searching for food, where they leave a pheromone trail to guide others. Similarly, ACO uses pheromones to guide the search for the optimal solution. GA, on the other hand, mimics the process of natural selection by evolving a population of solutions through crossover and mutation.\n\nEach algorithm has its strengths and weaknesses. For instance, PSO is easy to implement and can handle high-dimensional problems, but it may get stuck in local optima. ACO is effective in finding optimal solutions, but it may suffer from premature convergence. GA is global in nature and can handle multi-objective optimization, but it may require more computational resources.\n\nIn conclusion, the review of techniques for reducing energy consumption in cloud data centers has provided valuable insights into the strengths, weaknesses, and target objectives of each algorithm. The future research directions can focus on combining these algorithms to achieve better performance in terms of energy consumption and QoS.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep Neural Networks (DNNs) have proven to be highly effective in solving artificial intelligence problems. However, reducing the resource usage of DNN is essential in adding intelligence to Internet of Things (IoT) devices. Channel pruning based network compression has shown to be an effective method for reducing storage, memory, and computation without specialized software on general platforms. However, pruning flexibility limits the compression rate for a given target performance.\n\nIn this paper, we propose an approach to enhance channel pruning by reducing the granularity of filters, which improves its robustness to decision errors. We introduce a Decouple and Stretch (DS) scheme, which decouples each filter in a specific layer into two small spatial-wise filters. These spatial-wise filters are then stretched into two successive convolutional layers. Our scheme leads to up to 49% improvement in compression and 35% improvement in acceleration.\n\nTo further demonstrate hardware compatibility, we deployed pruned networks on the FPGA. The network produced by our DS scheme is more hardware-friendly, resulting in a latency reduction of 42%. In conclusion, our proposed DS scheme is an effective approach to enhance channel pruning-based network compression, leading to improved compression, acceleration, and hardware compatibility.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The increasing complexity of production processes has led to a growing need for a skilled workforce. As a result, companies are under pressure to train their employees and ensure they possess the required technical knowledge to manage, operate and interact with new machines and media. Digital literacy is now considered essential within fast-paced and continuously changing environments.\n\nThe organizational learning infrastructure plays a critical role in generating knowledge and facilitating the development of employees. However, research on production workers and their learning motivations remains underrepresented in current literature, despite the increasing attention given to personal and professional development, particularly in light of the digital transformation of workplaces.\n\nThis study aims to address this imbalance by assessing the challenges that production workers face and evaluating their interaction with media in the workplace. It also seeks to identify factors that influence their learning motivations. The research was conducted through workshops, multi-perspective interviews and a questionnaire-based survey in the German automotive industry.\n\nThe findings from this study are significant as they contribute to shaping a digital learning infrastructure that enables continuous knowledge management in production. The insights generated are critical in supporting workers' development and enhancing productivity in the industry.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurate classification and effective recognition of driving styles are crucial for improving the powertrain control performance of vehicles. In this study, a set of driving style classification and recognition methods were developed based on feature engineering. First, a specified road test was conducted to consider the influence factors, and simultaneously, corresponding driving data was collected followed by a detailed evaluation of the driving styles. Subsequently, the driving data, including the speed, acceleration, and opening degree of the accelerator pedal, were discretized using the information entropy, and 44 feature quantities were extracted to characterize the driving style. Principle component analysis was employed to reduce the dimension, and the fuzzy c-means clustering algorithm was used to classify the driving style. The successful classification rate improved by 9.81% in comparison to traditional features, attaining 92.16%. Finally, support vector machine-based parameter identification algorithm was applied, and the recognition accuracy reached 92.86%, improving by 7.15% in comparison with traditional features, demonstrating the feasibility of the proposed algorithm.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the realm of the Internet of Things (IoT), edge-cloud computing has emerged as a viable solution for effectively sharing computation resources due to the limited computing power of terminal equipment. By offloading partial computations to edge servers, terminals conserve energy and reduce data transmission delays as edge servers are typically stationed in close proximity to the terminal devices. Despite the benefits of existing solutions, significant data transmission is still necessary when offloading jobs from several terminals, particularly when the data used is related. To combat this, we propose a novel cache-aware computation offloading strategy with a shareable cache at the edge server to further minimize data transmission cost and job delay. \n\nOur research focuses on the computation offloading method with cached data and the corresponding cache-aware computation offloading location problem. The objective is to minimize the equivalent weighted response time of all jobs, given computing power and cache capacity constraints. We formulate the problem, derive the global optimum solution based on transformation to the transportation problem, and propose an online computation offloading strategy for practical deployment wherein global status information is difficult to obtain. \n\nIn the end, our experiments demonstrated that the online offloading strategy approximates the global optimal solution and reduces the weighted response time by an average of 26.13%, compared to other competing algorithms. Our proposed cache-aware computation offloading strategy represents a significant advancement towards more efficient edge-cloud computing in IoT environments.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rise of emerging technologies and 5G mobile communication methods, IoT applications have evolved significantly. This paper presents an overview of the new characteristics and challenges in mobile edge computing and summarizes the latest related models and work. The paper focuses on analyzing and discussing important optimization models as well as the use of moving models and wireless block data in mobile edge computing.\n\nBuilding on this foundation, the paper designs and verifies three aspects of mobile edge computing. Firstly, a joint optimization model for task offloading and power allocation is established, and a centralized joint optimization algorithm is proposed. This algorithm uses idle resources to distribute and unload computing tasks, thereby improving the balance between system delay and energy consumption.\n\nSecondly, delay-tolerable data is modeled as a partially observable Markov decision process in a software-defined transport and compute node selection process. Compared to existing schemes, this method effectively reduces system overhead, shortens data calculation execution time, and improves data calculation efficiency.\n\nLastly, the paper addresses the challenge of ensuring that delay-tolerant data transmission can tolerate transmission delay. By developing a new algorithm, the paper demonstrates that it is possible to achieve this while also improving data calculation execution time.\n\nIn summary, this paper provides significant insights into mobile edge computing and offers solutions to key challenges. Through its various models and algorithms, it offers a roadmap for future research and development in this exciting area.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The modern digital society is characterized by the widespread use of internet communications in the everyday lives of all social groups. However, this has led to an increased level of information noise, posing a challenge for communication practitioners to overcome (Joseph Turow, Couldry, N.). To address this problem, experts have also reviewed the importance of communication hygiene. In order to formulate an effective communication strategy in the current communication pattern of digital society, case studies of big data and smart data are presented as valuable tools.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Pyramid schemes, as an organized criminal organization, have resulted in enormous economic losses in many countries. An effective method to study organized crime is social network analysis, and the exponential random graph model is an excellent tool to examine the endogenous structure and network process of the organization. This study is the first to use the exponential random graph model to investigate the communication network of a particular pyramid scheme. The findings reveal that the communication network of pyramid schemes is sparse, and the central staff has lesser contact with the peripheral ones.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This study highlights the improvements achieved in the Sample Matrix Inversion (SMI) beamformer's performance through the utilization of the Reduced One Dimensional Mapped Real Transform (R-ID-MRT) algorithm. By transforming data with the R-ID-MRT algorithm, computational complexity and computation time are decreased in SMI beamformers. The R-ID-MRT SMI beamformers exhibit beam patterns, convergence to the desired beam output, and mean square error minimization in AWGN, Rayleigh, and Rician channels that are equivalent to those of traditional SMI beamformers.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Linear Discriminant Analysis (LDA) has been widely used to enhance the performance of speaker recognition systems by minimizing intra-speaker variation and maximizing inter-speaker differences. However, in this paper, we introduce a novel approach called Deep Discriminant Analysis (DDA) that utilizes a neural network to compensate for i-vector based speaker recognition.\n\nBy jointly optimizing against softmax loss and center loss, DDA learns a more compact and discriminative embedding space compared to LDA. Unlike LDA, DDA doesn't rely on the Gaussian distribution assumption of data and can learn a non-linear projection function. Through experiments conducted on a short-duration text-independent dataset based on the SRE Corpus, we observed that DDA achieves a noticeable improvement in performance when compared to the traditional LDA or PLDA methods.\n\nOverall, DDA is a promising technique that provides an effective way to extract more informative representations from speaker recognition datasets using a neural network-based compensation approach.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing and big data have become two of the most popular technologies in the modern world due to their vast applicability in various areas of interest. Among these areas, education and research hold a significant place. This research paper presents PABED (Project - Analyzing Big Education Data), a big data analytics tool designed for the education sector that utilizes cloud-based technologies. This tool, which employs Google BigQuery and R programming language, allows for comparison of undergraduate enrollment data for different academic years.\n\nWhile there are many proposed applications of big data in education, there is a lack of tools that can actualize the concept into practice. PABED is an initiative in this direction. This paper details the implementation and testing specifics of the tool. The successful use of cloud computing and big data technologies in education demonstrated by PABED sets a standard for more advanced educational intelligence tools in the future.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A multistage coil gun is a device used to launch a projectile. Increasing the velocity of the projectile can be achieved by increasing the number of stages or layers but controlling the projectile motion is crucial for regulating the velocity at the end of the barrel. To solve this problem, a non-uniform multistage coil design can be used where each coil is twisted separately and the current is injected simultaneously. However, setting the injection current is challenging since the magnetic force can either push or slow down the projectile. The proposed solution is to use the genetic algorithm method to regulate the current injection to six coils that work together effectively. This method can control projectile motion in the barrel, and the results show excellent performance with a maximum velocity of 31.43 m/s. The design is demonstrated through multistage coil simulation as a solution to regulate projectile motion inside the barrel.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a multi-chain Time of Arrival (ToA) positioning method that allows estimation of positions using all the received Loran-C signals from multiple chains, unconstrained to a single chain. Additionally, we propose an ASF estimation method based on receiver clock offset estimation. To validate these methods, we performed a test where one receiver was installed at a fixed location on land, and two receivers were placed on a vessel. We collected raw ToA measurements data from all three receivers while the vessel moved along a pre-determined path. \n\nSubsequently, we estimated the ASF using data from the land fixed receiver and measured the temporal change of ASF to generate the differential correction. The data from one of the receivers installed on the vessel was used to generate the ASF map. Loran-C multi-chain positioning was performed using the remaining receiver data. During this process, we mitigated the error by using differential correction and the ASF map. \n\nOur results showed that the horizontal positioning error was 2.12m, and the 2DRMS was 24.12m. Therefore, our proposed multi-chain ToA positioning and ASF estimation methods can effectively estimate the position without constraining to a single Loran-C chain. These methods can improve the accuracy of Loran-C positioning under challenging conditions, such as those encountered on vessels.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, opportunistic routing (OR) has emerged as a promising solution for efficient data delivery in noisy underwater acoustic channels at the network layer. Several studies have proposed OR protocols for underwater sensor networks, with varying degrees of modifications. Numerous studies examining protocol performance in terms of data delivery rate, delay and energy consumption have been conducted. However, there is a lack of research dedicated to evaluating the topological properties that OR protocols will generate. \n\nIn this paper, we evaluate the candidate selection procedures employed in OR protocols designed for underwater sensor networks. We discuss the key principles underlying the design of OR protocols for underwater sensor networks. Furthermore, we conduct a simulation-based performance evaluation to assess the topological properties, such as the number of hops, paths, and candidates, that will impact the efficiency of wireless underwater sensor network applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we introduce a new and innovative approach to deep sparse coding networks (SCN). Our network is capable of efficiently adapting its own regularization parameters for any given application, making it highly versatile. The network is designed to be trained end-to-end using a supervised task-driven learning algorithm via error backpropagation. During this training, the network learns both the dictionaries and the regularization parameters of each sparse coding layer, resulting in a smooth transformation of the reconstructive dictionaries into increasingly discriminative representations. \n\nAdditionally, the adaptive regularization allows our network greater flexibility in adjusting sparsity levels. Our system also utilizes a sparse coding layer with a \"skinny\" dictionary, which compresses high-dimensional sparse codes into lower-dimensional structures, improving computational efficiency. \n\nOur 15-layer SCN is demonstrated to have high adaptivity and discriminability on six benchmark datasets - Cifar-10, Cifar-100, STL-10, SVHN, MNIST, and ImageNet. These datasets are known to be particularly difficult for sparse coding models. Our experimental results show that our architecture outperforms traditional one-layer sparse coding models while using much fewer parameters. Furthermore, our multilayer architecture leverages the advantages of depth while sparse coding's ability to operate on smaller datasets, showcasing outstanding performance in data-constrained scenarios when compared with deep neural networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the modern era, the transmission of images, including those of televisions and videos, involves preliminary and restorative correction based on a certain law. This law was originally developed for imaging devices using cathode ray tubes, and while it differs from the law of visibility threshold dependence on stimulus magnitude, it is still used for compatibility reasons. However, instead of simply linearizing the sensations of the quantization scale, a more effective approach would be to optimize the choice of code combinations for image transmission. This methodology is particularly useful for television systems without lossy video information compression, such as those used for transmitting signals over short distances or by channels used for remote sensing of Earth's imagery.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, there has been a shift in health care towards storing Electronic Health Records (EHRs) on mobile cloud environments which combine mobile devices with cloud computing. This paradigm allows for easier sharing of medical data between patients and healthcare providers with lower costs and greater flexibility. However, there are concerns regarding the security and privacy of this data.\n\nTo address this issue, we propose a new framework for the sharing of EHRs using blockchain and the decentralized interplanetary file system (IPFS) on mobile cloud platforms. Our framework incorporates a smart contract-based access control mechanism, which ensures secure EHR sharing among patients and healthcare providers. We have implemented a prototype of our proposal using Ethereum blockchain on a mobile app with Amazon cloud computing. Empirical results have shown that our framework provides an effective solution for secure data exchange on mobile clouds while protecting sensitive health information against potential threats. Our system evaluation and security analysis also demonstrate the benefits of our lightweight access control design, which minimizes network latency and maximizes security and data privacy.\n\nOverall, our proposed framework offers a reliable solution for sharing EHRs on mobile cloud environments, while also addressing the issue of data privacy and security.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There is a growing interest in utilizing Artificial Intelligence (AI) to enhance situational awareness for disaster management. However, few studies have explored the use of AI in socio-economic recovery efforts. This study aims to investigate the development of an AI-based method for detecting socio-economic recovery by highlighting the correlations between public sentiment on social media and related market data. By analyzing the sentiment on social media platforms, this study found multiple correlations between public sentiment and socio-economic recovery activities geared towards restarting daily routines. Traditional recovery indicators, such as official statistics, often have a significant time lag before publication. Thus, leveraging the real-time and communication trend detection capabilities of social media data can significantly advance situational awareness in recovery operations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is challenging to come across research pertaining to data analytics in information systems operating in the Third Sector. Often, entities in the Third Sector rely on both private and public funds and are managed with a missionary spirit, lacking expertise in management or economics. Internal control measures are scant, loose, and difficult to implement, creating obstacles. This case study focuses on the Charities context, specifically, a critical area, highlighting the importance of and ease of using Information Technology in extracting and analyzing data to control users' current accounts in an ERPI, a residential structure for the elderly. Throughout this paper, we explore pertinent questions related to current accounts, providing answers, and discussing the proposed tests.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mutual coupling, resulting from a tight inter-sensor spacing in uniform linear arrays (ULAs), can significantly impact the accuracy of source localisation. To address this issue, sparse arrays such as coprime and nested arrays have been considered to achieve less mutual coupling and greater degrees-of-freedom (DoFs) compared to ULAs. However, coprime arrays have holes causing a reduction in uniform DoFs, while some sensors in a nested array may still be too closely located and lead to mutual coupling. This paper proposes a novel Loosely Distributed Nested Array (LoDiNA) in a three-level nested configuration with longer inter-element separation to generate a higher number of uniform DoFs and greater robustness against mutual coupling interference, making it an attractive option for DoA estimation for multiple stationary sources with noise. The proposed LoDiNA structure is simpler than existing nested arrays, and its feasibility is demonstrated through simulation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The enterprise bargaining power index is a crucial tool in resolving the issue of alliance profitability through the utilization of game theory. Evaluating bargaining ability is crucial to facilitate logistics alliance development and enhance the optimization and integration of logistics resources. This study has developed an evaluation index system for enterprise bargaining power in logistics alliances based on five key factors such as alliance enterprise value, learning ability, resources and skill input, skill dependence, and termination costs.\n\nThe research conducted by experts involved evaluating the index data, determining the weight of the indices using the analytical hierarchy process, and carrying out fuzzy comprehensive evaluation to obtain a bargaining power evaluation value for ten enterprises in a regional logistics alliance. The study then used BP neural network training for the fuzzy evaluation system with the objective of providing the enterprises with a powerful evaluation tool.\n\nIn conclusion, the development of an evaluation index system based on the identified factors provides an effective tool for evaluating the bargaining power of enterprises in logistics alliances. The fuzzy comprehensive evaluation and BP neural network training techniques proved to be valuable in providing accurate and relevant results to enhance decision-making on logistics alliance development and optimization of logistics resources.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the stock selection problem, the Sharpe ratio is a commonly used indicator, but it may not identify the portfolio with a stable upward trend as the best one due to its emphasis on returns over risk. To address this, this paper proposes the use of the trend ratio to evaluate portfolios. The trend ratio considers both the daily expected return and daily risk, and it evaluates each portfolio by comparing its trend line to the initial funds investment. This approach allows for a fair comparison of different portfolios and different investment periods. \n\nIn addition to the trend ratio, this paper presents different investment methods, including time deposit, buying round lots or buying odd lots. The appropriate investment method depends on the situation. To avoid the overfitting problem, the paper applies the 2-phase investment sliding windows and uses Global-best Guided Quantum-inspired Tabu Search with Not Gate (GNQTS) to choose the best investment method through multiple training. \n\nExperimental results demonstrate that the proposed method effectively and efficiently identifies the well-performing portfolio with higher return and lower risk in both the training and testing periods. Overall, the trend ratio combined with diversified investments and advanced optimization techniques can improve the accuracy of portfolio selection and enhance investment outcomes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep learning has become increasingly popular in real-world applications, yet most networks lack rigorous mathematical derivations and clear interpretations. Recent studies have attempted to build deep models by unrolling optimization models that include task information, but the dynamic nature of network parameters results in deep propagations that lack the nice convergence property of the original optimization scheme. In this study, a generic paradigm was developed to unroll nonconvex optimization for deep model design. Unlike existing frameworks, which replace iterations with network architectures, it is proven in theory that the propagation generated by this proximally unrolled deep model can globally converge to the critical-point of the original optimization model, even with partially available task information. The framework can successfully handle many real-world applications, as shown through experiments on various low-level vision tasks such as non-blind deconvolution, dehazing, and low-light image enhancement, outperforming state-of-the-art approaches.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The increasing need for executing diverse applications on vehicles has brought about the development of Intelligent Transportation Systems (ITS). However, the on-board units of vehicles do not have the capacity to handle the complex computation and high processing speed required by these applications. This is where cloud computing comes in, to execute these resource-intensive applications. However, the physical distance between the cloud and users causes undesired service delays. To address this, cloudlets have been introduced, reducing the physical distance and improving service quality. Similarly, third-party infrastructure is needed to meet the ever-increasing demand for cloud resources.\n\nVehicular cloud computing is a concept that leverages the on-board units of smart vehicles as third-party infrastructure for executing large applications. In this work, we present a three-tier vehicular cloud architecture, with the lower layer being formed by on-road vehicles, the middle layer consisting of cloudlet servers associated with roadside units, and the topmost layer being the centralized cloud. To allocate resources, Particle Swarm Optimization (PSO) is used to optimize resource allocation for users based on their task requirements. The fitness function for all three cloud layers takes into account various Quality of Service (QoS) objectives.\n\nSimulation analysis shows that the proposed three-tier vehicular cloud architecture outperforms some standard algorithms. The optimized resource allocation using PSO helps to improve the efficiency and reduce the cost of executing these resource-intensive applications. In conclusion, the proposed architecture offers a promising solution for executing cloud-based applications on vehicles and improving the overall vehicular cloud computing experience.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: 360-degree videos offer an all-encompassing perspective of a scene, but the data required to represent them can be cumbersome. As a result, finding ways to represent 360 videos with less data has become increasingly critical, and the Cube format has emerged as a popular option. However, it's necessary to convert Cube format to Equirectangular (ERP) for easy display purposes.\n\nThis paper aims to improve the performance of Cube-to-ERP conversions by utilizing a combination of Convolutional Neural Network (CNN) and classical interpolation methodologies. Based on the geometry features of the Cube-to-ERP format, we derive an optimal threshold for the boundary. This threshold guides the combination of CNN and the classical interpolation method, resulting in significantly improved performance.\n\nThrough experimental results, we demonstrate that the derived threshold possesses a certain degree of guiding significance. We also introduce a new evaluation criterion that employs the Marsaglia model to make evaluating the geometry conversion process easier and more accurate.\n\nIn summary, this study enhances the Cube-to-ERP conversion performance through a new combination of technologies and proposes a more straightforward evaluation criterion to assess the process's accuracy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a regression model to establish a relationship between knee joint angles and surface electromyography (sEMG) signals. Our proposed model consists of a wavelet coefficient correlation dimension (WCCD) feature extraction method and an Elman network for estimating the model. The experiment was conducted on five muscles that are responsible for knee joint motion, while the Codamotion system was used to record knee joint angles simultaneously. Firstly, we utilized the WCCD method for optimal feature extraction from the multichannel sEMG signals. Then, we used the Elman network to map these features to the knee joint angles. The findings indicate that the proposed WCCD method outperforms time-domain and frequency-domain methods. Consequently, our model has promising applications in intelligent prosthetics, exoskeleton robots, and medical rehabilitation robots.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This article presents a prototype framework designed to enable formal and machine-checked validation of GPU pseudo-assembly code algorithms using the Coq proof assistant. Such a framework would offer reliable validation of high-assurance GPU computations without relying on any specific source-to-assembly compilation toolchain. \n\nThe PTX pseudo-assembly language is used as an example in this article, with a formal operational semantics expressed via dependent Coq types. This allows for the easy development of proofs and proof tactics that refer directly to the compiled PTX object code. \n\nHowever, challenges arise when trying to model PTX's complex and highly parallelized computation model in Coq, and the article discusses how to overcome these difficulties while maintaining sufficient clarity and generality to establish useful properties of realistic GPU programs. Several examples demonstrate how the prototype can already be used to validate some realistic programs.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent research activities have centered around topics such as smart cities and autonomous driving, making simulations a critical tool for evaluating new algorithms, technologies, and products. These simulations help identify errors and optimize parameter sets before real-world implementation. However, simulation models for traffic problems must handle large-scale scenarios, connect entities from different domains, and run in a feasible time. To address these challenges, this paper proposes an extendable multi-level traffic simulation approach. We discuss existing simulation techniques, upcoming problems, available solution approaches, and development topics for our framework. As our first step, we coupled two different resolution levels of traffic simulation using High-Level Architecture (HLA) and evaluated the approach based on simulation results and performance.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we conduct a comparative analysis of the wireless standards IEEE 802.15.4 and IEEE 802.15.6 for a WBAN healthcare monitoring system. The focus of our work is to identify key factors that determine the norm providing the best quality of service (QoS) for this type of system under normal traffic conditions. \n\nTo achieve this goal, we conduct a series of simulations using the Castalia Simulator. We evaluate the average latency, throughput, and reliability of both standards under identical conditions. \n\nOur findings provide valuable insights into the performance of these two wireless standards. By considering important factors such as latency, throughput, and reliability, we identify the best standard for a WBAN healthcare monitoring system. \n\nOverall, our work contributes to the ongoing research on WBANs and their potential applications in healthcare. We believe that our analysis will be useful to researchers and practitioners working in this field, and pave the way for future improvements in wireless standards for healthcare monitoring systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The optokinetic reflex (OKR) is a behavioral response that stabilizes moving images on the retina through compensatory eye movement. The cerebellum is involved in OKR adaptation and serves as a model for assessing cerebellar learning functions. Cerebellar learning has two phases: short-term learning acquired after a single session that disappears within 24 hours, and long-term learning induced by repeating sessions and persists for days. Recent studies suggest coordination between the olivary system and cerebellar learning, but the effects of olivary system disruption on these two phases of cerebellar learning remain unclear.\n\nThis paper presents a simple model for cerebellar learning adaptation and memory formation. The model reproduces OKR gain adaptation of eye movement in both short and long term phases, and simulation results are consistent with previously reported experimental data from wild type mice. Additionally, the effects of irreversible olivary system lesion on the gain adaptability of OKR are explored by cutting off the connection of climbing fiber from the inferior olive (IO) neuron. The results show that the gain of OKR undergoes a significant decline in both short and long phases of learning compared to the normal case, suggesting that the olivary system plays a critical role in both phases of OKR adaptation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper tackles the challenge of Community Question Answering (CQA) in Arabic and proposes a novel approach that leverages both lexical and semantic features to improve answer retrieval. The authors conduct a thorough evaluation of their method using the SEMEval2017 CQA dataset for Arabic and achieve a Mean Average Precision (MAP) score of 62.85% using a supervised machine learning approach (linear SVM), outperforming the best reported results on the dataset to date. Their approach incorporates a variety of features including word embeddings, latent semantic similarities, and other lexical similarities.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The driver reveals that driving styles and behavior have a strong impact on vehicle control and energy efficiency. By collecting and analyzing driving patterns, it is possible to classify drivers into different categories, which can lead to improved safety systems, advanced driving assistance systems, and energy-efficient controls. To conduct an efficiency-oriented analysis, an artificial neural network (ANN) is utilized to classify drivers as aggressive, normal, or calm, based on three different driving inputs - vehicle acceleration, speed, and throttle pedal angle. The resultant models provide an accurate classification of driving behavior, with an overall accuracy rate of 90%. This classification acts as a reminder to drivers of their current driving behavior, encouraging them to take necessary steps to improve driving conditions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent studies in radiomics have shown promise in the use of deep learning techniques to learn radiomic features and combine multimodal imaging data. However, many deep learning based radiomic studies have focused on pattern classification, which is not suitable for survival analysis studies where some data samples have incomplete observations. In order to improve existing survival analysis techniques, we propose a deep learning method for building survival regression models that utilizes imaging features optimized by deep convolutional neural networks (CNNs) in a proportional hazards model. To ensure versatility for tumors of varying sizes, a spatial pyramid pooling strategy is incorporated. \n\nOur proposed method has been validated using a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients being treated for locally advanced rectal cancer. Compared to survival prediction models built on hand-crafted radiomic features through the Cox proportional hazards model and random survival forests, our deep learning method delivered highly competitive prediction performance.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Binary Neural Networks (BNNs) have received significant attention as they offer reduced memory usage and power consumption, without compromising recognition accuracy for Image Classification. BNNs accomplish this by replacing the multiply-accumulate operations of the convolution layer with bit-wise operations such as XNOR and pop-count. Hardware accelerators such as in-memory computing (IMC) benefit significantly from such bit-wise operations.\n\nHowever, the pop-count operation requires an additional digital processing unit (DPU) resulting in considerable data movement between the Process Engines (PEs) and data buffers. This data movement reduces the efficiency of IMC. In response, CORN, a BNN computing accelerator, has been proposed. CORN utilizes Spin-Orbit-Torque Magnetic RAM (SOT-MRAM) based data buffers to perform the majority operation in place of the pop-count process. This SOT-MRAM-based IMC accelerates the computing of BNNs.\n\nCORN implements the XNOR operation in the NVM memory array, and performs the majority write operation in the computing data buffer. This design eliminates the pop-counter implemented by the DPU and reduces data movement between the data buffer and memory array. As evaluated, the CORN achieves power savings of 61% and 14% with 1.74\u00d7 and 2.12\u00d7 speedup, compared to the FPGA and DPU based IMC architecture, respectively.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To achieve efficient Delay/Disruption-Tolerant Networking (DTN) routing, it is necessary to ensure that messages are quickly delivered from the source node to the destination node and that disused message replicas are promptly deleted from the network. Epidemic routing is a widely-used approach that forwards message replicas to all encountered nodes, thereby resulting in near-optimal message delivery delay when only a limited number of messages are being transferred. However, the performance of epidemic routing significantly deteriorates when multiple messages are being transferred concurrently.\n\nIn our previous work, we proposed an extension to epidemic routing called restrained epidemic routing that deliberately suppresses message forwarding in the later stages of message dissemination. This simple approach has proven to be effective in achieving faster and more efficient DTN routing.\n\nIn this paper, we have analyzed the characteristics of restrained epidemic routing in complex networks with various contact models. We have described the dynamics of restrained epidemic routing on complex networks with a given degree distribution using differential equations and the degree-based mean field approximation. Our findings suggest that restrained epidemic routing is a promising approach for achieving efficient DTN routing in complex networks, particularly in scenarios involving multiple message transfers.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We present a novel wireless power transmission (WPT) system that utilizes cavity-based technology, making it adaptable to any equipment regardless of its size or shape. This system provides both uniform and selective powering modes, without any restrictions on the placement or orientation of the energy harvester. Our design process comprises the creation of randomness, frequency selection, and waveform generation. We verify the effectiveness of our proposal in a lab lyophiliser's chamber by generating a random electromagnetic environment through mechanical stirring, and then evaluating this randomness in terms of the average to minimum power ratio. To select an appropriate frequency for the WPT system, we consider randomness and power uniformity, selecting the lowest usable frequency of the chamber (6 GHz) to maximize randomness and plotting the standard deviation of received powers from different locations to achieve power uniformity. At 6 GHz, a 2.5 dB standard deviation is calculated. Furthermore, we propose an electromagnetic time reversal (EMTR) technique to enable selective powering mode, which theoretically focuses 97% of the energy on a 0.5\u03bb-diameter area in an ideally random environment.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mild cognitive impairment (MCI) is an early stage of Alzheimer's disease, which is a progressive brain disorder that affects memory, thinking, and behavior. In this study, we proposed a novel method for MCI prediction based on graph convolutional networks (GCN). Our method combines neuroimaging data with other demographic information, including collection devices and gender, to build a graph that represents the individual features and data associations among subjects.\n\nWe modified convolutional neural networks (CNN) to construct GCN, which allows for more accurate analysis of functional connectivity networks (FCN). Unlike previous methods, our proposed model takes into account the individual differences among subjects and the demographic relationship, providing a more comprehensive approach to MCI prediction.\n\nOur experimental results demonstrate that our method achieved remarkable prediction performance. The novelty of our approach lies in its ability to effectively integrate diverse information sources into a single framework for prediction tasks. Ultimately, this approach could help improve early detection and intervention for individuals at heightened risk of developing Alzheimer's disease.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The world of high performance computing is rapidly evolving, and demand for computing resources is increasing steadily. As more and more organizations invest in various kinds of cyberinfrastructure resources, it has become crucial to analyze expenses and investments thoroughly to optimize the return on investment. This paper presents an analysis of the ROI for three different kinds of cyberinfrastructure resources: the eXtreme Science and Engineering Discovery Environment (XSEDE), the NSF-funded Jetstream cloud system, and the Indiana University (IU) Big Red II supercomputer.\n\nOur analysis involved assigning financial values to services either by comparing them with commercially available services or by surveying the value of the resources to users. Despite the different natures of these resources, we found that the ROI for all three was greater than 1. This means that investors are getting more than $1 returned for every $1 invested. \n\nWhile there are multiple ways to calculate the value and impact of investment in cyberinfrastructure resources, measuring the short-term ROI provides a clear picture of the net positive impact these resources have on universities, research institutions, and the federal government. As such, it is becoming increasingly critical to analyze investments in these resources to ensure organizations make the most of their investments.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We present a novel algorithm that offers an efficient approach to designing security strategies in mobile edge computing (MEC) environments. Our algorithm focuses on safeguarding nodes connected to the MEC, with the help of mobile users that act as agents to prevent suspicious activities. By leveraging a specific set of synthetic principles, our approach can quickly construct high-quality suboptimal solutions even when the number of targets becomes immensely large, reaching hundreds of millions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Periodic fractographic analysis is essential for improving the performance of mechanical pieces and avoiding economic and security problems in various industries. Experts in fracture classification of metallic materials use texture and surface marks to determine the type of fracture. However, when it comes to texture classification, the results of deep learning are not as good as in other classification tasks.\n\nTo improve deep learning's performance for texture analysis in fractographic classification, this paper proposes extracting handcrafted features (Haralick, fractal dimension, and local binary patterns) from the output of the convolutional layers of the VGG-19 model. Four datasets, including two common textural databases (KTH-TIPS and KTH-TIPS2-B) and two fracture datasets (one of real-scale images and one acquired with SEM), were used to evaluate the proposed methods.\n\nThe best performance for the KTH-TIPS and KTH-TIPS2-B datasets was achieved with local binary patterns (LBP) extracted from the first feature map of the third convolutional layer and the second feature map of the fifth convolutional layer group, respectively. For the fracture database of real-scale images, the best performance was achieved with fractal dimension features extracted from the feature maps of the first convolutional layer, and with LBP extracted from the first feature maps of the fourth convolutional layer for the SEM images.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The scalability of Internet of Things (IoT) systems is a significant concern due to the large amounts of data generated. Centralized data exchange leads to a single performance bottleneck, while a distributed data exchange faces network performance issues. A decentralized approach is the only solution that fully enables efficient IoT systems as there is no single performance bottleneck and network overhead is minimized. This paper presents an approach that leverages the semantics of DX-MAN for realizing decentralized data flows in IoT systems. The algebraic semantics of such a model allows for a well-defined data flow structure that is easily analyzed by an algorithm. The algorithm forms a direct relationship between data consumers and producers, preventing data from being passed alongside control among multiple coordinators. Data is only read and written on a decentralized data space. The approach is validated using smart contracts on the Blockchain, and experiments show that it scales well with the size of IoT systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional neural networks have revolutionized the field of computer vision by automating the process of feature extraction. These networks use initialization methods and training strategies to extract feature maps, which saves a great deal of time and effort required for feature engineering. However, these common networks lack the ability to consider the correlation between feature maps, which leads to an increase in redundant feature maps as the network becomes more complex.\n\nTo address this issue, we proposed a correlation layer and a correlation loss function which can compute the correlation coefficient matrix of the feature maps in the last convolutional layer and optimize the weights distribution respectively. To evaluate our approach, we studied two strategies - supervision and initialization - using Gaussian and He initialization methods as baselines.\n\nOur experimental results on the CIFAR-10 dataset demonstrated that the supervision strategy for multi-task training could considerably reduce the correlation between the feature maps learned and increase the classification accuracy from 0.39% to 1.14% on the test set. Our proposed correlation layer and loss function can thus effectively improve the performance of convolutional neural networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: For years, security experts have recognized the importance of sharing information and collaborating to combat threats. Yet, this has been easier said than done. Despite numerous attempts through legislation, academic work, and industry initiatives, effective sharing and collaboration remain complicated, slow, and untrustworthy due to bureaucratic issues.\n\nIn response, a new model called CARE has been proposed. This architecture addresses privacy, secrecy, lineage, and structural challenges that have plagued current sharing communities and platforms. With CARE, a marketplace based on smart contracts with transactional privacy over a distributed blockchain incentivizes sharing, combats free riding, and provides an immutable ledger for event attribution.\n\nThis paradigm shift has the potential to not only overcome the challenges of sharing but also open up new opportunities for business models, insurance risk assessments, and government-backed incentivization. By leveraging blockchain technology, the CARE model offers a promising solution to fostering effective sharing and collaboration among security practitioners.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, numerous researchers have been exploring the potential of using Unmanned Aerial Vehicles (UAVs) for a variety of civil and military missions, with minimal human intervention. One promising application involves the use of multiple UAVs to cooperatively monitor crowded areas. However, due to the lack of proper communication standards and rules, designing a reliable communication model is crucial for efficient multi-UAV coordination, bandwidth sharing based on data priority and urgency, and avoiding redundant transmission.\n\nTo address these challenges, we propose a centralized data-oriented communication architecture for crowds surveillance using UAVs. The Ground Control Station (GCS) serves as a central coordinator that manages bandwidth usage for the UAV fleet in its coverage area. We have defined two classes of urgent messages, critical state, and important result, that allow UAVs to send priority messages to the GCS. The GCS uses the class of the data and other relevant information about the detected event to authorize or deny UAV data transmission to optimize bandwidth usage efficiency.\n\nOverall, our proposed communication architecture provides a reliable and efficient framework for the cooperative monitoring of crowded areas using multiple UAVs. With proper implementation, this approach can significantly enhance the capability and effectiveness of UAV systems for various applications, including surveillance, disaster response, and search and rescue missions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper discusses a novel approach to diagnose faults in a parallel shaft gearbox. Given the gearbox's non-stationary and nonlinear vibration signals, the Multifractal Detrended Fluctuation Analysis (MFDFA) technique is implemented to compute multifractal spectrum parameters as the fault features. Furthermore, an enhanced K means clustering approach is applied to detect gearbox failures. The methodology is validated by employing gearbox preposition failure data, and the results demonstrate that the approach is highly effective.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we address the issue of integrated berth and quay crane allocation (I-BCAP) in general seaport container terminals. We present a dynamical modeling framework based on discrete-event systems (DESs) to describe the berthing process with multiple discrete berthing positions and multiple quay cranes. Using this model, we propose the model predictive allocation (MPA) algorithm based on model predictive control (MPC) with a rolling event horizon. To validate and evaluate our proposed approach, we conduct extensive Monte Carlo simulations with realistic datasets and real data from a container terminal in Jakarta, Indonesia, as well as real life field experiments at the same terminal. The simulation results show that our proposed MPA algorithm can improve the efficiency of the process by reducing the total handling and waiting cost by approximately 6%-9% as compared to the commonly used first-come first-served (FCFS) method combined with the density-based quay cranes allocation (DBQA) strategy. Additionally, our proposed method outperforms the state-of-the-art hybrid particle swarm optimization (HPSO)-based and genetic algorithm (GA)-based methods found in the recent literature. The real life field experiment demonstrates an improvement of around 6% as compared to the current allocation method in use at the terminal.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a unique approach to address the selection of dominant patterns of histograms of oriented gradients (DPHOGs) in the area of vehicle detection. The currently prevalent HOG features suffer from high misclassification rates and contain ambiguous and redundant features. Various adaptations of HOG have been proposed earlier to tackle these problems, but they continue to carry some ambiguous features. The proposed method solves this issue through feature selection, with a focus on excluding ambiguous features (leading to reduced classification times and improved classification rates). The proposed method utilizes the ideal vectors of vehicle and non-vehicle images to select features in dominant patterns. The DPHOG was evaluated using a kernel extreme learning machine, support vector machine, K-nearest neighbor, random forest, and deep neural network as classifiers. We also compared DPHOG with eight well-known feature selection methods and three existing feature extraction methods for vehicle detection. In terms of accuracy, true positive, false positive, and F1-score, the DPHOG outperformed all other methods and exhibited the least running time in each dataset.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a new behavioural mixer model is introduced that takes into account amplitude nonlinearity, phase nonlinearity, and memory effect of the mixer in a multi-box structure. The nonlinearity sources are modelled in different blocks and then connected in a cascade to form the mixer model. A test-bed was created that included a reference mixer, and through measurements, amplitude and phase characteristics of the main tones and IMD products were modelled. The proposed model delivers a good match between the measured and simulation results. By optimizing its variable parameters, the mixer model can be curve fitted to the characteristics of any mixer and used to model a range of mixer types. This makes the proposed mixer model suitable for use in computer simulation tools and linearization applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Video-based human action recognition is a key area of research in computer vision, and it is widely acknowledged that the success of such recognition projects is dependent on the quality of the feature extraction process and the representation of actions. Since the arrival of the Kinect camera, a plethora of Kinect-based human action recognition techniques have emerged. However, a comprehensive comparison of these techniques, particularly under the grouping of feature types, is still missing. In this paper, we compare ten recent Kinect-based algorithms as well as implementing and improving some of these techniques, assessing their ability for both cross-subject and cross-view action recognition using six benchmark datasets. Our experiments revealed that most methods outperformed cross-subject action recognition compared to cross-view action recognition, that skeleton-based features were more robust for cross-view recognition, and that deep learning features were most suited for larger datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper examines the practicality of utilizing deep and machine learning approaches for beam selection in the uplink of a mmWave communication system. The proposed system consists of an analog beamforming network combined with a zero-forcing baseband processing block, and the objective is to select the optimal configuration for the ABF network based on the estimated angles-of-arrival and received powers. To accomplish this goal, the study evaluates three machine/deep learning methodologies: k-nearest neighbors, support vector classifiers, and multilayer perceptron. \n\nThe research assesses the impact of various factors such as the Capon or MUSIC methods employed to estimate the AoAs and powers, the size of the training dataset, the number of beamformers in the codebook, the beamwidth, and the number of active users. The simulation results demonstrate that the classification accuracy and sum-rate are comparable to those achieved through exhaustive search, indicating that the proposed techniques are highly effective.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper introduces novel reflectarray resonant elements, which are designed using genetic algorithm (GA), and have a phase difference of -180\u00b0 to 180\u00b0 between TE (V-pol.) and TM (H-pol.) polarizations from 13 GHz to 17 GHz. These elements are also equipped with two axially symmetric structures in the unit cell, which helps suppress cross polarization level to less than about -60 dB over a wide frequency range. A reflectarray antenna with two-separated offset feeds has been designed by utilizing these elements in the Ku-band, and its effectiveness has been further verified by evaluating radiation patterns numerically.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The scattering properties of multiple-layer anisotropic metasurfaces can be effectively characterized using generalized boundary conditions. By determining the surface susceptibilities of all layers, it is possible to derive reflection and transmission coefficients for a wide range of incident angles and polarizations. This analytical method has been thoroughly tested and validated through comparison with full-wave simulations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: WiFi technology is being increasingly utilized by carriers for offloading their cellular network infrastructure or generating revenue through WiFi-only plans and on-demand passes. Although this technology holds immense importance, the current deployment of WiFi access points (APs) by carriers primarily follows heuristic approaches, which may result in significant opportunity costs for carriers due to the prevalent free-of-charge WiFi access policy. Our paper addresses this problem of optimizing WiFi AP deployment and data pricing to maximize carrier profit. We consider various demand models that predict traffic changes in response to alterations in price and AP locations. Our framework provides optimal and approximate solutions and highlights how critical parameters shape the carrier profit. Our evaluations on a WiFi dataset indicate that carriers can reduce their costs while charging users nearly 50% less than cellular service by using WiFi. Efficient integration of WiFi technology with next-generation carrier networks requires solving this problem.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Video games, like movies and music, are a popular form of entertainment. In addition to providing an interactive experience that can help alleviate daily stress, video games allow players to live out their heroic fantasies and find a sense of satisfaction. While 3D games are the most popular, 2D games have enduring appeal thanks to their straightforward and simple gameplay.\n\nThis paper presents a 2D platformer action game where players must jump on platforms, shoot enemies, or run from them to complete levels. The game features three unique enemies, each with different skills like shooting projectiles, flying, and pursuing the player. These enemies use reinforcement learning methods to improve and enhance their decision-making abilities based on their previous decisions and outcomes, allowing them to make more informed choices when facing the player.\n\nOne of the game's most unique features is that enemies can change their behaviors and actions according to their current state, with the thresholds being modified at the end of each level based on the player's actions in the previous level. Since every player has a distinct style of play, the enemies will take different actions based on the individual player's approach.\n\nThe game has been developed for Android devices using the Unity game engine and C# programming language. With its simple gameplay, unique enemies that adapt to the player's style, and use of reinforcement learning, this 2D platformer action game offers a fresh and engaging experience for players.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The selection of appropriate resources and efficient allocation of tasks in a parallel application is crucial for an optimal outcome. This involves mapping tasks to the appropriate resources in a manner that balances the execution time and resource usage. In this study, we explore the mapping and scheduling of parallel applications with inter-task communications in a cloud environment. Our proposed method employs fuzzy logic and minimizes communication overhead to reduce execution time and resource usage time. The results of our study show a significant improvement over the Max-Min-C and Min-Min-C algorithms.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wind power is a crucial source of renewable energy, and accurately predicting wind speed is essential for efficient management and decision-making in wind power generation. To address this challenge, a deep belief network with genetic algorithms (DBNGA) model was proposed for wind speed forecasting. The model integrates genetic algorithms to determine parameters for deep belief networks, and utilizes wind speed and weather-related data from Taiwan's central weather bureau. The study employs both time series and multivariate regression data to forecast wind speed, and various methods including SARIMA, LSSVRTSGA, LSSVRGA, and DBNGA models were used to predict wind speed. The results indicate that DBNGA models offer higher accuracy in forecasting wind speed compared to other models. Therefore, the study concludes that the DBNGA model is a feasible and effective approach for wind speed forecasting, which is essential for responsible and sustainable power consumption and production.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a method for estimating speed using the acceleration data measured by a wearable device during treadmill exercising is described. The authors utilize a regression algorithm that implements a deep convolutional neural network (CNN) model to estimate the moving speed. During the training of the model, a set of test speeds for both walking and running conditions is used to minimize the mean square error between inferred and test speeds and optimize the internal model parameters. The results indicate that the model is able to infer speeds accurately within 7% and 18% of actual running and walking test speeds, respectively. The deep CNN model parameters, including data sample size, convolution kernel size, and fully connected layer sizes, are optimized to ensure inference accuracy and enable a compact hardware design. The authors demonstrate the feasibility of designing a wearable device for speed inference from acceleration measurements through device simulation.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An imbalanced dataset refers to a dataset that contains a majority class having a significantly higher number of examples than the other classes. Such datasets pose a challenge in classification problems, since many classification algorithms fail to deliver satisfactory results.\n\nIn this paper, we will discuss our approach of using logistic regression analysis with Python to tackle imbalanced datasets. Specifically, we aim to determine various thresholds for classification based on the proportion of data in the imbalanced dataset.\n\nBy addressing the issue of imbalanced datasets through our logistic regression analysis, we hope to provide a reliable and effective solution to the problem of classifying unbalanced datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cataract, which is characterized by a clouding of the lens in the eye, is a common cause of visual impairment and blindness. Early detection and treatment of cataract can improve patients' quality of life and prevent blindness. However, the traditional method of cataract diagnosis and classification demands extensive manual processing, which involves skilled personnel, time, and precision, and often lacks accuracy and reliability. In recent years, pattern recognition-based approaches to cataract diagnosis have gained popularity. \n\nThis paper proposes a deep-learning algorithm that utilizes a 18(50)-layer convolutional neural network to identify cataract attributes. The network takes as input the retinal image in the G channel and outputs a heatmap that localizes the areas indicative of different levels of cataract. The model extends the training strategy and aims to improve the network's performance in detecting and grading cataract. Our proposed method achieves state of the art accuracy in comparison with other methods of cataract classification. \n\nMore importantly, our model provides a compelling reason for its classification by localizing the areas indicating cataract in the retinal image. This intuitive approach to cataract diagnosis and classification can significantly reduce the human effort and increase the accuracy and dependability of the diagnosis. Our method can help healthcare professionals identify cataract early, improve treatment outcomes, and reduce the risk of blindness for patients.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In today's world, the establishment of technology transfer centers has become a common practice funded by either European or national sources. These centers are administered by experts who facilitate the transfer of technology between innovators and potential users. Small and medium-sized enterprises are considered the backbone of an innovative and robust European economy and thus, technology and knowledge transfer to these firms are critical to ensuring their growth. Therefore, it is imperative to create preconditions for a sustainable transfer of technology to such companies. One of the most significant aspects of successful technology transfer is the education of experts involved in the process. This article discusses the findings of the Erasmus KA2 Project T4, which defined the competencies and curriculum for Transnational Technology Transfer Managers (TTTM) in small and medium-sized enterprises (SMEs). The project formulated training blueprints that could accelerate the growth of these companies.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper aims to compare the performance of four different optimization algorithms, namely the Genetic Algorithm, Particle Swarm Optimization algorithm, Modified Particle Swarm Optimization algorithm, and L\u00e9vy Flight Firefly Algorithm, for system identification of various types of nonlinear systems. The study evaluates their performance with Volterra nonlinear adaptive structures, matched-order fixed-nonlinearity LNL filter structures, reduced-order adaptable-nonlinearity LNL filter structures, and neural networks. Results indicate that the LFFA outperforms other algorithms in terms of faster convergence rates and lower minimum mean square errors. Overall, this paper provides a comprehensive comparison of these algorithms in system identification of nonlinear systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This article proposes a novel method for the recognition of different digital modulations by examining various cumulant combinations of modulations such as 2FSK, 4FSK, 2PSK, 4PSK, 2ASK, and 4ASK. The paper established new identification parameters by utilizing deep neural networks to improve the recognition rate. This DNN was designed to classify signals based on the unique features of each signal type extracted from high order cumulants. The study showed the exceptional classification performance of the proposed algorithm with an overall success rate of over 99% at SNR of \u22125dB and 100% at SNR of \u22122dB. Extensive simulations also demonstrated the robustness of the proposed method under various conditions including frequency offset, multipath effects, etc.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The technological advancements made in the energy sector have led to the generation of complex data. The present study focuses on analyzing the correlation between smart grid technology and the use of big data approaches. The key areas of the smart grid system that utilize big data technologies have been identified and the detection of smart grid attacks through big data analytics has been given significant attention. In order to achieve efficient results, careful consideration of the choice of algorithms and metrics is important. Therefore, a prototype application utilizing big data techniques has been developed for the detection of attacks on the smart grid. The study has determined that the accuracy of the proposed algorithm is 92% for random forests and 87% for decision trees.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a novel hybrid probabilistic interval prediction method for short-term load forecasting. This method integrates K-means clustering based feature selection and online Gaussian processes regression(OGPR) to improve the accuracy of load forecasting results. The K-means clustering algorithm contributes to relevant feature selection during a dynamical process to better capture the load characters along with time. OGPR, which includes both dynamically updating the hyper-parameters and the training sample sets, is used as a forecasting engine to perform load probability interval prediction. To validate our proposed model, we use load data from the Queensland market, Australia. The comparative results show that our approach outperforms others in terms of gaining higher quality prediction interval.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the widespread use of the internet, the number of malware attacks on computer systems is on the rise. Although there are various techniques employed to identify malware, they often fail to detect unknown malware. This necessitates the use of dynamic malware investigation systems that utilize machine learning techniques for windows-based malware recognition. This paper proposes two methods to analyze malware behavior and select features of windows executable files. One such method is the use of Cuckoo, a malicious code analysis tool that provides comprehensive results based on the tests carried out on the malware. The second method involves feature selection for windows dynamic malware analysis using Genetic Algorithm. Three classifiers were compared to determine the accuracy of Windows-based malware detection: Support Vector Machine, with a detection accuracy of 81.3%, Naive Bayes classifier with an accuracy of 64.7%, and Random Forest classifier, which achieved 86.8% accurate results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a solution for managing the energy consumption of Electric Vehicles (EVs) in Smart Cities. Our approach leverages deep learning techniques to predict the trajectory and delay of EVs, thereby enhancing their energy consumption. Specifically, we train two Recurrent Neural Networks (RNNs) on 60-day urban traffic data. Our trained networks accurately predict trajectory and delay, even for long prediction intervals. \n\nIn addition, we design an algorithm that utilizes well-known energy models for traction and air conditioning. This algorithm helps prevent battery exhaustion and optimizes energy management for EVs. Our experimental results show that our solution significantly improves route trajectory and delay predictions, thereby enhancing energy management. By combining the RNN and energy model approaches, we demonstrate the efficiency of our proposed solution for EV energy management in Smart Cities.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Multivariate time series forecasting is crucial for many applications, such as finance, signal processing, air quality forecasting, and pattern recognition. However, determining the most relevant variables and proper lag length from multivariate time series data presents a significant challenge. In this paper, we propose an end-to-end recurrent neural network framework equipped with an adaptive input selection mechanism to improve the prediction performance of multivariate time series forecasting.\n\nOur proposed model, called AIS-RNN, consists of two main components. The first neural network component generates context-dependent importance weights to dynamically select the input, while the second component predicts the target variable using the selected input. We conducted experiments using several public benchmark datasets to evaluate our end-to-end approach's performance against machine learning-based baselines. The results show that our proposed approach outperforms the baseline models.\n\nMoreover, we compared the AIS-RNN's performance with the M3-specialized models on the public M3 dataset. Our proposed model achieved a higher performance than the M3-specialized models. In addition, the adaptive input selection mechanism of AIS-RNN provides an advantageous interpretation of variable importance.\n\nOverall, our proposed AIS-RNN model demonstrates improved accuracy and interpretability for multivariate time series forecasting.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Diseases such as stroke and proprioception loss can lead to difficulty in controlling the arm during daily activities. Studies have shown that intensive motor training using virtual reality games can help patients restore their arm functions. In this paper, a new method called pseudo-haptic feedback is proposed to add motion assistance or resistance to the virtual reality-mediated upper-limb rehabilitation process. This method uses alterations to the cursor speed to create the illusion of virtual forces aiding or opposing the arm movement during rehabilitation training. \n\nTo demonstrate the feasibility of this approach, the proposed method was experimentally evaluated with human subjects. The results showed that the motion assistance mode was more time-efficient and easier than the motion resistance mode according to the subjects. Moreover, the difficulty of the path following task in virtual reality-mediated upper-limb rehabilitation training can be adjusted using the proposed pseudo-haptic feedback method without requiring any haptic feedback devices. \n\nOverall, the proposed method is a promising approach to enhance the effectiveness of virtual reality-based rehabilitation and help patients recover arm function lost due to various diseases.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a tracking control scheme for hypersonic flight vehicles (HFVs) that utilizes an interval type-2 fuzzy neural network (IT2FNN) with a quantization mechanism. The scheme incorporates a parameterized tracking error model of the HFV that takes into account uncertainties, which are approximated by the IT2FNN. The tracking control is designed using a prescribed performance control technique that allows for enhanced and adjustable transient characteristics of the tracking errors through the use of prescribed performance functions. The control scheme involves the design of continuous control laws for the fuel equivalency ratio, canard deflection, and elevator deflection through an adaptive backstepping control design procedure with logarithmic quantization mechanism. This strategy helps avoid inadvertent increases in the effective gains of continuous controllers as well as reducing the communication loads between the controller unit and actuator unit. The controllers designed in this paper can limit the tracking errors of the flight path angle and angle-of-attack. The performance of the presented tracking controllers with quantization mechanism has been validated through comparative simulations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distributed robots offer a promising alternative to traditional large rovers for surface exploration. By exploring a larger area simultaneously, exploration speed can be improved. However, distributed rovers are limited by their mass resources, which necessitates the use of a lightweight position and attitude estimation system for guidance and control. To address this challenge, we propose a compact and lightweight attitude and position estimation system for distributed rovers that uses light from the leader agent.\n\nOur system utilizes small photodiodes as sensors for position estimation, with the photodiodes on the follower rover detecting light from the leader in different directions to estimate the vector in the light source direction with respect to the rover body coordinate system. Attitude is determined using the estimated light source direction vector and the gravitational vector measured by the acceleration sensor in the rover. The distance between the leader and follower rovers is estimated using the elevation angle of the light source and light source height information. While mutual position recognition is possible with communication between the leader and follower, this paper examines limits achievable with minimal hardware. \n\nWe also discuss light source recognition from the leader rover under ambient light and provide an overview of the exploration system as a whole. With our compact and lightweight system for distributed rover guidance and control, we hope to maximize exploration efficiency while minimizing resource consumption.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a novel approach, called Multi-objective Chaotic Mutation Immune Evolutionary Programming (MOCMIEP), to address the challenge of determining the optimal size and location of Distributed Generation Photovoltaic (DGPV) in the power system transmission network. The technique is based on Pareto optimality and aims to minimize two objective functions: active power losses and Fast Voltage Stability Index (FVSI), simultaneously. The MOCMIEP method has been tested on the IEEE 118-bus Reliability Test System (RTS), and the results demonstrate its effectiveness in identifying a set of Pareto solutions that the decision maker can select from based on system priorities. Overall, this technique offers a promising solution for tackling the DGPV location and sizing problem in power system transmission networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The effective storage, processing, and analysis of power device condition monitoring data is a significant challenge. To overcome this challenge, a framework has been proposed that can support both MapReduce and Graph for massive monitoring data analysis. This framework is based on the Aliyun DTplus platform. \n\nThe proposed framework employs a power device condition monitoring data storage system based on the MaxCompute table. Additionally, a parallel permutation entropy feature extraction method is designed and implemented using the MaxCompute MapReduce. This allows for efficient data processing and storage on the DTplus platform. \n\nThe framework also employs a Graph-based k-means algorithm for the clustering analysis of massive condition monitoring data. The algorithm is designed to be efficient and accurate when dealing with large datasets. \n\nTo compare and evaluate the performance of the framework and the parallel algorithms, performance tests were conducted. The results were analyzed based on CPU cores consumption, memory utilization, and parallel granularity. The experimental results demonstrate that the designed framework and parallel algorithms can efficiently process massive power device condition monitoring data. \n\nOverall, the proposed framework provides an efficient and effective solution for the storage, processing, and analysis of large-scale power device condition monitoring data. The combination of MapReduce and Graph-based algorithms provides a powerful analytical tool that can handle the complexities of massive datasets.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A robust scheme is presented for the attitude stabilization and trajectory tracking of the 3 DOF Hover in the presence of parameter uncertainties and perturbations. The control approach utilizes an adaptive Super Twisting algorithm (ASTW) with a continuous differentiator. The continuous differentiator effectively estimates unmeasurable states and solves the problem of chattering in the output signals. \n\nExperimental tests were conducted on the Quanser Hover platform to validate the proposed control scheme under external disturbances and parameter uncertainties. Results demonstrate the smoothness of the continuous differentiator's signals, even in the presence of noised measurements induced by sensors. In addition, the experimental tests confirmed the robustness, accuracy, and finite-time convergence of the proposed control approach, even in the face of uncertainties and disturbances. \n\nOverall, the presented control scheme offers an effective and reliable solution for the attitude stabilization and trajectory tracking of 3 DOF Hover systems in the presence of parameter uncertainties and perturbations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Remote sensing images captured by high-altitude satellites have a wide coverage area but also tend to have large differences within the same class and small differences between classes. Classifying these images using conventional methods such as Hierarchical ELM (H-ELM) and multilayer kernel ELM (ML-KELM) can be challenging due to their deep network structures and numerous parameters, resulting in lengthy training times and high memory consumption.\n\nTo address this issue, the proposed densely connected kernel ELM (Dense-KELM) learning model is presented based on the ML-KELM to classify remote sensing image scenes. Results of experiments show that compared to H-ELM and ML-KELM, the Dense-KELM model has higher classification accuracy at the same model depth. The training time for Dense-KELM is only slightly more than ML-KELM, but significantly less than H-ELM. This model can effectively extract high-level features of the images, represent details between scenes and enhance the accuracy of classifying remote sensing images. The densely connected network structure can also reduce the number of parameters for the model, enhance training speed and save space for storage.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The preservation of shape features is crucial when filling holes in surfaces. In this study, we propose a method for filling holes utilizing feature lines, which include the boundary lines, ridge lines, and valley lines of the hole. Initially, we extract these feature lines and generate a series of curves from them. Subsequently, we obtain a sequence of control points through curve-fitting. Finally, we apply this sequence to a Non-Uniform Rational B-Spline (NURBS) surface to fill the hole. Our experimental results demonstrate that this technique effectively preserves the original shape characteristics of the hole.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Over the past few years, there have been significant developments in eye tracking-based applications. As a result, low-cost commercial remote vision-based eye trackers have emerged in the market, and researchers have begun to explore the possibility of using eye-tracking capabilities in multi-screen setups. However, one of the main challenges faced when using these eye trackers in multi-screen setups is the lack of an intuitive and reliable way to track eye movements across multiple screens without losing important eye tracking data. \n\nTo address this challenge, this work proposes a novel data-driven approach that utilizes deep recurrent neural networks to enable reliable and responsive switching between low-cost multi-screen eye trackers. Our approach has achieved impressive results, with a high level of accuracy and a lower positive rate in detecting the screen that the subject is attending to. Specifically, our F1 measure score has reached a competent 85%.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the advancement of deep learning, the field of image super-resolution has experienced remarkable progress. However, the performance of depth map super-resolution still lags behind that of color images. To address this challenge, a novel method called multilevel recursive guidance and progressive supervised network (MRG-PS) is introduced in this paper. \n\nThe proposed approach comprises a multilevel recursive guidance structure that extracts features from both color and depth streams. The depth stream is guided by color features at each level, enhancing the accuracy of depth map super-resolution. Furthermore, a progressive supervision module is developed to supervise the multilevel recursion process and obtain depth residual information at different levels. Finally, the residual fusion and construction strategy are utilized to fuse all residual information and reconstruct the high-resolution depth map.\n\nExperimental results suggest that the proposed method performs better than the state-of-the-art methods, offering an effective solution to the problem of depth map super-resolution.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The PowerFlex 750-Series products equipped with TotalFORCE control are industry-leading Architecture Class AC drives, bus supplies, and common bus inverters specially designed for the Low Voltage market, with a power range of 160kW to 6000kW. The development of these products involves rigorous product software and hardware verification and validation, which necessitated the adoption of an Opal-RT Hardware-in-the-Loop (HIL) system. Not only was this system used for initial product design verification, but it will also be utilized for regression testing throughout the product\u2019s life. \n\nTo ensure the usefulness and reliability of the HIL system, the accuracy of the system\u2019s simulation must be consistent with that of the product being tested. Therefore, it was important to demonstrate the HIL system\u2019s fidelity and accuracy through simulation and experimentation. \n\nThis paper presents the successful results of the conducted simulation and experimental tests that validate the effectiveness and flexibility of the HIL system in testing and verifying the PowerFlex 755TM Common Bus Inverter. It is evident through these tests that the HIL system provides an accurate and flexible test and verification platform, which ensures confidence in the reliability and functionality of the product.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The study of plant bioelectric potential has been conducted using various methods, including decision tree, multilayer perceptron, ANN, CNN, and more. However, achieving the best accuracy has proven to be a serious challenge as previous studies have not yielded satisfactory results. Given that data is in a sequence format, it is interesting to explore the use of deep learning with LSTM and time series methods with ARIMA. Both of these methods share similar characteristics and offer a novel contribution to this field. As such, this research aims to compare the effectiveness of LSTM and ARIMA for analyzing bioelectric potential data sets. The accuracy was determined through root mean square error (RMSE) and mean absolute error (MAE). Ultimately, the ARIMA model proved to be better than the LSTM method, presenting promising results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Trust modeling has become a prominent area of research, particularly with regards to decision making in information exchange, social exchange, and cognitive motivation. The behavior of trust is temporal, and it fluctuates based on cognitive parameters. To better understand the complex relationships between individuals, we have proposed a psychological approach based on a computation cognitive trust model between pet and owner, using difference equations. Our results demonstrate that our proposed model aligns with our assumptions and we have also obtained results using simulation language, providing a deeper understanding into this area of study.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet of Things (IoT) has revolutionized the way we collect, share and utilize data in the virtual world. However, the centralization of security assurance within IoT applications creates limitations in meeting the security needs of a rapidly growing number of devices worldwide. To address this challenge, blockchain technology (BCT) has emerged as a promising solution that provides security and protects privacy on a large scale. Smart contracts, in particular, offer opportunities to improve the reliability of IoT applications by establishing trust for both data and executed processes.\n\nWhile many literature surveys and positioning articles have explored the integration of BCT with IoT, these discussions have been superficial and have failed to explore the challenges in developing BCT for IoT at the technical level. This paper examines the state of the art of BCT-based applications using a system design approach to clarify critical research areas for enabling BCT for security assurance in IoT applications.\n\nThe paper models and discusses the relations between BCT and IoT, defines the needs of eliminating threats in IoT-based applications as functional requirements (FRs), and establishes mappings between FRs and physical solutions (PSs) to identify limitations and critical areas for the applications of BCT in a large-scale distributed environment.\n\nIn summary, the paper highlights the potential of BCT for improving security in IoT applications and provides insights into the technical challenges that must be addressed to enable the seamless integration of BCT with IoT networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurately identifying DNA-binding proteins (DBPs) from protein sequences is a challenging task for protein function annotation. To address this issue, we have introduced a novel computational method, called TargetDBP. Our approach involves extracting four single-view features, including AAC, PsePSSM, PsePRSA, and PsePPDBS, to represent various base features. We used a differential evolution algorithm to determine the weights of these four features, which were then combined to form the original super feature. We selected an excellent subset of the super feature using SVM-REF+CBR, a feature selection algorithm. Finally, we used support vector machine to learn the model on the selected feature subset. To evaluate our proposed method, we established a new, non-redundant benchmark dataset from PDB database. Our results showed that TargetDBP achieved higher performance than other state-of-the-art predictors. The TargetDBP web server and datasets are freely available at http://csbio.njust.edu.cn/bioinf/targetdbp/ for academic use.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Artificial Intelligence plays an important role in problem solving in our daily lives. Recently, many approaches have been presented for developing TCBR (textual case-based reasoning) applications. One of the fundamental applications is a QA (question answering) system, which requires expertise knowledge accumulation in a library of past case explanations and the assistance of multi-agent. However, this challenge requires an intelligent approach to overcome. Despite many QA systems being developed for English and other well-known languages, no significant work has been found for Urdu language. This is because Urdu is an under-resourced language in terms of available computational resources, despite being spoken by millions of people. Urdu inherits vocabulary from Arabic, Persian and the native languages of South Asia, making it morphologically complex. \n\nThe proposed system meets the challenges in this domain by incorporating a Question Answer system and utility assembly of Urdu language processing. This solution demonstrates the functioning of the knowledge system and yields good results.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Object detection plays a crucial role in the development of autonomous cars. However, traditional computer vision and machine learning methods for object detection are prone to delays in response time. Fortunately, recent advances in artificial neural networks, specifically algorithms like YOLO (You Only Look Once), have addressed this issue without sacrificing precision. \n\nIn this paper, we showcase the use of the newest YOLOv3 algorithm for detecting traffic participants. After training the network for 5 object classes (car, truck, pedestrian, traffic signs, and traffic lights), we demonstrate the effectiveness of this approach in a variety of driving conditions, including bright and overcast skies, snow, fog, and nighttime scenarios.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In a world where electric mobility is rapidly becoming the norm, the importance of electric storage cannot be overstated, especially in applications like electric vehicles. While battery technologies come in a variety of forms, Lithium-ion (Li-ion) technology has emerged as the market leader due to its exceptional performance characteristics. However, ensuring the safety and optimum performance of Li-ion batteries requires the use of a battery management system (BMS), which plays a critical role in accurately estimating the state of charge (SOC). In this paper, we propose an Extended Kalman Filter (EKF) for SOC estimation, which is crucial for the optimal operation of the BMS.\n\nTo achieve the best performance of the EKF algorithm, an accurate battery model is required. In this work, we present the first-order Thevenin model, which is used to model battery behavior. The model's internal parameters were identified using the least square algorithm. Simulation results of the model and EKF algorithm for SOC estimation are presented for a lithium battery with a 3.7V/2.6Ah capacity. The simulation results are followed by their implementation on an electronic card that features the PIC18F4550 microcontroller.\n\nThe proposed EKF algorithm is shown to accurately estimate the SOC of a Li-ion battery, validating the suitability of the first-order Thevenin model for battery modeling. Additionally, the implementation of the EKF algorithm on an electronic card demonstrates the practicality of the proposed method. Overall, our work highlights the critical role of the BMS, specifically SOC estimation, in ensuring the safe, reliable, and optimal operation of Li-ion batteries, which are essential in enabling the transition to a more sustainable and electrified future.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The company responsible for incremental distribution network operation under the electricity reform will focus on the operating income of this network. To address this issue, this article proposes a dynamic economic dispatch model that takes into account the power-to-gas (P2G) technology. Additionally, it presents a catastrophe genetic algorithm that uses a double iterative optimization genetic algorithm to solve time coupling problems, including those associated with energy storage and demand-side response. To illustrate the effectiveness of this model, the IEEE33 node model was used, and various regulatory methods were investigated. The results confirm that incorporating P2G and demand-side response is crucial for enhancing the operating income of incremental distribution networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, the relevance of AC power flow equations to represent the transmission planning problem has been increasing. This growth is expected to continue due to the rapid improvement of both computational resources and robust optimization techniques. However, using the DC mathematical formulation instead of the AC formulation may result in difficulties in representing significant problems associated with voltage magnitude and reactive power. Despite its advantages, the AC formulation remains challenging to solve even with current solution methodologies. Therefore, developing novel methods that can solve the AC formulation effectively is important. \n\nIn this study, new heuristic methods for solving the transmission planning problem are proposed. It is found that the proposed heuristics for the AC transmission planning problem are both efficient and effective in solving the problem. The researchers have also determined the most suitable heuristic criteria for the proposed heuristics. The simulations are performed on the North Northeast Brazilian test system.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Bitcoin is a digital currency that has gained significant growth and attention in recent years due to its decentralized nature. This means that unlike traditional currencies, Bitcoin does not rely on a centralized authority to control its supply and verification. Instead, it uses a peer-to-peer network of volunteers to distribute pending transactions, verify transactions, and implement a replicated ledger that everyone agrees on.\n\nIn a comparative measurement study of nodes in the Bitcoin network, researchers sought to determine the number of volunteers in the Bitcoin P2P network by scanning the live Bitcoin network for 37 days in 2018. This study was motivated by the fact that Bitcoin has experienced explosive growth in terms of users, transactions, value, and interest over the past few years. The study included the IP addresses of Bitcoin nodes, size of the network, power law in the geographic distribution, protocol and client versions, and network latencies.\n\nBased on the observations made from the measurement study, the researchers proposed a simple distance-based peer selection rule for improved connectivity and faster data propagation. The evaluation results showed that the proposed peer selection rule has the potential to reduce data dissemination latency. Overall, this study provides valuable insights into the current state of the Bitcoin network and suggests ways to improve its performance.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In order to address the challenge of effectively alleviating the semantic gap in medical image retrieval, a novel graph semi-supervised learning method for automated annotation of medical images is proposed. The model leverages semi-supervised technology, allowing it to learn from vast amounts of unlabeled data and minimize the negative impact of limited labeled data on generalization ability. By enhancing graph-based semi-supervised learning technology and normalizing decision boundary on its iterative results, the scoring model mitigates the impact of asymmetric dataset. Moreover, image similarity calculation is analyzed in detail, considering the relationship between word extraction and image in image learning model. The high-level semantic feature of an image is combined with physician's diagnosis information to facilitate more effective similarity calculation. In order to assess the effectiveness of the method, Toy data and clinical data sets gastroscope image sets were subjected to a series of experiments. The results demonstrate that the proposed approach outperforms traditional image annotation methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Information fusion plays a crucial role in both engineering and biological systems, including human cognition. The process of fusion can occur on multiple levels, ranging from the low-level combination of signals to the high-level aggregation of heterogeneous decision-making processes. Nevertheless, despite the explosive growth of deep learning research in recent years, the application of fusion in neural networks has not witnessed the same revolution. Most neural fusion methods are either ad hoc, poorly understood, distributed rather than localized, and/or lack explainability.\n\nThis paper proposes the use of the fuzzy Choquet integral (ChI), a potent nonlinear aggregation function, for fusion in neural networks. The authors demonstrate that ChI can be represented as a multilayer network, termed ChIMP, and introduce an improved version, iChIMP, which uses stochastic-gradient-descent-based optimization to handle the exponential number of ChI inequality constraints. ChIMP/iChIMP are also designed to enable explainable artificial intelligence (XAI). Synthetic validation experiments are conducted, and iChIMP is employed to fuse a set of heterogeneous deep models for remote sensing. The results demonstrate that model accuracy is improved, and the XAI indices provide valuable insight into the quality of the data, model, and its decisions.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we introduce a novel method for designing optical filters based on photonic crystals, that involves a mathematical model powered by machine learning. This approach is primarily targeted towards the near-infrared spectral range. The proposed mathematical model can effectively predict both the design and spectral response of the filter, thereby considerably reducing the time and effort required for simulations. We elaborate on the numerical simulation of the optical filter device and also present the spectral results and mathematical modeling.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper describes the practical application of the newly developed educational system named \"TechPedia\", which has been tailored to meet the needs of professional secondary schools specializing in ICT, electrical engineering and automation. Experts from various European universities have contributed to the content of the system in order to make secondary education and training more appealing to students, thus facilitating their seamless growth and transition to technical universities.\n\nTo achieve this goal, the project consortium has developed a modern electronic learning environment and provided hundreds of learning objects that include learning modules, worksheets, tests, a multilingual dictionary, and an encyclopedia. These learning objects have given the project its name - TechPedia. The educational system is currently in its pilot phase where it is being tested with students and teachers in various schools across Europe.\n\nIn addition, students can compare their knowledge through an international \"Technical Olympiad\". The project has been recognized and awarded the first prize at the eLearning 2016 nationwide competition.\n\nOverall, the TechPedia educational system is set to revolutionize the way technical subjects are taught in secondary schools, enhancing students' learning experience and increasing the attractiveness of technical education. The successful pilot phase and recognition by the eLearning 2016 competition are a testament to the promising future of TechPedia.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Millimeter wave (mmWave) communication is a critical technology for 5G and future networks, thanks to its high data transmission rate and large bandwidth. However, beam tracking is a complex task in a vehicular mmWave system due to fast mobility and the narrow beam of mmWave transmission. To address this challenge, the authors propose an intelligent beam tracking scheme with low training overhead for mmWave vehicular transmission. The proposed scheme uses past channel state information (CSI) to predict future channels by leveraging a machine learning prediction model. With the predicted CSI, base stations can reduce channel estimations, saving the overhead of pilots. The authors use a long short-term memory (LSTM) structure to build the prediction model based on a dataset of channel vectors of each coherence time duration. Experimental results demonstrate that the proposed LSTM accurately predicts vehicular user channels and achieves satisfactory transmission rates with less pilot overhead than traditional beam training schemes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Network Slicing is a critical component of 5G cellular network communication. It involves the partitioning of one physical network into several virtual networks to achieve specific goals such as security, flexibility, and better control over the network. This logical isolation in devices, services, and core networks allows for different characteristics and types of services to be set up.\n\nOur proposed work utilizes the end-to-end network slicing concept in 5G networks to address the crucial issue of isolating slices. We prioritize the virtual networks to increase performance and reduce latency for high priority applications. Our study also shows that using the NS-3 network simulator, simulation results prove that our approach enhances both latency and performance.\n\nOverall, our work demonstrates the potential benefits of network slicing in 5G networks and how end-to-end slicing can be utilized to optimize performance and reduce latency for high priority applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Due to the shortage of spectrum in traditional cellular networks, the application of millimeter-wave technology has been suggested to be used in small cells with high-density deployment and multi-hop backhaul structure. Therefore, a cross-layer optimization problem has been formulated to maximize energy efficiency while taking into account the selection of the route and allocation of resources for this novel architecture. To achieve this optimization, the problem has been decoupled into two sub-problems consisting of resource allocation at the link-physical layer and route selection at the network layer. While the non-convex and NP-hard problem of resource allocation has been solved using stochastic algorithms, the simplified linear programming problem at the network layer has been solved through an LP solver to enhance energy efficiency. Additionally, two joint optimization strategies have been proposed, with the first scheme utilizing stochastic algorithms and linear programming to optimize resource allocation and route selection, respectively, based on the provided routing, while the second scheme uses linear programming to assess the suitability of individuals in stochastic algorithms. The simulation results have confirmed that the proposed schemes can provide almost optimal solutions by improving both energy and spectrum efficiency, and also demonstrate that identifying the right path without overloading any base station can significantly improve network throughput.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The security risks surrounding data security and user privacy protection in cloud environments are becoming increasingly serious, making ciphertext data storage an important tool for preventing privacy disclosure. However, a new challenge arises when attempting to search for keywords within ciphertext data without revealing keyword information. To address this issue, searchable encryption (SE) has been proposed, allowing for direct ciphertext-search capabilities. Public-key encryption with keyword search (PEKS) is commonly used in multi-user data sharing scenarios, as it has been extensively studied and developed in recent years. However, existing PEKS schemes often lack flexible access policies. To address this limitation, a new scheme has been proposed that combines the advantages of policy control from attribute-based encryption (ABE) with the robust security of lattice-based cryptography. This new keyword-searchable ABE scheme supports flexible attribute control policy by integrating ABE and PEKS techniques. The security of the scheme is proven under the learning with errors (LWE) assumption. With lattice-based cryptography being resistant to quantum attacks, this new scheme offers stronger security in a quantum era.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Matrix completion is a fundamental problem in image processing, where the aim is to recover an incomplete matrix from its observed entries. One popular approach to this problem is to treat it as a low-rank matrix approximation problem. In a recent paper, a new regularization method called truncated Frobenius norm (TFN) was proposed, which was used in combination with a hybrid truncated norm (HTN) model for solving matrix completion problems. \n\nTo address this model, a two-step iteration algorithm was designed, along with an adaptive way to adjust the penalty parameter to reduce computational costs. The convergence of the proposed method was also discussed and mathematically proven. \n\nThe proposed approach not only improves the recovery performance but also promotes the stability of the model. It eliminates large variations that exist when estimating complex models and achieves competitive success in matrix completion. \n\nExperimental results on synthetic data, real-world images, and recommendation systems confirm the effectiveness and superiority of the proposed method. The statistical analysis strategy used in the experiments showed that the proposed method is more stable and effective than other state-of-the-art approaches.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the growing interest in computer vision and robotic applications, recovering 3D human poses has become increasingly important. However, this task is quite challenging due to the diverse appearances, viewpoints, occlusions, and geometric ambiguities within monocular images. Existing methods attempt to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions. However, these methods have limited scalabilities due to insufficient 3D pose data for training and the domain gap between the 2D and 3D spaces.\n\nTo address this issue, this paper proposes a simple yet effective self-supervised correction mechanism to learn all intrinsic structures of human poses from abundant images. The proposed mechanism involves two dual learning tasks: the 2D-to-3D pose transformation and 3D-to-2D pose projection. These tasks serve as a bridge between 3D and 2D human poses in a type of \"free\" self-supervision for accurate 3D human pose estimation. \n\nThe 2D-to-3D pose transformation sequentially regresses intermediate 3D poses by transforming the pose representation from the 2D domain to the 3D domain under the sequence-dependent temporal context. The 3D-to-2D pose projection refines the intermediate 3D poses by maintaining geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, these two dual learning tasks enable the model to adaptively learn from 3D human pose data and external large-scale 2D human pose data.\n\nAdditionally, the self-supervised correction mechanism is applied to develop a 3D human pose machine that jointly integrates the 2D spatial relationship, temporal smoothness of predictions, and 3D geometric knowledge. Evaluations on the Human3.6M and HumanEva-I benchmarks demonstrate the superior performance and efficiency of this framework over all the compared competing methods.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Soft robots have become a popular topic in recent years due to their unique properties. Unlike traditional rigid robots, soft robots can deform in a wide range to suit the environment and can compliantly contact operating objects. This property makes them highly suitable for various applications, such as physical rehabilitation, minimally invasive surgery, and other fields.\n\nIn this paper, we present an omni-directional flexural inflatable flexible arm model, which consists of three independent controllable pneumatic components. The components have different elongations under the effect of pressure and flexible arms have bending deformation. Under ideal conditions, this model can achieve a 90\u00b0 bending and 360\u00b0 twisting.\n\nTo model the flexible arm, we utilized the Yeoh model of hyperelastic rubber material and the geometric analysis method. We also employed finite element analysis to simulate the actuation characteristics of these modules. The analytical and computational results were then compared to experimental results to ensure the validity of the model for future design and control of soft robotic actuators.\n\nOur research has contributed to advancing the feasibility and control of soft robots, presenting infinite possibilities for future application in industries such as healthcare, rescue missions, and exploration.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Motor imagery brain-computer interfaces (BCIs) allow for the control of external machines through neurophysiological signals generated during the imagination of limb movements, without actually physically moving. Electroencephalography (EEG)-based motor imagery tasks have distinct characteristics of event-related (de)synchronization (ERD/ERS) in specific frequency bands, such as alpha and beta rhythms in the sensorimotor area. Motor imagery features are typically extracted using common spatial patterns (CSPs) based on this phenomenon. However, researchers have reported that ERD features exhibit significant inter-subject variation. This study investigated the correlation between various ERD features and classification accuracy during a motor imagery task. Results indicate that while ERDs are representative features of a motor imagery task, they may not be useful in accurately estimating classification accuracy.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A new optimization algorithm using Seeker Optimization Algorithm (SOA) has been proposed to reduce the side lobe ratio or level (SLL) maximum with half power beam width (HPBW) of the thinned large multiple concentric circular antenna of the uniformly exuberated isotropic element. The Concentric Circular Antenna Array (CCAA) has gained recognition in the field of radio and telecommunications systems due to its 360-degree scan around its focus, beam pattern, and all-azimuth scan capability, among other benefits. The proposed algorithm examined a 9 ringed CCAA with central element feeding, which was found to be suitable for isotropic elements.\n\nThe optimization efficiency was verified through antenna array synthesis, which showed that the relative sidelobe level (SLL) was less than -20 dB with fixed HPBW, compared to the uniform array case. The simulation results showed that by using SOA with isotropic elements, the effective number of antenna elements could be reduced from 279 to 120 with a consistent reduction in side lobe ratio by 21.04 dB relative to that of the main beam, with a fixed HPBW.\n\nTo validate the results obtained using SOA, Standard Particle Swarm Optimization (PSO) was used to scrutinize the outcomes. Overall, the proposed approach using SOA demonstrated significant improvements in antenna performance and optimization efficiency.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In industrial wireless sensor networks, various applications with different requirements are rapidly developing, and thus providing Quality of Service (QoS) is becoming increasingly essential. However, traditional network architectures face significant difficulties in implementing and configuring QoS-based network management. To address these issues, we propose a hierarchical software-defined network architecture for wireless sensor networks. Our approach makes complex network management possible, and the system becomes more adaptable. \n\nAdditionally, we introduce the QSDN-WISE QoS-based routing protocol, which consists of a clustering algorithm, a routing algorithm, and local network maintenance. The double-cluster head-based uneven clustering algorithm, called DCHUC, avoids energy holes and reduces the workload of a single cluster head. Our centralized QSDN-WISE routing algorithm constructs two heterogeneous forwarding paths for nodes that meet the different data level requirements. Finally, our approach reduces the number of control messages in the network using local network maintenance. \n\nSimulation results show that our proposed approach delivers QoS support for data with varying requirements, balances network energy consumption, and prolongs the network's lifetime. In summary, our approach offers a promising solution for QoS-based network management in industrial wireless sensor networks.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we investigate the problem of estimating a scalar linear process over a channel that utilizes the timing of transmitted symbols for communication. The decoder observes each transmitted symbol with a random delay, and the encoder uses the holding times between successive transmissions to encode messages. The decoder then decodes the message from the inter-reception times of successive symbols. This scenario is reminiscent of a telephone system, where a transmitter signals a phone call to the receiver through a \u201cring\u201d and, after a random time to establish the connection, knows that the \u201cring\u201d has been received.\n\nOur main contribution is to establish the minimum capacity requirements for the channel to enable the estimation error to converge to zero with high probability. Specifically, we show that the timing capacity of the channel should be at least as large as the entropy rate of the process. In the special case when symbol delays are exponentially distributed, we provide a precise sufficient condition using a random-coding strategy. \n\nOverall, our results shed light on the fundamental limits of timing-based communication channels for estimating linear processes and provide insights for designing communication systems that utilize timing information.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This brief provides analytical results on the impact of additive weight/bias noise on the performance of a Boltzmann machine (BM) where the output of the unit belongs to the set {-1, 1} as opposed to {0, 1}. When such noise is present, it is observed that the state distribution remains a Boltzmann distribution but with an increased temperature factor. Consequently, a gradient ascent learning algorithm is proposed, and a corresponding learning procedure is developed. The learning process is then compared with the learning procedure employed for training a BM with noise. It is found that both methods are identical, and therefore, the learning algorithm for noise-free BMs is applicable as an online learning algorithm for an analog circuit-implemented BM, irrespective of the variances of the additive weight noise and bias noise.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To enhance the noise immunity of the commonly used mel-frequency cepstral coefficients (MFCC) for voice signal parametrization, a possible solution is to incorporate a psychoacoustic model of frequency masking. Furthermore, by considering the formation mechanism of formant regions in the voice signal spectrum, the spectral samples that correspond to multiple harmonics of the fundamental tone could also be modified. The effectiveness of this modified algorithm was tested using a single word recognition system that is adapted for MFCC voice signal parametrization only. The results showed that this proposed additional voice signal transformation has a positive impact on the parametrization algorithm.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The security of the Internet of Things (IoT) has become increasingly concerning, and there is a pressing need to rapidly detect and fix device vulnerabilities. However, with the vast number of IoT devices, conducting regular security inspections is challenging. Current vulnerability detection technology relies on simple feature matching, which lacks the accuracy to detect firmware vulnerabilities, while direct control flow graph matching is costly. To address this problem efficiently and accurately, we propose a staged firmware vulnerability detection method based on code similarity.\n\nOur method uses function embedding based on neural networking in the first stage to analyze function similarities, enabling the efficient inspection of large-scale firmware security. In the second stage, the similarity of function local call flow graphs is calculated, providing fine-grained firmware security analysis and improving the accuracy of vulnerability detection. Compared with state-of-the-art approaches, our method demonstrated superior accuracy in experimental results. The average retraining time for our method is just one hour, with real-world firmware vulnerability detection experiments showing an 86% true positive rate for the top 30.\n\nOverall, our method offers an effective solution for detecting and fixing vulnerabilities in IoT devices in a timely and accurate manner. This is critical in mitigating security threats associated with the increasing prevalence of the Internet of Things.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unmanned aerial vehicle (UAV) communication is a promising technology for Internet of Things (IoT) systems, and combining it with nonorthogonal multiple access (NOMA) can lead to high-capacity IoT uplink transmission systems. In our research paper, we proposed a joint optimization approach to maximize the system capacity by optimizing the subchannel assignment, the uplink transmit power of IoT nodes, and the flying heights of UAVs. \n\nTo begin with, we developed an efficient subchannel assignment algorithm that utilized K-means clustering method and matching theory. Then, we determined the distributed uplink transmit power of IoT nodes as well as the flying heights of UAVs through successive optimization approach. Additionally, we proposed an alternative optimization algorithm to find near-optimal solutions. \n\nThe numerical results obtained from our proposed scheme demonstrate its superiority, thereby verifying our approach. Overall, this research highlights the potential of UAV communication and NOMA for constructing high-capacity IoT uplink transmission systems.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Many applications in hyperspectral remote sensing involve detecting solid targets automatically. The design and applicability of such systems depend on their detection performance under various deployment scenarios. Therefore, it is important to develop an accurate and realistic performance model. In this paper, we present a performance prediction model for hyperspectral target detection using the common matched filter and normalized matched filter detection algorithms. We discuss a statistical model for hyperspectral data, which includes a replacement model for target pixels and a mixture of t-elliptically contoured distributions for background materials. Analytic forms for detector output distributions are not always available for this general input model. The key contribution of this paper is to develop an efficient and robust performance simulation technique using Monte Carlo with importance sampling. Our simulation technique requires a significantly smaller sample size than standard Monte Carlo to obtain accurate performance estimates at low false alarm rates. The proposed technique is useful for predicting detection performance across a wide range of input models when analytic solutions are unavailable.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper discusses a simulation model for a unique indoor localization approach that relies on the phase difference between components in an antenna array. The aim of this localization system is to pinpoint the location of a semi-passive RFID tag. To demonstrate the feasibility of this method, a series of experiments using a readily available transceiver were conducted to measure the phase differences. The findings demonstrate that this simulation model provides an accurate estimation and evaluation tool for phase measurements in indoor localization applications.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To overcome the limitations of traditional static measurement methods that include low efficiency, inconvenient data storage, and higher requirements for testers, a dynamic measurement method has been proposed for real-time online measurement of the amplitude frequency response of the broadcasting transmitter. The proposed model and test principles of the measurement system are explained in detail.\n\nThe dynamic method involves real-time collection of input and output signals from the broadcast transmitter. The generalized cross-correlation algorithm is used initially to find the system delay, followed by the FFT algorithm of time delay data interception to calculate the system's amplitude-frequency response.\n\nExperiments on the proposed method have demonstrated accuracy and reliability, and as such, it is deemed practical and generalizable.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The VANET broadcasting algorithm is essential in enhancing the speed and reliability of vehicle-to-vehicle communication. The multi-hop broadcasting technique is an emerging approach that promises better transmission efficiency. This research puts forward a multi-hop broadcasting algorithm, Delay-Constraint Broadcast Combined with Resource Reservation Mechanism (DBCRRM), which can remarkably improve the Packet Delivery Ratio (PDR). The proposed algorithm considers the distance between the source and receiving nodes to select the forwarding node and utilizes the Resource Reservation Mechanism (RRM) to reserve the resource of the timeslot and frequency. Additionally, the algorithm ensures that the delay is within an acceptable range. \n\nVANET broadcasting algorithms are typically assessed via simulation as field tests are difficult to conduct. This limitation often results in a gap between simulation and practical results. To address this shortcoming, the researchers developed a field test platform for broadcasting algorithms in VANET and practically tested the Delay-Constraint Broadcast (DB) and DBCRRM. The results of the practical tests indicated that DBCRRM and DB outperformed the test results of the two-hop and one-hop broadcasts. Therefore, the DBCRRM algorithm is practical and can enhance the efficacy of VANET broadcasting.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Current forensic techniques for detecting image manipulation have a crucial flaw in assuming that each pixel belongs to one of two classes: genuine or manipulated. This binary approach can result in mis-labeling, especially in situations where there is either a genuine image with unique content or when there are multiple manipulations present. To address this issue, we propose a new approach that uses n-ary clustering algorithms to relax these constraints. This approach is exemplified in the widely used pixel descriptor space of residual co-occurrences. Our experiments on public benchmark datasets have shown that this approach provides substantial benefits for image manipulation localization.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a new approach to analyze manufacturing data related to the production of circuit breakers, which are indispensable components of power distribution lines. Although the life expectancy of a circuit breaker is typically 30 to 40 years, some fail prematurely within a short period of 0-5 years, and identifying the causes of early failure of these crucial components is of utmost importance. To address this problem, we propose a structured methodology that leverages machine learning and data analytics to explore the manufacturing data, uncover failure root causes, and provide actionable recommendations to the manufacturer for enhancing their manufacturing operations.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The integration of solid-liquid phase change materials (PCMs) into electronics packaging has shown promise in reducing transient temperature rises of components experiencing pulsed thermal loads. However, determining the impact of incorporating PCMs in various locations and configurations within an electronics package is challenging due to the non-linear response of PCM to a transient heat load. To address this issue, ParaPower, a new parametric design tool was developed to facilitate electronics package design. ParaPower includes the ability to incorporate PCM volumes in explicit locations and evaluate their impact on temperature distributions within the electronics package over time. This study determined the spatial and temporal order of convergence for the reduced-phase-change thermal model underlying ParaPower. Results were compared and validated against an analytical solution and a higher-fidelity commercial software. The study showed that the fast-solving methods used in ParaPower produced results with comparable accuracy to those obtained using high-fidelity commercial software. This tool enables fast exploration of design spaces, allowing for increased efficiency in reducing size and cost while minimizing overdesign. The research also quantifies the trade-off between time steps, grid size, and accuracy to obtain the necessary balance. Thus, design tools such as ParaPower have the potential to significantly advance design theory.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The use of public blockchain network (PBN) has become increasingly popular in wired networks like the bitcoin network. Proof-of-work (PoW) algorithm is usually deployed among miners to reach consensus on user data during the mining process. However, the computation required with PoW consensus mechanism makes it difficult to apply PBN in wireless mobile networks since most Internet of Things (IoT) and mobile devices (IMDs) are resource-limited.\n\nWith the rise of mobile edge computing (MEC), IMDs can now offload their computation tasks to edge nodes. But, there is still the issue of competition among enormous solo mining IMDs when reaching consensus. This paper addresses this challenge by formulating the computation resource allocation problem of PBN from the perspective of coalition game theory under the MEC environment. \n\nThe proposed algorithm utilizes coalition formation game theory to maximize the system sum utility while considering both the individual profit of IMD and coalition profit. It has been proven that this algorithm converges to a Nash-stable partition with a fast convergence rate and reaches near-optimal solution with low computational complexity. \n\nThe simulation results indicate that the proposed algorithm outperforms other schemes in terms of system sum profit and ratio of rewarded IMDs to overall IMDs. The optimality and convergence of the proposed algorithm have also been well demonstrated. \n\nIn summary, this paper presents a coalition game-theoretic approach to solve the computational resource allocation problem of PBN under the MEC environment. The proposed algorithm provides a practical solution with high efficiency and low computational complexity.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The accuracy of action recognition depends on correctly interpreting the spatial attribute of a detected person's posture and the temporal attribute of their body movement. While deep learning has improved image recognition, there has not been the same progress in action recognition due to the complexity caused by the additional temporal dimension and the lack of annotated training data. To address this, a handcrafted cued LSTM model is proposed for human action recognition based on RGB-D data. The model uses handcrafted cues, pre-processed results designed to facilitate focused learning, as input to the LSTM structure. Spatial cues are derived from Skeleton View-invariant Transformation, while temporal cues are obtained by computing the displacements of all joints across down-sampled raw data. The experiment conducted on NTU-RGB-D, the most comprehensive dataset for action recognition, shows that recognition based on the proposed handcrafted cues outperforms recognition based on raw data. Furthermore, the recognition performance is improved through the proposed techniques of feature fusion and/or decision fusion of these two handcrafted cues, outperforming the state-of-the-art approaches conducted on the same dataset using the same train/test protocol.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Reducing delay and latency in cloud computing environments presents a significant challenge for researchers. With the proliferation of smart cities around the world, there is an increasing demand for Smart Communities (SCs), Smart Buildings (SBs) and Smart Homes (SHs) to use cloud resources to process and store data. To address these challenges, a new fog computing infrastructure has been introduced to enhance cloud efficiency. Virtual Machines (VMs) are installed on fog servers, to whom consumers' requests are allocated. This paper proposes an integrated environment that combines cloud and fog technologies to overcome delay and latency issues in cloud computing while enhancing fog performance. \n\nLoad balancing is another major challenge when there is an increasing number of incoming requests on fog and cloud. This paper introduces two load balancing algorithms that are optimized for efficient performance: Cuckoo search with Levy Walk distribution (CLW) and Flower Pollination (FP). The proposed algorithms are compared to existing Cuckoo Search (CS) and BAT algorithms. Comparative analysis is performed on the basis of Closest Data Center (CDC), Optimize Response Time (ORT) and Reconfigure Dynamically with Load (RDL). Real-time data centers (DCs) of clouds, clusters, and processing time (PT) of fogs are also optimized based on CLW and FP. \n\nOverall, the proposed integrated environment and load balancing algorithms represent a significant advance in cloud and fog computing technologies. They offer improved performance, reduced delay and latency, and greater efficiency in processing and storing data for Smart Communities, Smart Buildings, and Smart Homes.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper covers the theoretical evaluation and practical determination of parameters for a 3-phase, 4 pole, 320 W, 5 m/s linear permanent magnet synchronous motor (LPMSM). The motor was designed in a laboratory and manufactured by a local company. The parameters were estimated through analytical calculations and validated with standard FEM packages. The determined parameters through practical testing were in excellent agreement with the theoretical estimates. This demonstrates the accuracy in design, precision in manufacturing, and correctness in testing. Additionally, the LPMSM motor's field weakening capability was investigated.", "response": "1"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper tackles the problem of millimeter-wave (mmWave) channel estimation in massive MIMO communication systems. A new Bayes-optimal channel estimator is derived using recent advances in the approximate belief propagation Bayesian inference paradigm. By leveraging the inherent sparsity of the mmWave MIMO channel in the angular domain, we recast the underlying channel estimation problem into that of reconstructing a compressible signal from a set of noisy linear measurements. Then, the generalized approximate message passing (GAMP) algorithm is used to find the entries of the unknown mmWave MIMO channel matrix. Unlike all the existing works on the same topic, we model the angular-domain channel coefficients by Laplacian distributed random variables. Furthermore, we establish the closed-form expressions for the various statistical quantities that need to be updated iteratively by GAMP. To render the proposed algorithm fully automated, we also develop an expectation-maximization (EM) based procedure that can be easily embedded within GAMP's iteration loop in order to learn all the unknown parameters of the underlying Bayesian inference problem. The computer simulations show that the proposed combined EM-GAMP algorithm under a Laplacian prior exhibits improvements both in terms of channel estimation accuracy, achievable rate, and computational complexity, as compared to the Gaussian mixture prior that has been advocated in the recent literature. In addition, it is found that the Laplacian prior speeds up the convergence time of GAMP over the entire signal-to-noise ratio range.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Safety assurance in critical IoT systems is increasingly important as those systems are becoming ubiquitous in various application areas, such as health care and transportation. Recently, novel runtime monitoring approaches started to adapt expressive rule- or query-based languages to capture safety properties, which are evaluated over runtime models. In this paper, we define two distributed query-based runtime monitoring strategies for IoT and cyber-physical systems built on top of a distributed runtime model deployed over the Data Distribution Service as standard communication middleware. We provide detailed scalability evaluation of our prototype implementation over a case study motivated by an open-source IoT demonstrator.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cyanobacterial blooms increasingly pose threats to ecosystems and human health. This paper is aimed to propose systematic risk pre-control schemes by understanding the complex causalities between cyanobacteria and multiple influencing variables. This research remains a challenge for three reasons. Firstly, the time-series evolution of cyanobacteria is characterized by deep uncertainties and nonlinear dynamics. Secondly, latent variables with hidden information usually exist in this kind of complex aquatic system. Thirdly, it is difficult to identify an efficient pre-control scheme that specifies variables for preferential regulation. To address these problems, we propose a latent variable structured Bayesian network model and a corresponding parameter learning algorithm. The model is tested by real-time spatio-temporal data. The computational results reveal that the proposed model demonstrates better performance in terms of inference accuracy and degree of system understanding. Based on sensitivity analysis and combination-effect analysis, a systematic risk pre-control scheme is proposed for decision-makers to prevent cyanobacterial blooms under the scenario of global warming.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Daily human activity recognition using sensor data can be a fundamental task for many real-world applications, such as home monitoring and assisted living. One of the challenges in human activity recognition is to distinguish activities that have infrequent occurrence and less distinctive patterns. We propose a dissimilarity representation-based hierarchical classifier to perform two-phase learning. In the first phase, the classifier learns general features to recognise majority classes, and the second phase is to collect minority and subtle classes to identify fine difference between them. We compare our approach with a collection of state-of-the-art classification techniques on a real-world third-party dataset that is collected in a two-user home setting. Our results demonstrate that our hierarchical classifier approach outperforms the existing techniques in distinguishing users in performing the same type of activities. The key novelty of our approach is the exploration of dissimilarity representations and hierarchical classifiers, which allows us to highlight the difference between activities with subtle difference, and thus allows the identification of well-discriminating features.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The paper proposes a bio-inspired model for an imitative sensorimotor learning, which aims at building a map between the sensory representations of gestures (sensory targets) and their underlying motor pattern through random exploration of the motor space. An example of such learning process occurs during vocal learning in humans or birds, when young subjects babble and learn to copy previously heard adult vocalizations. Previous work has suggested that a simple Hebbian learning rule allows perfect imitation when sensory feedback is a purely linear function of the motor pattern underlying movement production. We aim at generalizing this model to the more realistic case where sensory responses are sparse and non-linear. To this end, we explore the performance of various learning rules and normalizations and discuss their biological relevance. Importantly, the proposed model is robust whatever normalization is chosen. We show that both the imitation quality and the convergence time are highly dependent on the sensory selectivity and dimension of the motor representation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In pig production, food processing and profitability ratios can be assessed by detecting live weight of live pigs in real time. Traditional pig weight detection often requires direct contact with pigs, which is limited by low efficiency and can result in even death. Detecting the weight of non-contact pigs has become a challenge in pig production for decades. The proposed system offers significant features such as color, texture, centroid, major axis length, minor axis length, eccentricity and area. For recognition for estimation weight of pig, we use statistics from the original database with neural network. Analysis of experimental results of contactless pig weight estimation as well.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper addresses the sensitivity analysis of an extended and dual Kalman filters used for state estimation (state of charge (SoC), inner resistance) of lithium-ion batteries mainly for traction applications. The Kalman filters in different configurations are now widely applied in modern battery management systems. It is well known that the quality of estimation is dependent from the accuracy of model used. In this case a compromise between model's complexity and quality of estimation is one of main critical factors while developing a Kalman filter based estimation solution. In case of state estimation of lithium-ion batteries this point becomes important due to the following reasons: transformation of chemical effects to electrical model causes the loss of model accuracy; model parameters are not constant due to ageing and temperature rice; parameters of derived discrete time models are sensitive to measurement noise. In this paper authors analyse the influence of varying inner parameters of a lithium-ion battery and the choice of time discrete model onto accuracy of SoC estimation. An extended and dual discrete Kalman filter using zero-order hold method and Tustin transform are tested using new European driving cycle.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Predicting and supporting students' behavior turns out to be all the more difficult because of the vast volume of information in educational databases. Currently in Lebanon, the absence of existing system to dissect and screen or the student performance and progress is not used. We analyze educational data to figure out the reasons behind students' behaviors and make a decision for solutions and treatment paths. A systematical system on anticipating student performance is proposed by utilizing mining data procedures to enhance students' accomplishments. We could actually improve students' achievement, success, and bring the benefits and impacts to students, educators and academic institutions. This paper centers on how the expectation calculation can be utilized to distinguish the most imperative qualities in students' information (behavior). We could really enhance students' accomplishment, achievement, and convey the benefits and effects to students, instructors and scholarly establishments.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Nokia Mobile Networks, Ulm, Germany This paper addresses the processing principles and performance of OFDM based radar, with particular focus on the LTE mobile network base-stations and the use of the LTE downlink transmit waveform for radar/sensing purposes. We specifically address the problem stemming from the unused subcarriers, within the transmit signal passband, and their impact on the frequency domain radar processing. We also formulate and adopt a computationally efficient interpolation approach to mitigate the effects of such empty subcarriers in the radar processing. We evaluate the target range and velocity estimation performance through computer simulations, and show that high-quality target detection can be achieved, with LTE waveform, when combined with the interpolation approach. Impacts of the different LTE carrier bandwidths and number of transmitted LTE sub-frames are also evaluated, together with aggregating up to 5 individual 20 MHz LTE carriers.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Living in the world of convergence where the demand for cloud storage has rapidly increased. This is due to vast amounts of data need to be stored and shared between multiple people via internet or intranet connection. It is essential for making it efficient and effective. Although existing cloud storage provides a number of features and capabilities, such as reliability where the user can access various resources despite location and time, there are still some drawbacks or gap in managing the unlimited resources. Hence, it is undeniable that, the most important feature that a cloud storage should possess is a smart data sharing management. In order to overcome such implication, virtual Intelligent Dynamic Data Cloud Center (ViDaC) application are proposed to fill the gap. Virtual Intelligent Dynamic Data Cloud Center (ViDaC) application is the smart data sharing management system which will not only able to manage data smartly, it will also assist in validating and ensure each content of the data or document submitted should fulfil the given conditions and specifications. Failure in abating the problems in cloud storage data management will not only affect the quality of productivity but will also invoke various critical management issues in an organization. Therefore, ViDaC is the solution to provide a better environment for managing data and so all the limitless resources can be supervised and coordinate effectively. Besides that, this paper also studies the existing design architecture of resource management in the cloud environment.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The threat of radio frequency jamming attack to cognitive radio network is an issue that has been discussed for a long time. Q-learning is a widely used anti-jamming algorithm due to its model-free characteristic. However, the traditional Q-learning based anti-jamming algorithms suffer from some limitations when dealing with high-dimensional or continuous inputs. The recently proposed double Deep Q-learning Network (DQN) overcomes this weakness by approximating the table based Q function with a deep neural network. In this paper, we apply the double DQN algorithm with frequency hopping strategy against RF jamming attack in a multi-user environment. We test the performances of three types of neural networks which are the fully connected network (FCN), the convolutional neural network (CNN) and the long short term memory (LSTM). The simulation shows the effectiveness of the double DQN algorithm. Meanwhile, the FCN agent gives the best result concerning stability.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The response speed of a network impacts the efficacy of its actions to external stimuli. However, for a given bound on the update rate, the network-response speed is limited by the need to maintain stability. This work increases the network-response speed without having to increase the update rate by using delayed self-reinforcement (DSR), where each agent uses its already available information from the network to strengthen its individual update law. Example simulation results are presented that show more than an order of magnitude improvement in the response speed (quantified using the settling time) with the proposed DSR approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rise of social networking and the awareness of privacy protection, the large-scale positioning data is getting harder to access while the clickstream data of social networking is accumulating. It is necessary to conduct a comprehensive study on the temporal property and spatial distribution of social network's clickstream to determine the latent relationship between clickstream and location semantics which semantically annotate locations to help us understand the purpose of a social network user at a certain location and a particular time. In this paper, we first infer the location semantics from Internet logs as our baseline which is a large scale, imprecise positioning mobile dataset through combining the utilization of derived time pattern and speed pattern. Then we design a method to reconstruct location semantics through the social network. We are the first to our knowledge that define \u201cclick motif\u201d of social network's clickstream which is the combination of clicks in clickstream that occurs repeatedly and has significance. Click motif not only can help us to classify location semantics but also enhance the user experience of social networking. Besides, we propose a completed process to extract typical click motifs from a large-scale complicated clickstream data via embedding and clustering method. By extensive experiments and analysis, we demonstrate that different combination of click motifs can convey the users' location semantics, therefore clickstream may disclose location privacy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Learning the correlation among labels is a standing-problem in the multi-label image recognition task. The label correlation is the key to solve the multi-label classification but it is too abstract to model. Most solutions try to learn image label dependencies to improve multi-label classification performance. However, they have ignored two more realistic problems: object scale inconsistent and label tail (category imbalance). These two problems will impact the bad influence on the classification model. To tackle these two problems and learn the label correlations, we propose feature attention network (FAN) which contains feature refinement network and correlation learning network. FAN builds top-down feature fusion mechanism to refine more important features and learn the correlations among convolutional features from FAN to indirect learn the label dependencies. Following our proposed solution, we achieve performed classification accuracy on MSCOCO 2014 and VOC 2007 dataset.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Handwriting recognition has been a heated topic over years. Due to the development of deep learning, a lot of research has been done to apply convolutional neural network (CNN) model to this task, which have achieved outstanding accuracy. Instead of focusing merely on CNN models, this article takes the features of Chinese handwritten character into consideration and manages to extract the information of the stroke order of the online handwriting recognition. To achieve this, two methods are proposed: (1) Design a two-branch model combining CNN and recurrent neural network (RNN);(2) Give a new channel division strategy. Also, the task of advanced prediction of the character which little research has been worked on is the key point. With the information of stroke order and some data augmentation strategy, methods proposed have achieved satisfying results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper investigates the problem of sliding mode observer (SMO) design for Takagi-Sugeno (T-S) fuzzy descriptor fractional order systems (FOSs) with fractional order $0<; \\alpha <; 1$. First, an SMO is designed for the T-S fuzzy descriptor FOS. Then, a fuzzy fractional order integral-type sliding surface is constructed. By utilizing the fuzzy sliding surface, the assumption that all local input matrices are identical in most existing results about sliding mode control (SMC) for T-S fuzzy systems, can be removed. Then, a new sufficient condition is proposed in terms of linear matrix inequalities (LMIs), which guarantees the admissibility of the sliding mode dynamics. Moreover, adaptive SMC strategy is used such that the reachability condition can be guaranteed. Finally, three examples are given to verify the effectiveness of our results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A key challenge in renal diagnosis using digital pathology has been the scarcity of reliable annotated datasets that can act as a benchmark for histological investigations. This paper uses a novel medical image dataset, titled Glomeruli Classification Database (GCDB), consisting of renal glomeruli images bifurcated into binary classes of normal and abnormal morphology. Based on this dataset, we direct our pioneering efforts to explore suitable deep neural network techniques related to kidney tissue slide imaging so as to establish a state of the art in this relatively unexplored domain. The paper focuses on classifying normal and abnormal categories of glomeruli which are the vital blood filtration units of the kidney. The results obtained using publicly available transfer learning models are held in comparison with supervised classifiers configured with image features extracted from the last layers of pre-trained image classifiers. Contrary to popular belief, transfer learning models such as ResNet50 and InceptionV3 are empirically proved to under-perform for this particular task whereas the Logistic Regression model augmented with features from the InceptionResNetV2 show the most promising results on the GCDB dataset.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Emerging cloud applications like machine learning, AI and big data analytics require high performance computing systems that can sustain the increased amount of data processing without consuming excessive power. Towards this end, many cloud operators have started deploying hardware accelerators, like FPGAs, to increase the performance of computationally intensive tasks but increasing the programming complexity to utilize these accelerators. VINEYARD has developed an efficient framework that allows the seamless deployment and utilization of hardware accelerators in the cloud without increasing the programming complexity and offering the flexibility of software packages. This paper presents a modular approach for the acceleration of data analytics using FPGAs. The modular approach allows the automatic development of integrated hardware designs for the acceleration of data analytics. The proposed framework shows the data analytics modules can be used to achieve up to 3.5x speedup compared to high performance general-purpose processors.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents the development and validation of a new oscillatory component load (OCL) model for a measurement-based real-time estimation of dynamic load model parameters. The proposed OCL model has been derived from a second-order differential equation that considers three necessary components, namely static, exponential recovery, and damped oscillatory components. The performance evaluation of the proposed OCL model has been demonstrated through DIgSILENT PowerFactory-based dynamic power system simulations, real-time substation data captured from phasor measurement units located at the primary substations of Leadenham, Holwell, and Regent Street cities, and real-time transient voltage events data provided by the EirGrid. Further, the effectiveness of the proposed OCL model has been assessed by comparing its performance with that of the existing exponential recovery load model. A new flexible and efficient SynchroHub platform, developed at the University of Manchester for real-time data acquisition, processing and visualization, has been used for the real-time estimation of dynamic load model parameters. The validation results revealed the efficacy of the proposed OCL model in tracking both the first as well as the subsequent oscillations of dynamic load model response of the power system for both the steady state as well as transient operating conditions accurately.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The distributed cubature Kalman filter is widely used in the field of target tracking, however, the presence of model uncertainties will undermine its tracking stability and effectiveness for tracking maneuvering target. In order to eliminate this effect on maneuvering target tracking, this paper develops a distributed consensus based strong tracking cubature Kalman filter scheme. First, each node obtains its local estimation with the usage of local observations via strong tracking cubature Kalman filter, where the suboptimal fading factor and adaptive factor are introduced for adaptively modifying the filter gain. Then, the designed filter gain is used for updating the local state estimation. Second, after all, nodes have achieved its local estimation, each node exchanges its local estimation to its neighbors and updates its local estimation according to the consensus communication protocol. It can be further proved that the distributed interaction between neighbors will contribute to enhancing the tracking stability. The detailed proof for stochastic boundedness of the estimation error is analyzed by introducing a stochastic process. Simulation results demonstrate that the proposed algorithm can achieve higher tracking accuracy than the existing methods for tracking a maneuvering target.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless Sensor Network is one of the major areas in wireless communications which extend its application in military, security, internet and scientific purposes. For a network which has dynamically varying channel gain and random topology, selection of operating parameters which improves network performance is always a challenging task. Single channel MAC is generally used in statistical analysis, but providing multi channel MAC will improve the performance of the channel. Multichannel is available in 802.11b/g which operates at 3 different non overlapping channels. In this paper we implemented 802.11g multichannel MAC using NS-2.34 and compared the performance of the network with single channel MAC in terms of throughput, packet delivery ratio and throughput. The multichannel uses back off algorithm and switches between the 3 non overlapping channels whenever the contention window of distributed coordination function reaches the maximum threshold value and thus gives better performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This work presents a parameterized and modularized approach for the implementation of artificial neural network (ANN) on a field-programmable gate array (FPGA). The design investigates how to efficiently model an ANN that is easily adoptable to various applications with least modifications to the hardware description language (HDL). The Verilog HDL has been used to model the network. Fixed point precision and activation function implementations have been investigated to monitor FPGA resource utilization for different network topologies. The network implemented has been adopted to different applications including handwritten recognition of the MNIST dataset. This work shows that a well modularized network is easily adoptable for different applications hence helping take advantage of the re-configurability of FPGAs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There are many applications such as augmented or mixed reality with limited training data and computing power which results in inapplicability of convolutional neural networks in those domains. In this method, we have extracted the perceptual edge map of the image and grouped its perceptual structure-based edge elements according to gestalt psychology. The connecting points of these groups, called curve partitioning points (CPPs), are descriptive areas of the image and are utilized for image representation. In this method, the global perceptual image features, and local image representation methods are combined to encode the image according to the generated bag of CPPs using the spatial pyramid matching. The experiments on multi-label and single-label datasets show the superiority of the proposed method.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In smart mariculture, an automatic and accurate prediction of key water quality parameters is a significant and challenge issue. This paper focuses on the prediction of pH and water temperature parameters in key water quality parameters. Firstly, the water quality parameters are preprocessed by improved method. Then, the Pearson correlation coefficient method is used to find the correlation between the water quality parameters. Finally, the SRU (Simple Recurrent Unit) deep learning model is used to establish a prediction model for the key water quality parameters, so as to achieve accurate prediction. Meanwhile, we also evaluate the prediction effect of prediction model built by RNN (Recurrent Neural Network) deep learning network. The experimental results show that the proposed prediction method has higher prediction accuracy than the method based on RNN when the time complexity is similar. The proposed method takes 11.3ms to predict every data in average, and the prediction accuracy can reach 98.91%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The smartphone represents a wearable and wireless system with the potential to have transformative influence on the biomedical and healthcare industry. An intrinsic feature of the smartphone is a gyroscope sensor, for which with a software application the smartphone functions as a wearable and wireless gyroscope platform. The resultant gyroscope data recording presents a clinical recognizable signal, which has been successful demonstrated to quantify aspects of human movement characteristics, such as the patellar tendon reflex. Gait another associated feature of human movement can be readily quantified by a smartphone functioning as a wearable and wireless gyroscope platform. The research objective is to distinguish between an affected leg and unaffected leg during hemiplegic gait based on a smartphone functioning as a wearable and wireless gyroscope platform though machine learning classification. A single smartphone is applied to quantify hemiplegic gait. The smartphone is first mounted to the affected leg and then the unaffected leg with velocity constrained to a constant velocity by a treadmill. Through wireless connectivity to the Internet the gyroscope signal data is conveyed as an email attachment for post-processing at a remote location. Software automation consolidates the gyroscope signal data of hemiplegic gait to a feature set for machine learning classification. With the application of a multilayer perceptron neural network considerable classification accuracy is attained for distinguishing between the affected leg and unaffected leg of hemiplegic gait. Future implications of the successful implementation of a smartphone as a wearable and wireless gyroscope for machine learning classification of hemiplegic gait through a multilayer perceptron neural network elucidate pathways to highly optimized therapy through machine learning with the potential for patients to reside remote from their therapist.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the technological advances in the field of multimedia, associated with the generalization of their uses in many applications such as television archiving, motion tracking, video surveillance, etc. Semantic analysis and automatic understanding of large collections of video documents become a major problem. Consequently, the need for a system, which will allow to effectively manipulate video content is undeniable. This paper presents an approach that allows the systematic video analysis using deep learning and ontology generation. The proposed approach permits the extraction and the building of an ontology using results obtained by the deep learning techniques such as key frames, detected objects and actions (movements).", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In Simulation Optimization, the computational cost, due to the huge number of simulation replications along with the optimization process cost, is considered as a key challenge. This encourages the investigation of the effect of the number of simulation replications on the performance of the optimization algorithm, in terms of the solution fidelity versus the computational cost. Existing simulation replication strategies with optimization algorithms have been proposed utilizing a single probability distribution to fit their parameter settings for solving stochastic simulation problems. In this paper, simulation replication strategies are developed by a proposed Strategy Design Technique using three distributions: Bimodal Gaussian, Exponential, and Poisson for involving discrete and continuous stochastic parameters. A Differential Evolution algorithm is hybridized with Monte-Carlo simulation to solve a set of stochastic, continuous and constrained problems, using the designed strategies. Several experiments are conducted, using modified IEEE-CEC'2006 test problems. Optimization results' fidelity is empirically assessed in terms of a proposed probability of correct final selection of the solution. It reflects the ability of the proposed simulation strategies to direct the Differential Evolutionary algorithm in the search space, even under different stochastic settings and low simulation budget. Additional experiments have been conducted to study the compromising between the simulation and the optimization budgets under different settings using the same total computational load. The results indicate that the proposed strategies obtain robust results with a remarkable reduction in the simulation budget, under different stochastic settings.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Gear pitting diagnosis has always been an important research topic and different diagnostic methods are used. This paper uses artificial neural network (ANN) to diagnose early gear pitting. There are several issues that are inevitable with the application of ANN for diagnosis of early gear pitting: feature extraction, neural network structure optimization, and training stability. This paper proposes a new feature extraction method that uses fast Fourier transform (FFT) to select a number of frequencies in the frequency spectrum and use the frequency amplitudes as the inputs of ANN. The particle swarm optimization (PSO) is used to optimize the initial value of the network to make the training more stable. Furthermore, the performance of the ANN diagnosis under different working conditions are also compared and analyzed in the paper. The proposed method has been validated through the data collected from the gear pitting test experiment. The validation results have shown that the faults diagnosis accuracy could reach 100%, which proves that the proposed method is reasonable.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, focusing on the problem of multi-UAVs cooperative reconnaissance mission assignment, we proposed an improved harmony search algorithm which took the total reconnaissance intelligence as the objective function. The algorithm integrated with the mutation and the crossover process as well as the adaptive approach to adjust the parameters, which improved the latter convergence rate and avoided the individual into the local optimum. Simulation results showed that the improved harmony search algorithm can greatly improve the efficiency to solve the problem.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Concept drift is a known phenomenon in software data analytics. It refers to the changes in the data distribution over time. The performance of analytic and prediction models degrades due to the changes in the data over time. To improve prediction performance, most studies propose that the prediction model be updated when concept drift occurs. In this work, we investigate the existence of concept drift and its associated effects on software defect prediction performance. We adopt the strategy of an empirically proven method DDM (Drift Detection Method) and evaluate its statistical significance using the chi-square test with Yates continuity correction. The objective is to empirically determine the concept drift and to calibrate the base model accordingly. The empirical study indicates that the concept drift occurs in software defect datasets, and its existence subsequently degrades the performance of prediction models. Two types of concept drifts (gradual and sudden drifts) were identified using the chi-square test with Yates continuity correction in the software defect datasets studied. We suggest concept drift should be considered by software quality assurance teams when building prediction models.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a nonlinear parallel control algorithm is developed for an electro-hydraulic actuator to achieve high velocity tracking performance and reduce energy consumption. The parallel control system consists of a servo valve and a variable displacement pump. A separate strategy is employed for practical use. The variable displacement pump is considered as feedforward according to velocity commands and leakage; while the servo valve is used to improve the dynamics and accuracy of the closed loop system. Two disturbance observers are employed to deal with the uncertainties and disturbance in the control process. A radial basis function neural network (RBFNN)-based extended disturbance observer is proposed to compensate for the unknown parameters and disturbance in the velocity control loop. To attenuate the influence of model uncertainties and disturbance in the pressure control loop, a nonlinear disturbance observer is adopted. The stability of the whole system is proved through the Lyapunov theory. The comparative experiments are conducted to verify the effectiveness of the proposed nonlinear parallel controller. The results demonstrate that the proposed algorithm can improve the velocity tracking performance for a valve-pump parallel controlled electro-hydraulic actuator under uncertainties and disturbance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: IoT today invented for creating, editing and sharing information and data. Some sources predict the number of tens and hundreds billion connected devices for the year 2020-2025. One of the most popular standard machine-to-machine application layer protocols to interconnect the things and applications to the Internet of Things is Message Queuing Telemetry Transport protocol. Quality of service (QoS) deal with network performance elements like latency, error rate and uptime. MQTT provides three levels of QoS. Usage time of IoT devices is constrained by its most critical resource - battery. This paper deals with the estimation of energy consumption in transferring data using lightweight MQTT protocol over it different QoS levels. For experiments, we implemented client-server architecture and employ MQTT publish/subscribe protocol to transfer data between nodes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The complexity of supply chains increase, especially due to the geographical spread of supplier and customer networks. In the connected and automated supply chains of the industry 4.0, even more nodes are incorporated in supply chains. This paper discusses the possible improvement of process quality in the industry 4.0 through the different blockchain and distributed ledger technologies. We derived hypotheses from a literature review and asked German blockchain experts from the industry to validate and discuss the hypotheses. We find that the different blockchain technologies and consensus algorithms have different strength with regard to quality improvement. One central finding is that IOTA, developed especially for the IoT and deemed the 'next evolutionary step' is scalable and hence may increases the process efficiency, but at the same time is more vulnerable than other blockchain implementations, which again may reduce the overall process quality.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Data mining can be used for data analysis of social media users who visit E-Commerce. This study uses data mining techniques aimed at comparing the classification in sentiment analysis from the views of E-Commerce customers who have been written on Twitter. The data set is derived from tweets about E-Commerce in Tokopedia and Bukalapak. Text mining techniques, transform, tokenize, stem, classification, etc. are used to build classification and analysis of sentiment analysis. Rapidminer is also used to assist in making analysis sentiments for comparison by using three different classifications in the dataset with the Decision Tree, K-NN, and Na\u00efve Bayes Classifier approaches to find the best accuracy. The highest result of this study is the Na\u00efve Bayes approach with an accuracy of 77%, precision 88.50% and recall of 64%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: While many approximate computing methods are quite application-dependent, reducing the size of the data representation used in the computation has a more general applicability. We present a tuning assistant for floating to fixed point optimization (TAFFO), an LLVM-based framework designed to assist programmers in the precision tuning of software. We discuss the framework architecture and we provide guidelines to effectively tradeoff precision to improve the time-to-solution. We evaluate our framework on a well-known approximate computing benchmark suite, AxBench, achieving a speedup on 5 out of 6 benchmarks (up to 366%) with only a limited loss in precision (<3% for all benchmarks). Contrary to most related tools, TAFFO supports both C and C++ programs. It is provided as a plugin for LLVM, a design solution that improves significantly the maintainability of the tool and its ease of use.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To solve the problem of supervised convolutional neural network (CNN) models suffering from limited samples, a two-channel CNN is developed for medical hyperspectral images (MHSI) classification tasks. In the proposed network, one channel of end-to-end network, denoted as EtoE-Net, is designed to realize unsupervised learning, obtaining representative and global fused features with fewer noises, by building pixel-by-pixel mapping between the two source data, i.e., the original MHSI data and its principal component. On the other hand, a simple but efficient CNN is employed to supply local detailed information. The features extracted from different underlying layers of two channels (i.e., EtoE-Net and typical CNN) are concatenated into a vector, which is expected to preserve global and local informations simultaneously. Furthermore, the two-channel deep fusion network, named as EtoE-Fusion, is designed, where the full connection is employed for feature dimensionality reduction. To evaluate the effectiveness of the proposed framework, experiments on two MHSI data sets are implemented, and results confirm the potentiality of the proposed method in MHSI classification.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Long term forecasting of the Loop Current (LC) and its eddies, also called the Loop Current System (LCS) in Gulf of Mexico (GoM) region is crucial for the GoM communities to take adequate preparations to avoid undesired outcomes of this natural phenomena. In this paper, a new approach is developed to forecast the LC and its eddies. The sea level anomaly (SLA) data of the GoM is utilized as observations. The key element of the proposed approach is based on a time series data decomposition strategy, Robust Principal Component Analysis (RPCA). The time components of SLA data obtained by RPCA are fed to a recurrent neural network, the Long Short-Term Memory (LSTM) algorithm, which predicts the timing of eddy separations from the extended LC, and their positions. In the experimental study, observations of sea surface height variations during a period of 23 year were used to train the LSTM network and observations from two additional two years to validate the performance of the prediction model. As shown in the paper, the proposed model can predict the movements of the LCS six weeks in advance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In 6D pose estimation, both template based or learning based models need template/training data corresponding to different poses. We propose a new model based on point clouds. It needs less training data and therefore is lighter, faster and precise enough for texture-less objects. We first extract key points automatically from the point cloud of the object and then generate rigid transformation invariant point-wise features of the cloud as input feature. Then we use a hierarchical neural network architecture to predict the key points coordinates corresponding to the reference pose. At last we can calculate the relative transformation between the current and the reference poses. The hierarchical structure takes into account the symmetry or invariance problem of certain object geometries.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An efficient model for IOT devices using edge Computing in resource heavy or resources Intensive multimedia deal with applications. In today\u2019s world Internet of things has paved a wave for opportunities in remote monitoring and decision making. One such area in surveillance monitoring using cameras, where videos are seize and sent to the cloud or transmitted to remote servers, users and processed .Edge Computing is proposed to be used for such intensive application(human detection, motion analysis, Home or location surveillance)", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Insulation condition monitoring is essential for the safety of our distribution power grid. However, the traditional monitoring solution is hardly applied due to the concerns that arise from natures of distribution grid, such as complex feeder networks, massive operation site, and high economic benefit. In this paper, a wireless sensing network based on Internet of things (IoT) technology is demonstrated for insulation condition perception. First of all, the general framework of the wireless sensing network is designed with considerations on extendibility and accessibility. Next, it goes into detail about the technical requirements of the essential units of the sensing network including sensor node, power management, and data communication. With this concept, two practical cases of IoT-based partial discharge sensing networks realized on the switchgear cabinet and power cable are presented with details involving the design of transient earth voltage and high frequency current sensor nodes, the energy consumption, Lora/NB-IoT-based network structures, and data usage strategies. This IoT-based insulation condition monitoring is proven to be applicable for the distribution grid and expected to be referenced for more possible IoT applications.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Document retrieval (DR) forms an important component in end-to-end question-answering (QA) systems where particular answers are sought for well-formed questions. DR in the QA scenario is also useful by itself even without a more involved natural language processing component to extract exact answers from the retrieved documents. This latter step may simply be done by humans like in traditional search engines granted the retrieved documents contain the answer. In this paper, we take advantage of datasets made available through the BioASQ end-to-end QA shared task series and build an effective biomedical DR system that relies on relevant answer snippets in the BioASQ training datasets. At the core of our approach is a question-answer sentence matching neural network that learns a measure of relevance of a sentence to an input question in the form of a matching score. In addition to this matching score feature, we also exploit two auxiliary features for scoring document relevance: the name of the journal in which a document is published and the presence/absence of semantic relations (subject-predicate-object triples) in a candidate answer sentence connecting entities mentioned in the question. We rerank our baseline sequential dependence model scores using these three additional features weighted via adaptive random research and other learning-to-rank methods. Our full system placed 2nd in the final batch of Phase A (DR) of task B (QA) in BioASQ 2018. Our ablation experiments highlight the significance of the neural matching network component in the full system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Based on the high-order difference co-array concept, an enhanced four-level nested array (E-FL-NA) is first proposed, which optimizes the consecutive lags at the fourth-order difference co-array stage. To simplify sensor location formulations for comprehensive illustration and also convenient structure construction, a simplified and enhanced four-level nested array (SE-FL-NA) is then proposed, whose performance is compromised but still better than the four-level nested array (FL-NA). This simplified structure is further extended to the higher order case with multiple sub-arrays, referred to as simplified and enhanced multiple level nested arrays (SE-ML-NAs), where significantly increased degrees of freedom can be provided and exploited for underdetermined direction of arrival estimation. Simulation results are provided to demonstrate the performance of the proposed E-FL-NA, whereas a higher number of detectable sources is achieved by the SE-ML-NA with a limited number of physical sensors.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Electroencephalogram (EEG) signals exhibit highly irregular patterns. This irregularity, which arises from i.i.d. measurement noise, has been partially resolved by memoryless classifiers, such as deep convolutional neural networks (CNN). However, there are other major sources of irregularity, including brain network modes, mental states, and various physiological factors. These internal states drift over time, in which case it would be better to use memory-based neural networks, such as long short-term memory networks (LSTM). This paper presents a novel EEG signal classification framework that resolves a trade-off between memoryless and memory-based classification. The proposed method uses deep reinforcement learning (RL) to find a trial-by-trial control strategy for the attention control system that switches between CNN (memoryless) and LSTM (memory-based)-or is a mixture of both. The simulation on the EEG dataset, which was collected while performing a complex cognitive task, shows that the proposed attention control system outperforms other EEG classification methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Automatic detection of mandibular canal in cone beam CT data is an essential step for planning and guiding implant surgery. In this work, we present a new detection method based on combining statistical shape models and Lie group. The proposed methodology consists of three steps. Firstly, a method based on multi-scale low rank matrix decomposition is used for noise removal and image enhancement. Subsequently, a Lie group based statistical shape model is constructed to represent shape variation and fast marching is employed to localize the location of the mandibular canal more accurately. Quantitative results show that accurate and fully automatic detection of mandibular canal is feasible. Moreover, the proposed method based on Lie group based statistical shape model outperforms two previous methods based on statistical shape model in the literature, i.e. conventional and conditional statistical shape models. The average value of Dice similarity index and symmetric distance are 0.92 and 1.02 mm, respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a non-linear control technique and an artificial intelligent controller are presented and applied to a single-phase, grid-connected PV system based on the current source inverter (CSI) topology. These techniques are investigated in order to assure a good performance of CSI in term of total harmonic distortion (THD). The design of the non-linear controller is based on the Sliding mode control (SMC) approach to establish the inverter control laws. An analysis based on Lyapunov stability approach is developed to guarantee the global asymptotic stability of the system. The intelligent controller is based on the fuzzy logic controller (FLC) to generate a reference signal. A conventional regulator PI is used in input and output of the FLC to improve the performance. Both control techniques are used to control the injected current and synchronize it with the network. This paper performs a comparative performance evaluation of both approaches. The analytical expectations of the systems are verified using MATLAB/Simulink software environment.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Applications of wireless sensor networks span from the agricultural field to the battlefield and from smart city to smart city health care. There are certain applications where millions of sensor nodes or motes are deployed to sense the environmental parameters. However, in certain applications like health care and smart city applications, few hundreds of nodes can solve the purpose. Routing in energy-constrained networks like WSN is a challenging task. Parameters like consumed energy, congestion and signal to noise ratio etc. are considered to find a routing path in WSN. The Survival Path Routing (SPR) protocol is an earlier version of the work proposed here. The said protocol fails to provide a better result when the number of nodes in the network is increased. Thus a Scalable Survival Path Routing (SSPR) protocol is proposed where the method of clustering is applied to meet the efficiency of a scalable network. Weights of the nodes are computed to choose the cluster heads among them. It is observed that the proposed protocol SSPR outperforms SPR and Directed Diffusion protocol in terms of packet delivery ratio, end to end delay and throughput.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Simulation of neural networks is a significant task for contemporary artificial intelligence research. Despite the availability of modern processing hardware, the task is still too demanding to be done in a sequential way. Therefore, a parallel computation approach is almost always necessary. Modern graphical accelerators (GPUs) represent highly parallel machines with a significant computational performance that can be unleashed only under certain conditions including threads scheduling, proper sources occupation, aligned data access, communication management, etc. We have proposed a novel acceleration approach for large neural networks. It is using a GPU and incorporating biologically highly precise spiking neurons that can imitate real biological neurons. The simulator can be, for example, used for research of communication dynamics of large neural networks with tens of thousands of spiking neurons.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A reduced-cost gradient search with numerical derivatives for design optimization of antenna structures is presented. Our approach involves management of the antenna response Jacobians based on relative relocation of the design during subsequent algorithm iterations, as well as the size of the trust region (i.e., optimization domain at current iteration). Consistent computational savings are demonstrated (40 percent on average as compared to the reference algorithm) for a set of benchmark antennas, along the algorithm robustness.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unmanned aerial vehicles (UAVs) may be used for providing seamless network coverage in urban areas for improving the performance of conventional cellular networks. Given the predominantly line-of-sight channel of drones, UAV-aided seamless coverage becomes particularly beneficial in case of emergency situations. However, a single UAV having a limited cruising capability is unable to provide seamless long-term coverage, multiple drones relying on sophisticated recharging and reshuffling schemes are necessary. In this context, both the positioning and the flight strategy directly affect the efficiency of the system. Hence, we first introduce a novel UAV energy consumption model, based on which an energy-efficiency-based objective function is derived. Second, we propose an energy-efficient rechargeable UAV deployment strategy optimized under a seamless coverage constraint. Explicitly, a two-stage joint optimization algorithm is conceived for solving both the optimal UAV deployment and the cyclic UAV recharging and reshuffling strategy. Our simulation results quantify the efficiency of our proposed algorithm.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Autonomic optical transmission and networking requires machine learning (ML) models to be trained with large datasets. However, the availability of enough real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously being deployed in the network. One option is to generate data from simulations and lab experiments, but such data could not cover the whole features space and would translate into inaccuracies in the ML models. In this paper, we propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The dataset for ML training can be initially populated based on the results from simulations and lab experiments. Once ML models are generated, ML retraining can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits of the proposed learning cycle for general use cases. In addition, two specific use cases are proposed and demonstrated that implement different learning strategies: (i) a two-phase strategy performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment (the accuracy of this strategy is shown for a use case of failure detection and identification), and (ii) in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where results show the significant benefits of collective learning.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The reliability and lifetime determine the levelized cost of energy (LOCE) of photovoltaic (PV) modules and effectiveness of PV system. Although this theme has attracted researchers attention in recent years, there still lack an effective method to model PV modules power degradation. Therefore, this paper put forward to adopt gamma process to establish the relationship between PV modules power degradation, and temperature, relative humidity (RH). And then PV modules service lifetime is predicted under accelerated damp-heat conditions. Firstly, accelerated damp-heat tests are carried out on three different temperature and RH levels. Based on Peck model, a data transformed method is proposed to obtain more power degradation data under other seven damp-heat conditions. Secondly, gamma process with an exponential transformation is applied to model PV modules power degradation under accelerated damp-heat conditions. The relationship between power degradation and temperature, RH is established by theoretical derivation and validated by experimental data. Then Expectation Maximum (EM) algorithm is proposed to estimate model's parameters. Finally, PV modules lifetime under several different damp-heat conditions is predicted. It is found that PV modules lifetime is approximate 20 to 25 years under (50\u00b0/45% RH) condition. But we also conclude that PV modules lifetime sharply decreases as the increment of temperature and RH. Hence, more factors or other test types are recommended to be considered in later accelerated tests.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Within this work, we present an approach to model the opinion of a human towards a specific topic in a fine-grained way by using weighted bipolar argumentation graphs. In addition, we discuss how the therefore required rating of related aspects can be collected by means of emotion recognition techniques and discuss an application scenario based on the state-of-the-art Argumentative Dialogue System EVA in which the proposed techniques can be applied.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Within the context of trajectory planning for autonomous vehicles this paper proposes methods for efficient encoding of motion primitives in neural networks on top of model-based and gradient-free reinforcement learning. It is distinguished between 5 core aspects: system model, network architecture, training algorithm, training tasks selection and hardware/software implementation. For the system model, a kinematic (3-states-2-controls) and a dynamic (16-states-2-controls) vehicle model are compared. For the network architecture, 3 feedforward structures are compared including weighted skip connections. For the training algorithm, virtual velocity constraints and network scheduling are proposed. For the training tasks, different feature vector selections are discussed. For the implementation, aspects of gradient-free learning using 1 GPU and the handling of perturbation noise therefore are discussed. The effects of proposed methods are illustrated in experiments encoding up to 14625 motion primitives. The capabilities of tiny neural networks with as few as 10 scalar parameters when scheduled on vehicle velocity are emphasized.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Blockchain has been widely recognized as the trust machine underpinning the Internet of Things(IoT). However, the poor performance of the existing mainstream platforms renders this expectation unattainable. New technologies like DAG and Hashgraph emerge as promising candidates to address the pressing issues, but their abilities in decentralized consensus are still in question. We propose a 3D-DAG model to address the trilemma of decentralization, scalability and security. Experiment results show that, by adopting the separation of concerns(SOC) architectural principle and a software defined chain(SDC) approach, 3D-DAG can meet challenging requirements expected by IoT applications.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This manuscript discusses the changing roles of Semiconductors for the development of Autonomous Driving within the scope of Vehicle IoT and Deep Learning. Also the development of Autonomous Driving will change the outlook of transportation industries with shorter product life cycle and different modes of business profitability. Along with these changes of technologies and market, the semiconductor business model will also require changes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a new methodology to extract the unknown parameters of a single-diode photovoltaic (PV) cell model. The first contribution of this paper is the development and implementation of a new version of the wind-driven optimization algorithm, called an adaptive wind-driven optimization (AWDO) algorithm. The advantages of the AWDO algorithm are: 1) accurate extraction of the global values of the optimized PV parameters in changing weather conditions, which is achieved by building solutions from random operations; and 2) capability of handling the given complex multi-modal and multi-dimensional optimization problems. The second contribution is the identification of a generalization model to generalize the extracted parameters of a single-diode PV cell model. That provides an ability of the proposed methodology to work with any I-V characteristic curve of PV cells and at any weather condition on a 15-min basis. To validate the proposed methodology, it has been tested for 1307 I-V characteristic curves of a PV module at various weather conditions on a 15-min basis. Additionally, its accuracy and computational efficiency are verified and compared with five well-known existing extraction methods: Villalva's model, particle swarm optimization, biogeography-based optimization, Gang's model, and bacterial foraging optimization by both simulation and outdoor measurements. The results show that the AWDO algorithm can provide the extracted five parameters with an acceptable range of accuracy and faster than the aforementioned models. Therefore, the proposed methodology (AWDO based on Chenlo's model) can be confidently recommended as a reliable, feasible, valuable, and fast optimization algorithm for parameter extraction of a single-diode PV cell model.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the development of Internet of Things (IoT) and blockchain technologies, people find more and more blockchain applications in the IoT domain. While it is reasonable that IoT systems use hierarchical network structures, their sheer large scales may lead to hundreds or even thousands of non-leaf nodes, which may serve as full nodes when participating in IoT blockchains. From IoT blockchain design perspective, it is important to understand the scalability of the energy consumption feature of IoT blockchains. In our research we have collected real-world data that reflect the energy consumption features of several consensus algorithms of blockchain. In this work-in-progress paper, we report our results based on linear regression models. These models provide reference estimations of the energy consumption impact in designing blockchains for IoT systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Vehicular ad-hoc networks (VANETs) have recently attracted a lot of attention due to their immense potentials and applications. Wide range of coverage and accessibility to end users make VANETs a good target for commercial companies. In this paper, we consider a scenario in which advertising companies aim to disseminate their advertisements in different areas of a city by utilizing VANETs infrastructure. These companies compete for renting the VANETs infrastructure to spread their advertisements. We partition the city map into different blocks, and consider a manager for all the blocks who is in charge of splitting the time between interested advertising companies. Each advertising company (AdC) is charged proportional to the allocated time. In order to find the best time splitting between AdCs, we propose a Stackelberg game scheme in which the block manager assigns the companies to the blocks and imposes the renting prices to different companies in order to maximize its own profit. Based on this, AdCs request the amount of time they desire to rent the infrastructure in order to maximize their utilities. To obtain the Stackelberg equilibrium of the game, a mixed integer nonlinear optimization problem is solved using the proposed optimal and sub-optimal algorithms. The simulation results demonstrate that the sub-optimal algorithm approaches the optimal one in performance with lower complexity.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We provide a novel framework for the synthesis of a controller for a robot with a surveillance objective, that is, the robot is required to maintain knowledge of the location of a moving, possibly adversarial target. We formulate this problem as a one-sided partial-information game in which the winning condition for the agent is specified as a temporal logic formula. The specification formalizes the surveillance requirement given by the user by quantifying and reasoning over the agent's beliefs about a target's location. We also incorporate additional non-surveillance tasks. In order to synthesize a surveillance strategy that meets the specification, we transform the partial-information game into a perfect-information one, using abstraction to mitigate the exponential blow-up typically incurred by such transformations. This transformation enables the use of off-the-shelf tools for reactive synthesis. We evaluate the proposed method on two case-studies, demonstrating its applicability to diverse surveillance requirements.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep learning techniques have shown promising outcomes in single image super-resolution (SR) reconstruction from noisy and blurry low resolution data. The SR reconstruction can cater the fundamental limitations of transmission electron microscopy (TEM) imaging to potentially attain a balance among the trade-offs like imaging-speed, spatial/temporal resolution, and dose/exposure-time, which is often difficult to achieve simultaneously otherwise. In this work, we present a convolutional neural network (CNN) model, utilizing both local and global skip connections, aiming for 4\u00d7 SR reconstruction of TEM images. We used exact image pairs of a calibration grid to generate our training and independent testing datasets. The results are compared and discussed using models trained on synthetic (downsampled) and real data from the calibration grid. We also compare the variants of the proposed network with well-known classical interpolations techniques. Finally, we investigate the domain adaptation capacity of the CNN-based model by testing it on TEM images of a cilia sample, having different image characteristics as compared to the calibration-grid.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The depletion of fossil fuel energy resources, increasing concern of air pollution, global warming and energy crisis have encouraged the research to develop innovative alternatives in engineering systems. This work presents a problem of minimizing the life cycle cost (as an economic criterion) of an air conditioning system for vapor compression and maximizing the useful heat (as a thermodynamic criterion). The mathematical model that represents the objective function has four decision variables: evaporation temperature and condensation temperature of the refrigerant, and volumetric flow rate of air in the condenser and the evaporator. The problem is a global optimization and the search method of solution is genetic algorithm. A thermodynamic model included design of exchanger. The methodology used is for R-1234yf, a low GWP refrigerant, which is R-134a refrigerant substitute.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional neural networks (CNNs) are widely used and have achieved great success in computer vision and speech processing applications. However, deploying the large-scale CNN model in the embedded system is subject to the constraints of computation and memory. An optimized block-floating-point (BFP) arithmetic is adopted in our accelerator for efficient inference of deep neural networks in this paper. The feature maps and model parameters are represented in 16-bit and 8-bit formats, respectively, in the off-chip memory, which can reduce memory and off-chip bandwidth requirements by 50% and 75% compared to the 32-bit FP counterpart. The proposed 8-bit BFP arithmetic with optimized rounding and shifting-operation-based quantization schemes improves the energy and hardware efficiency by three times. One CNN model can be deployed in our accelerator without retraining at the cost of an accuracy loss of not more than 0.12%. The proposed reconfigurable accelerator with three parallelism dimensions, ping-pong off-chip DDR3 memory access, and an optimized on-chip buffer group is implemented on the Xilinx VC709 evaluation board. Our accelerator achieves a performance of 760.83 GOP/s and 82.88 GOP/s/W under a 200-MHz working frequency, significantly outperforming previous accelerators.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Pattern recognition algorithms have recently been pursued for automatic analysis of delayered IC images, i.e. the detection of circuit components. Wide experimentation on the existing training-based approaches are hampered by heavy data labeling, expensive model training, or long processing time. In this paper, we propose a global template projection and matching (GTPM) method that requires no training and a minimal amount of data labeling for circuit component detection. Our proposed GTPM method achieves a higher or comparable accuracy as the reported approaches while being more computationally efficient.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Throughout the last few decades, a breakthrough took place in the field of autonomous robotics. They have been introduced to perform dangerous, dirty, difficult, and dull tasks, to serve the community. They have been also used to address health-care related tasks, such as enhancing the surgical skills of the surgeons and enabling surgeries in remote areas. This may help to perform operations in remote areas efficiently and in timely manner, with or without human intervention. One of the main advantages is that robots are not affected with human-related problems such as: fatigue or momentary lapses of attention. Thus, they can perform repeated and tedious operations. In this paper, we propose a framework to establish trust in autonomous medical robots based on mutual understanding and transparency in decision making.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Falls are one of the reasons why older people need to receive care. Walking training is effective for preventing older people from falling. Accordingly, various research related to walking training has been conducted. We aimed to introduce robots in walking training of older people, and we developed a walking-promotion robot. In this paper, we designed motions for promoting walking by older people and evaluated the effects on the participants motivation to walk and the impression of the robot conveyed by the robots motions. From the experimental results, we confirmed that a robot did affect the participants motivation to walk.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Early childhood is a significant period when transitions take place in children. This period is a hot topic among researchers who pursue this domain across different scientific disciplines. Many studies addressed social, scientific, medical, and technical topics during early childhood. Researchers also utilized different analysis measures to conduct experiments on the different types of data related to the early childhood to produce research articles. This paper aims to review and analyze the literature related to early childhood in addition to the data analyses and the types of data used. The factors that were considered to boost the understanding of contextual aspects in the published studies related to early childhood were considered as open challenges, motivations, and recommendations of researchers who aimed to advance the study in this area of science. We systematically searched articles on topics related to early childhood, the data analysis approaches used, and the types of data applied. The search was conducted on five major databases, namely, ScienceDirect, Scopus, Web of Science, IEEE Xplore, and PubMed from 2013 to September 2017. These indices were considered sufficiently extensive and reliable to cover our field of the literature. Articles were selected on the basis of our inclusion and exclusion criteria (n = 233). The first portion of studies (n = 103/233) focused on the different aspects related to the development of children in early age. They discussed different topics, such as the body growth development of children, psychology, skills, and other related topics that overlap between two or more of the previous topics or do not fall into any of the categories but are still under development. The second portion of studies (n = 107/233) focused on different aspects associated with health in early childhood. A number of topics were discussed in this regard, such as those related to family health, medical procedures, interventions, and risk that address the health-related aspects, in addition to other related topics that overlap between two or more of the previous topics or do not fall into any of the categories but are still under health. The remaining studies (n = 23/233) were categorized to the other main category because they overlap between the previous two major categories, namely, development and health, or they do not fall into any of the previous main categories. Early childhood is a sensitive period in every child's life. This period was studied using different means of data analysis and with the aid of different data types to produce different findings from the previous studies. Research areas on early childhood vary, but they are equally significant. This paper emphasizes the current standpoint and opportunities for research in this area and boosts additional efforts toward the understanding of this research field.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There are two fundamental purposes in mobile edge computing, i.e., performance enhancement and cost reduction. By offloading computation tasks to a mobile edge cloud (MEC), a user equipment (UE), also called mobile user, mobile subscriber, or mobile device, can possibly reduce its average response time, which is the main performance measure, and can possibly reduce its average power consumption. Optimizing both performance and cost may be conflicting requirements. In this paper, we optimize the cost-performance ratio (CPR), i.e., the power-time product, which combines performance (average response time) and cost (average power consumption) into one quantity. A unique feature in mobile edge computing is the competitiveness of mobile users, who are selfish in competing for resources in a mobile edge cloud. We take a game theoretic approach to the stabilization of a competitive mobile edge computing environment. The main contributions of the paper are summarized as follows. 1) We consider a mobile edge computing environment with multiple UEs and a single MEC. We establish an M/G/1 queueing model for the UEs and an M/G/m queueing model for the MEC. The UEs are entirely heterogeneous in terms of task characteristics, computation and communication speeds, and power consumption models for both computation and communication. 2) We analytically derive the average response time and the average power consumption of each UE and the MEC, so that cost-performance ratio optimization can be studied mathematically and rigorously. 3) We establish a non-cooperative game framework to systematically study the stabilization of a competitive mobile edge computing environment. Our framework includes a set of seven non-cooperative games among the UEs and the MEC, each attempts to minimize its payoff function, i.e., its cost-performance ratio. These games are different in terms of the number of variables to play and which variables to play. 4) We develop efficient algorithms for each player to find the best response in each game. All these algorithms are the poly-log time in the length of an initial search interval and the accuracy requirement. We also develop an iterative algorithm to find the Nash equilibrium of the games. 5) We demonstrate the numerical examples of our algorithms and performance data of our games for the idle-speed model and the constant-speed model respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Beamforming techniques play an important role in multi-antenna communication systems and this work focuses on the downlink power minimization problem under a set of quality of service constraints. Conventional iterative algorithms can obtain optimal solutions but at the cost of high computational delay. Fast beamforming can be achieved by leveraging the powerful deep learning techniques. In this work, we propose a beamforming neural network (BNN), based on convolutional neural networks and exploitation of expert knowledge, for the power minimization problem. Instead of estimating beamforming matrix directly, we predict key features using the BNN which takes complex channel as input. Then the beamforming matrix is recovered from the predictions according to the uplink-downlink duality. The BNN adopts the supervised learning method with a loss function based on the mean-squared error metric to update network parameters. Simulation results show the BNN can achieve satisfactory performance with low computational delay.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Product Quality Accident (PQA) is a kind of accident mainly caused by product quality defects originated in design and production. To decrease the number and the severity of PQA in usage is a routine task of quality and reliability engineer. Especially with the advent of the era of intelligent manufacturing and big data, the functional structure of product is becoming increasingly complicated and the dimension of big data in product lifecycle is high. As a result, it is difficult to artificially identify the root cause of PQA by traditional method. Therefore, this paper proposes a novel method based on an advanced data mining algorithm for root cause identification. Firstly, the quality accident formation mechanism is introduced in detail. Secondly, with the aid of the domain mapping theory, PQA relevance tree is contributed. Thirdly, with the setting of reasonable support and confidence, the FP-Growth algorithm is used to further mine the relevance rules and construct a complete PQA relevance tree, which could reduce the ambiguity of PQA root cause identification. Finally, the effectiveness of the proposed technique is verified by a root cause analysis example of an engine quality accident.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: CoAP (Constrained Application Protocol) enables embedded devices to offer RESTful Web Services and exchange small binary message headers. Next to the transmission of sensor data and control information between smart home devices, there is an application field of streaming data using CoAP. There are CoAP implementations available in Java, that allow an execution on heterogeneous devices and operating systems due to the Java Virtual Machine and Java Runtime Environment. Because of the timing demands of live streaming applications in terms of low latency communication at application layer, we evaluate two different CoAP Java implementations. We compare the timing fluctuations of the packet processing with a CoAP C implementation. As a conclusion we identify the Java Garbage Collector to cause large fluctuations of the packet processing, which results in bad suitability for a live streaming scenario. Furthermore, we recommend the usage of native code applications because of better timing results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Lightning electromagnetic pulse (LEMP) waveforms associated with lightning strikes to the 634-m high Tokyo Skytree are calculated by the finite-difference time-domain (FDTD) method in the two-dimensional (2-D) cylindrical coordinate. The three-dimensional (3-D) tortuous lightning channel and the tall strike object are represented by an \u201cequivalent vertical\u201d phased-current-source array. The current propagation speed along the equivalent vertical lighting channel is determined in two steps: first, the channel is projected on a plane including the strike object and a field observation point, and then it is converted into the straight vertical channel right above the strike object. Thus, the height-dependent lightning-current propagation speed along the equivalent channel depends on the direction of the observation point. The proposed method greatly reduces computational resources required for calculating LEMPs without deteriorating accuracy since it allows the use of the 2-D FDTD method. LEMP waveforms measured at a horizontal distance of 27, 57, and 101 km (in different directions) from the strike object are accurately reproduced by the proposed method. The results clarified that the lower region of the tortuous lightning has a large influence on the wavefront of the LEMPs at a horizontal distance ranging from 20 to 100 km from the strike point.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In Infrastructure-as-a-Service (IaaS) clouds, remote users access provided virtual machines (VMs) via the management server. The management server is managed by cloud operators, but not all the cloud operators are trusted in semi-trusted clouds. They can execute arbitrary management commands to users' VMs and redirect users' commands to malicious VMs, which is called the VM redirection attack. The root cause is that the binding of users to VMs is weak. In other words, it is difficult to enforce the execution of only users' management commands to their VMs. In this paper, we propose UVBond for strongly binding users to their VMs to solve this problem. UVBond boots user's VM by decrypting its encrypted disk inside the trusted hypervisor. Then it issues a VM descriptor to securely identify that VM. To bridge the semantic gap between high-level management commands and low-level hypercalls, UVBond uses hypercall automata, which accept the sequences of hypercalls issued by commands. We have implemented UVBond in Xen and confirmed that a VM descriptor and hypercall automata prevented attacks and that the overhead was not large.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the field of automatic target recognition and tracking, long-term tracking for aerial infrared target has been recently seen with great interest. Although deep trackers and correlation filtering trackers offer competitive results on performance, the problems of deformation, abrupt motion, heavy occlusion, and out of view still remain unsolved. In addition, since this paper focus on infrared images, it is also important to consider that infrared images have a significant drawbacks, such as low resolution, low contrast, and lack of textures. In this paper, we adopt correlation filtering trackers and deep learning detection method to achieve accurate tracking results. Our tracking system composed of three parts: the DTB correlation filtering tracker (DTB-CF), a better regression model to discriminate the target from the background with adjustable Gaussian window functions; the UTA correlation filtering tracker (UTA-CF), an optimum regression model to update the target appearance with simultaneously optimal in position, scale, and integration of multi-feature fusion; and the YOLOv3 re-detector, which ensures re-location of the correct position of the target when the tracking fails. In addition, we introduce the ratio between average peak-to-correlation energy (APCE) of the current frame and average APCE of former frames as a criterion to update the UTA-CF tracker to maintain the target model stability. And we combine the nearest neighbor maximum value method with APCE as criterion together to initialize the YOLOv3 re-detector. We evaluate our algorithm on real aerial infrared target thermal image sequences in terms of precision plot, success plot, and speed. The experimental results show that our method has a significant improvement than the state-of-the-art methods for long-term tracking both in accuracy and robustness for aerial infrared object tracking.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Head detection plays an important role in localizing and identifying persons from visual data. Most existing methods treat head detection as a specific form of object detection. Head detection is nontrivial due to the considerable difficulty in building the local and global information under conditions of unconstrained pose and orientation. To address these issues, this paper presents an effective adaptive relational network to capture context information, which is greatly helpful to suppress missed detection. We show that the fundamental contextual properties, such as the global shape priors from different heads and the local adjacent relationship between the head and shoulders, can be systematically quantified by visual operators. Specifically, we propose a two-step search algorithm to quantify the global intergroup conflict with adaptive scale, pose and viewpoint. Meanwhile, a structured feature module is introduced to capture the local relation of intraindividual stability. Finally, the global priors and local relation are integrated seamlessly into a single-stage head detector that is end-to-end trainable. An extensive ablation analysis demonstrates the effectiveness of our approach. We achieve state-of-the-art results on two challenging datasets, i.e., HollywoodHeads and Brainwash.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a novel context-aware fog computing framework for intelligent transportation systems (ITS) called CFC-ITS. This scheme consists of multiple intelligent tiers: Internet of Things tier, fog service tier, and global cloud service tier supporting edge analytics for ITS services in a connected car environment. Each tier on the fog senses different contexts like location, time, available resources, and estimated response time for efficiently processing tasks to provide delay-sensitive services while optimizing virtualized resources. A preliminary prototype and a testbed for this study were built to validate the robustness of the proposed approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The hierarchical routing protocol in the wireless sensor network (WSN) is widely applied because of its good network stability and effective communication capability. As the earliest hierarchical routing protocol, Low-Energy Adaptive Clustering Hierarchy(LEACH) had low energy efficiency without considering the state of nodes. In this paper, we propose an energy efficient LEACH-based protocol to improve LEACH. The state of the cluster heads in the previous rounds, as well as residual energy and alive nodes density, are used as indicators to measure the current round cluster head selection. The modified protocol fixes the issue that the number of cluster heads dropped rapidly with dead nodes increase. The simulation results show that the proposed protocol can prolong the network lifetime of WSN effectively, and reduce the energy consumption with the balancing energy distribution.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This work considers the estimation of the seismic events using a Bayesian filtering method, a multiple model particle filtering (MMPF) framework, in particular, from the noisy seismic records. According to the nonlinearity of the seismic signals, the nonlinear dynamical system can be used for the modelling of the signal. In geological studies, it is very important to accurately estimate the events from the recorded seismic signals, under noisy environment in particular since the signal is usually acquired usually under noisy conditions resulting in the difficulty in information extraction. This work proposes the MMPF, an effective filter in a sequential Monte Carlo method, in cooperated with the seismic wavelet model for the estimation of the seismic events under the noisy environments. To evaluate the performance of the method, a set of synthetic noisy signals with different numbers of events, noise levels, was created and used in the simulations. Results illustrates that the proposed MMPF can provide the excellent seismic event estimates under noisy environment.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a computationally efficient method for performance verification and tuning of model-based controllers in constrained nonlinear systems subject to probabilistic uncertainties with arbitrary distributions. The proposed method is based on an extension of generalized polynomial chaos, an asymptotically convergent spectral method for uncertainty propagation, to directly handle arbitrary uncertainty distributions. The proposed arbitrary polynomial chaos (aPC) method only requires knowledge of the statistical moments of the distribution, and is capable of estimating the aPC expansion coefficients using a minimal number of closed-loop simulations. Advantages of the proposed aPC method are demonstrated on a benchmark continuous reactor problem controlled by a scenario-based nonlinear model predictive controller.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The widespread adoption of technology-enhanced learning in various knowledge disciplines has pushed forward the development of information technology-assisted media for language learning and teaching. However, most of the existing electronic-learning (e-learning) solutions have underexplored and under-addressed given specific characteristics of grammar learning, which is one of the most demanding areas of language education. The lack of pedagogically informed instructional design to enhance learning performance on the current system can result in low motivation and engagement due to an imbalance and excessive increase of the cognitive load. This paper attempts to address these deficiencies posed by the existing systems by proposing smart communication networks that are driven by the student learning experience to manage cognitive load in the context of grammar learning. The e-grammar learning networks serve as a collaborative learning platform that combines a pedagogically informed instructional model named attention, relevance, confidence, and satisfaction (ARCS) and cyber interaction among teaching/learning agents. From the technological perspective, our numerical simulations demonstrate the desirable performance indicators of the proposed networks to facilitate information exchange and learning. From the education perspective, our empirical studies show that the overall smart network-enabled e-grammar learning system has desirable characteristics to motivate learners (m = 3.78) and manage their overall cognitive load (m = 1.73), which suggest the promising capability of the proposed system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This work demonstrates an ongoing effort to employ and explain machine learning model predictions for classifying alerts in Security Operations Centers (SOC). Our ultimate goal is to reduce analyst workload by automating the process of decision making for investigating alerts using the machine learning model in cases where we can completely trust the model. This way, SOC analysts will be able to focus their time and effort to investigate more complex cases of security alerts. To achieve this goal, we developed a system that shows the prediction for an alert and the prediction explanation to security analysts during their daily workflow of investigating individual security alerts. Another part of our system presents the aggregated model analytics to the managers and stakeholders to help them understand the model and decide, on when to trust the model and let the model make the final decision. Using our prediction explanation visualization, security analysts will be able to classify oncoming alerts more efficiently and gain insight into how a machine learning model generates predictions. Our model performance analysis dashboard helps decision makers analyze the model in signature level granularity and gain more insights about the model.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Power charge limit is a colossal smidgen of talented pass confirmation since it impacts sharp shape to expense ordinary, inside the present structures we related an aggregate portrayals selector basically make completely with respect to Gray Correlation Analysis (GCA) to push off the trouble emphasis. Second, a blend of Kernel capacity and Principle Component Analysis (KPCA) is to get a handle in trademark extraction system to control manage look the dimensionality deal. At duplicated staying, to check respect brightness, we activate a differential movement (DE) on a to an outstanding recognition essential stage fundamentally based absolutely it creates the impression that obviously Support Vector Machine (SVM) classifier. Our proposed great cost looking is formed through systems for approach for these three furnished materials. Regardless the by using and monstrous by method for structures for technique for and tremendous execution of those frameworks is a standard affiliation a fundamental association strikingly by and large through and through less. To vanquish the ones dangers, we approach interminable Huge Data green frame for differing bundle flung data satellite television for pc TV for pc TV for PC programming. The proposed plot passes on 3 straightforward gadgets, close to 1) remote sensing Big Data acquisition unit (RSDU); 2) data processing unit (DPU); and 3) data analysis decision unit (DADU). First, RSDU gets estimations from the satellite television for pc TV for PC TV for PC TV for PC TV for PC TV for PC and sends those records to the Base Station, wherein starter directing happens. Second, DPU plays out a crucial confine alive and magnificently to constrain rebuilding of right 'ol basic time Big Data through utilizing exhibiting filtration, stack creating over, and parallel masterminding. Third, DADU is the better layer unit of the proposed set up; its miles committed for social event, setting without end of the outcomes, and certificate of affinity layout really in fragile of the outcomes had been given from DPU. The proposed making outlines has the assistance of allocating, stack changing, and parallel cure of enormously astonishing solid bits of information. Along those strains, it results in skilfully endeavoring to discover into pushing a couple of bit flung seeing Big Data the use of earth observatory device.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: HTTP adaptive streaming (HAS) is the state-of-the-art technology for improving the quality of user experience under conditions of time-varying available bandwidth. Developing the bitrate adaptation algorithm becomes more challenging with the transition to 4G cellular networks. The features of bandwidth variation caused by changes in radio channel quality are significantly different from the pattern caused by changes in radio channel resources. In this paper, we propose a bitrate adaptation algorithm for 4G cellular networks with bandwidth variation pattern differentiation. By investigating the bandwidth data profiles collected from an actual 4G cellular network with the field test approach, we distinguish the pattern of bandwidth capacity variations as sustained fluctuations and instantaneous hopping. With bandwidth variation pattern differentiation, the proposed algorithm performs a smoothed bitrate adaptation to sustained fluctuations and an instant bitrate adaptation to instantaneous hopping. Performance evaluations obtained on a 4G cellular network testbed demonstrate that the algorithm achieves reductions in bitrate switching frequency and playback stalls while increasing the average bitrate.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The difference of multi-label classification from traditional classification is that an instance may associate a set of labels simultaneously. In recent study, some scholars have proposed that the information which derives from the query instance's nearest neighbors, can be useful when predicting the labels of the query instance. On the basis of their research, we propose a new approach to multi-label classification, which employs neural tensor network (NTN) to explore the relations among the labels of neighbors and classify the query instance with these correlations. This method utilizes the correlations and interdependencies between labels and leverages the potential of data. Experiments on real data show that our method can achieve good performance in multi-label classification.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: State-of-health (SOH) estimation is critical for the battery management system. With autonomous ability and superior nonlinear mapping capability, machine learning is now a hot topic in this field. The training sample set of current machine learning methods based on single learner is often the regional data, which leads to a small range of data acquisition and affects the generalization ability of the model. Regard this issue, the idea of ensemble learning is considered, and by generating differential data samples and synthesizing the output of a series of base learners, a good learning performance can be achieved. Furthermore, gray relational analysis is used for feature correlation analysis. In this paper, NASA battery data sets are used to verify and validate the performance of the proposed method, which indicated an enhanced accuracy of the results based on the ensemble learning method. The proposed battery healthy assessment model based on ensemble learning can be concluded to provide highly accurate and stable SOH predictions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Drive test has been the method to evaluate the performance of the automotive antennas in urban and suburban/ rural environments. Virtual drive test (VDT) using high-frequency electromagnetic and wave-propagation simulation software can be used instead to evaluate vehicular antennas performance. This paper presents a simulation-based method for evaluating LTE automotive antenna installed on a roof of a vehicle in an urban environment with and without surrounding traffic. A full-wave method; namely, the Multi-Level Fast Multipole Method (MLFMM) is used to perform the analysis of the antenna placed on a vehicle. Whereas Ray-optical Intelligent ray tracing model (IRT) is used to evaluate the installed antenna performance, in here received power, in a non-stationary environment. The received power of the vehicle antenna with and without traffic was found to minorly disagree in a complex urban environment.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Passwords form part of our daily routine and even though there are alternative authentication mechanisms, such as biometrics, passwords stubbornly persist. Passwords have been around for the last few decades, but also the various problems associated with users trying to create passwords that are strong and secure. Users are faced with a cognitive burden in managing passwords which often leads to poor password practices or users recycling passwords across various accounts. While there is no anticipated end to the use of passwords, scholars have identified that passwords need to be better supported - one such method is using a password manager. There is a wealth of technical research relating to password managers, which has led to drastic improvements and the maturing of the technology. However, there is a little research on why people would choose to adopt password managers. To explore these factors, this research uses an adapted version of the Unified Theory of Acceptance and Use of Technology (UTAUT2) that includes trust as an additional construct. Using empirical data, the results of the study show that performance expectancy, habit, and trust are key factors in the intention to adopt a password manager.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper we describe a falls detection and classification algorithm for discriminating falls from daily life activities using a MEMS accelerometer. The algorithm is based on a shallow Neural Network with three hidden layers, used as fall/non fally classifier, trained with daily life activities features and fall features. The novelty of this algorithm is that synthetic falls are generated as multivariate random Gaussian features, so only real daily life features must be collected during some day of normal living. Moreover, the features related to synthetic fall events are generated as complement of normal features. First of all, the features acquired during daily life are clustered by Principal Component Analysis and no Fall activities shall be recorded. The complement set of the normal features is found and used as a mask for Monte Carlo generation of synthetic fall. The two feature sets, namely the features recorded from daily life activities and those artificially generated are used to train the Neural Network. This approach is suitable for a practical utilization of a Neural Network based fall detection characterized by high Recall-Precision rate.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Registration is an important task in automated medical image analysis. Although deep learning (DL) based image registration methods out perform time consuming conventional approaches, they are heavily dependent on training data and do not generalize well for new images types. We present a DL based approach that can register an image pair which is different from the training images. This is achieved by training generative adversarial networks (GANs) in combination with segmentation information and transfer learning. Experiments on chest Xray and brain MR images show that our method gives better registration performance over conventional methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The large pose variations and misalignment errors exhibited by person images significantly increase the difficulty of person Re-Identification (ReID). Existing works commonly apply extra operations like pose estimation, part segmentation, etc., to alleviate those issues and improve the robustness of pedestrian representations. While boosting the ReID accuracy, those operations introduce considerable computational overheads and make the deep models complex and hard to tune. To chase a more efficient solution, we propose a Part-Guided Representation (PGR) composed of Pose Invariant Feature (PIF) and Local Descriptive Feature (LDF), respectively. We call PGR \u201cPart-Guided\u201d because it is trained and supervised by local part cues. Specifically, PIF approximates a pose invariant representation inferred by pose estimation and pose normalization. LDF focuses on discriminative body parts by approximating a representation learned with body region segmentation. In this way, extra pose extraction is only introduced during the training stage to supervise the learning of PGR, but is not required during the testing stage for feature extraction. Extensive comparisons with recent works on five widely used datasets demonstrate the competitive accuracy and efficiency of PGR.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Remote sensing is often a crucial part of cyber-physical and networked systems. Measurements taken at remote locations can both serve as monitoring tools as well as to inform control decisions. Over the past few years, a number of cyberattacks were mounted against such systems with severe socio-economic consequences. In this paper we explore how physical measurements can be used in combination with a consensus algorithm, to ensure secure state estimation for a class of tree-like networked systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Due to the variety of financial markers, to determine an appropriate timing for buying or selling stocks is always a difficult task, the common way to handle it is using trading strategies formed by technical or fundamental indicators. To deal with the problem, an approach was proposed for optimizing a group trading strategy portfolio in the previous approach. To avoid unpredictable loss, the stop-loss and take-profit points are commonly used by investors to handle it. However, they are not easy determined by users when different trading strategies are employed. In this paper, attempting to provide a more useful group trading strategy portfolio, we propose an algorithm for obtaining a group trading strategy portfolio and its stop-loss and take-profit points using the grouping genetic algorithm. Experiments were conducted on a real dataset to reveal the effectiveness of the proposed approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Telecommunication is a term for distance communication which is any communication that is not face to face. Distance communication is important to people with communication disabilities because it provides opportunities for more independence, confidence, engagement in the community, self advocacy, self determination, emergency preparedness, social connections, and employment. Distance communication has many forms such as Telephone Relay Services, Video Relay Services, Free phones, or AAC speech generating devices etc. In Thailand Thai Minspeak\u00ae system with 144-location overlay was firstly created and installed on Window 8 Tablet Samsung so as to assist those people to communicate with other people in their daily in the vicinity. However, with the limitation of the tablet, an app was created and installed on a smart phone. The tablet and a smart phone are connected and worked together for calling and receiving functions via the aux cable specially created. The whole system (Thai Minspeak\u00ae system on the tablet, the app on the smart phone, and the aux cable) enables people with communication disabilities in Thailand to communicate with each other or with other people not in the vicinity but also over a distance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We address the pickup and delivery problem by exploring alternative modeling approaches to assess service quality and to effectively manage the scheduling and allocation of transportation requests by implementing a software simulation of a system of smart cooperative autonomous agents. Our problem definition was derived from a real-life context within our campus wherein golf carts are used to transport people. Given that the complexity of the pickup and delivery problem is NP-hard, our prime strategy to address this complexity is the design a decentralized architecture to distribute decision-making over components of the system and to develop a set of heuristics to find optimal solutions without an exhaustive search. The decentralized model is implemented as a set cooperating autonomous components, each of which has computing power and decisionmaking autonomy, while contributing cooperatively to derive an optimal solution. The game simulation is used to generate descriptive data that capture pickup and delivery scenarios. Analysis of experimental data is used to explore various scenarios and to perform effective resource planning and allocation. The simulation system is available on various platforms (pc, laptop, smart devices).", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The cost-effective management of spare parts is an important objective for all manufacturing and service companies. One of the most difficult challenges, for this objective, is accurate demand forecasting and optimized supply planning decisions to achieve best availability level for the spare parts. The main objective of this paper is to propose a predictive approach to identify the best forecasting method with least error cost. Moreover, in business aircraft industry the best forecasting method for a part can change due to the high-level uncertainty in demand. To this purpose, a methodology to select the best forecasting method based on binary classifier machine learning is developed. Proposed methodology is applied in a real case for a well-known business aircraft. The results indicate that neural network is the best machine learning method for 98% of demand and random forest is the best machine learning method for only 2% of parts.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In GitHub, integrators inspect submitted code changes, make evaluation decision, and close pull requests. However, some pull requests may get reopened for further modification and code review. It is important to predict reopened pull requests immediately after pull requests' first close, and help integrators reopen pull requests in time. If pull requests are reopened a long time after their close, they may cause conflicts with newly submitted pull requests, add software maintenance cost, and increase burden for already busy developers. To the best of our knowledge, we present the first look at predicting reopened pull requests in GitHub. We propose an approach DTPre which is an automatic predictor of reopened pull requests based on Decision Tree classifier. DTPre mainly analyzes code features of modified changes, review features during evaluation, and developer feature of contributors. We evaluate the effectiveness of DTPre on 7 Open Source projects containing 100,622 pull requests. Experimental results show that DTPre has high performances by achieving a precision of 95.53%, recall of 99.01% and F1-measure of 97.23% on average. In comparison with predictors based on neural network, na\u00efve Bayes, logistic regression and SVM, DTPre based on decision tree improves F-1 measures by 41.76%, 59.45%, 42.25% and 9.98% on average across 7 projects.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Identifying potential abuses of human rights through imagery is a novel and challenging task in the field of computer vision, which will enable to expose human rights violations over large-scale data that may otherwise be impossible. While standard databases for object and scene categorization contain hundreds of different classes, the largest available dataset of human rights violations contains only four classes. Here, we introduce the human rights archive (HRA) database, a verified-by-experts repository of 3050 human rights violations photographs, labeled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. With the HRA dataset and a two-phase transfer learning scheme, we fine-tuned the state-of-the-art deep convolutional neural networks (CNNs) to provide human rights violations classification CNNs. We also present extensive experiments refined to evaluate how well object-centric and scene-centric CNN features can be combined for the task of recognizing human rights abuses. With this, we show that the HRA database poses a challenge at a higher level for the well-studied representation learning methods and provide a benchmark in the task of human rights violations recognition in visual context. We expect that this dataset can help to open up new horizons on creating systems that are able to recognize rich information about human rights violations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a new framework for pulmonary nodule diagnosis using radiomic features extracted from a single computed tomography (CT) scan. The proposed framework integrates appearance and shape features to get a precise diagnosis for the extracted lung nodules. The appearance features are modeled using 3D Histogram of Oriented Gradient (HOG) and higher-order Markov Gibbs random field (MGRF) model because of their ability to describe the spatial non-uniformity in the texture of the nodule regardless of its size. The shape features are modeled using Spherical Harmonic expansion and some basic geometric features in order to have a full description of the shape complexity of the nodules. Finally, all the modeled features are fused and fed to a stacked autoencoder to differentiate between the malignant and benign nodules. Our framework is evaluated using 727 nodules which are selected from the Lung Image Database Consortium (LIDC) dataset, and achieved classification accuracy, sensitivity, and specificity of 93.12%, 92.47%, and 93.60% respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Contents posted on social media have attracted attention as a means to enhance the development of tourism, because there are huge amounts of information that are for the organizers of tourist spots and events. In this study, we focus on tweets posted by tourists including descriptions related to tourist spots and events. These kinds of tweets are called sightseeing tweets. To extract useful opinions from the sightseeing tweets, we define classes into which they are classified of them. These classified sightseeing tweets are used for analyzing opinions and polarities related to tourist spots and events. In addition, we propose a new model with convolutional neural networks to classify tweets. Distributed representation is one of the most well-known approaches to convert text data into numeric vectors so that text data can be input to neural networks. Our model utilizes multi-channel distributed representation, which is a hybrid representation of the word representation for text data. To evaluate the proposed model, we conducted experiments using actual sightseeing tweets. The experimental results show that the proposed model outperforms other deep-learning-based models.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To determine the optimal trajectory for the dive phase of a hypersonic missile, a maneuver strategy using inverted flight is presented. The combat scene that the hypersonic missile attacks the fixed target on the ground is considered in this paper. Particularly, a maneuvering form named inverted flight is first applied to the hypersonic missiles. Afterward, an optimal trajectory is designed by minimizing the attack time with a preset terminal flight path angle, where the constraints of the angle of attack, dynamic pressure, heating transfer rate, and normal overload are taken into account. In order to solve the designed trajectory optimization problem, an improved hp-adaptive pseudospectral method with mesh size reduction is presented. The simulation results show that the proposed algorithm can significantly reduce the mesh scale with satisfactory accuracy and the trajectory obtained by the proposed algorithm is in accordance with the actual flight law. Furthermore, contrast simulation demonstrates that the inverted flight has better trajectory performance than normal flight.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a new channel estimation algorithm is proposed that exploits channel sparsity in the time domain for an orthogonal frequency division multiplexing (OFDM)-based underwater acoustical (UWA) communications systems in the presence of Rician fading. A path-based channel model is used, in which the channel is described by a limited number of paths, each characterized by a delay, Doppler scale, and attenuation factor. The resulting algorithm initially estimates the overall sparse channel tap delays and Doppler shifts using a compressed sensing approach, in the form of the orthogonal matching pursuit (OMP) algorithm. Then, a computationally efficient and novel channel estimation algorithm is developed by combining the OMP and maximum a posteriori probability (MAP) techniques for estimating the sparse complex channel path gains whose prior densities have complex Gaussian distributions with unknown mean and variance vectors, where a computationally efficient maximum likelihood algorithm is proposed for their estimation. Monte Carlo simulation results show that the mean square error and symbol error rate performances of the OMP-MAP algorithm uniformly outperforms the conventional OMP-based channel estimation algorithm, in case of uncoded OFDM-based UWA communications systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The rapid growth in the adoption of wireless sensor networks (WSNs) is motivated by the advantages offered with respect to wired systems, such as cost-effectiveness, easiness of installation, scalability, flexibility, and self-organization. However, due to their nature, the nodes in WSN rely on a limited energy source; therefore, an efficient communication among the nodes is desirable to prolong the lifetime of the WSN. In particular, the alternation of active and sleep states and the regulation of the transmission power represent two common approaches to save energy. This paper proposes the simultaneous use of two fuzzy logic controllers to dynamically adjust the sleeping time and the transmission power of the nodes in order to optimize energy consumption. The experimental results show a network lifetime improvement ranging from 30 to 40%, according to the adopted Medium Access Control (MAC) protocol.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we investigate cooperative spectrum sensing (CSS) in a cognitive radio network (CRN) where multiple secondary users (SUs) cooperate in order to detect a primary user, which possibly occupies multiple bands simultaneously. Deep cooperative sensing (DCS), which constitutes the first CSS framework based on a convolutional neural network (CNN), is proposed. In DCS, instead of the explicit mathematical modeling of CSS, the strategy for combining the individual sensing results of the SUs is learned autonomously with a CNN using training sensing samples regardless of whether the individual sensing results are quantized or not. Moreover, both spectral and spatial correlation of individual sensing outcomes are taken into account such that an environment-specific CSS is enabled in DCS. Through simulations, we show that the performance of CSS can be greatly improved by the proposed DCS.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Direct policy search is a promising reinforcement learning framework in particular for controlling continuous, high-dimensional systems. As one of direct policy search, reward weighted regression (RWR) was proposed by Peters et al. The RWR algorithm estimates the policy parameter based on EM algorithm and is therefore prone to overfitting. In this paper, we focus on variational Bayesian inference to avoid overfitting problem, and propose the direct policy search reinforcement learning based on variational Bayesian inference (VBRL). The performance of the proposed VBRL is assessed in several experiments with mountain car and ball batting task. These experiments highlight the VBRL produces higher average return and outperforms the RWR.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, the authors design a controller for a robot-trailer system to solve the path following problem. A kinematic state-space model is derived using tractor steering angle rate as system input so that undesirable effects of input disturbances can be reduced. A tracking error of the trailer position may exist when there are biases on the measurements of heading angles. An improved integral separation combined with linear quadratic regulator (LQR) is designed for the system to remove the trailer position error. The tractor relative location is used to estimate whether the system is under steady state. The controller with integral action can be difficult to tune, so a genetic algorithm (GA) is then used to find optimal LQR controller parameters. Simulation results verify the approaches above.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Millimeter wave (MmWave) is a promising technology nowadays because it provides users with ultra-high data rate links with low latency time. Hence, internet of things, tactile internet and cloud computing applications which becomes an essential matter in 5G networks can be supported using it. MmWave standard, IEEE 802.11 ad, proposed using scheduled medium access control (MAC) based on time division multiple access (TDMA) to share radio resources among active mmWave users. MmWave has a highly dynamic time-variant channel with high blocking probability which highly degrades the entire system performance. Hence, applying simple channel independent scheduling schemes, e.g., round robin (RR), will be inefficient. Thus, an efficient radio resource scheduling with considering the changes in mmWave channel is needed for maximum benefit from available resources. This paper presents the performance evaluation of one of the channel sensitive scheduling schemes, proportional fairness scheduling (PFS), in downlink mmWave network by executing numerical simulation in different channel conditions and various mmWave beamwidths regarding the total system data rate and fairness between users.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Heterogeneous network concept is going to be utilized in the fifth generation mobile communication systems in order to enhance both network throughput and user throughput. Many techniques have been investigated to increase the throughput. However, the techniques are easily degraded with many causes. Performance degradation could be avoided with assistance of some side information such as location of a terminal and Doppler frequency. If Doppler frequency is known before the signal transmission, throughput reduction can be alleviated by selecting modulation schemes with lower cardinality. This paper proposes Doppler frequency estimation technique using overlap frequency domain equalization and the optimum FDE weight which is derived for the proposed estimation technique. The proposed estimation technique with the proposed FDE weight attains better estimation performance than that with the conventional MMSE weight.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Optimal control of bilinear ensemble systems has been of great importance and interest in the areas of mathematical and computational optimal control. However, effective methods for solving these emerging optimal control problems remain underdeveloped. In this work, we extend the iterative method developed in our previous work for solving the fixed-endpoint optimal control problems to solve for the optimal control with free-endpoint conditions. The method is based on solving the corresponding optimal control problem for a linear ensemble system at each iteration and does not require numerical optimization. We discuss the convergence of the iterative method and explore the stochastic ensemble systems driven by Gaussian noise. We also illustrate its robustness and applicability with numerical examples.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Genes expression data in large data matrix provide challenges in applying analytical methods, interpreting, and drawing conclusions. This study aims to evaluate genes expression classification on different maize lines and different organs (Apex, Root, Shoot, Ear, and Tassel) based on Maize Nested Association Mapping (NAM) datasets using K-Nearest Neighbor (KNN) approach. As a result, we obtained the accuracy value and AUC value of 0. 926 and 0.992 respectively from KNN analysis with k = 5. Besides, we showed the classification of genes expression datasets to distinguish organ-specific expression.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Small, low-cost, wireless cameras are becoming increasingly commonplace making surreptitious observation of people more difficult to detect. Previous work in detecting hidden cameras has only addressed limited environments in small spaces where the user has significant control of the environment. To address this problem in a less constrained scope of environments, we introduce the concept of similarity of simultaneous observation where the user utilizes a camera (Wi-Fi camera, camera on a mobile phone or laptop) to compare timing patterns of data transmitted by potentially hidden cameras and the timing patterns that are expected from the scene that the known camera is recording. To analyze the patterns, we applied several similarity measures and demonstrated an accuracy of over 87% and and F1 score of 0.88 using an efficient threshold-based classification. Furthermore, we used our data set to train a neural network and saw improved results with accuracy as high as 97% and an F1 score over 0.95 for both indoors and outdoors settings. From these results, we conclude that similarity of simultaneous observation is a feasible method for detecting hidden wireless cameras that are streaming video of a user. Our work removes significant limitations that have been put on previous detection methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mangosteen is one of the fruits that has an enormous export potential in Thailand. It is well-known as the queen of fruit. Mangosteen export generates large revenue; however, fruit is not defect free it contains many undesirable external as well as internal condition which results in the shipment rejection and decrease the reliability of the export. Therefore, this research investigates an approach for texture image analysis based surface roughness detection and classification into 3 classes: i.e., Glossy Surface, Mid Rough Surface and Extreme Rough Surface. In this study, for the first time, we propose the textural features extracted using Gray-Level Co-occurrence Matrix (GLCM) for surface roughness classification of mangosteen.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Health indicators (HI) attempt to identify and quantify history and ongoing degradation processes by extracting feature information from collected data. The main problems in the existing HI construction methods are as follows: (1) existing methods use fixed learning rate to train deep learning network, which is inefficient; (2) existing HIs construction methods do not take into account phase degradation; (3) failure threshold is difficult to determine in remaining useful life (RUL) prediction. In this paper, a HI construction method of rolling bearings based on a deep convolutional neural network with polynomial decaying learning rate (PDCNN) considering phase degradation is proposed. In this paper, the original vibration signal is input into the PDCNN, and the features extracted from the deep convolutional neural network are input into the deep neural network (DNN) to construct HI. The training label setting takes into account phase degradation. The polynomial decaying learning rate is used to train the neural network efficiently. Examples show that the HI constructed by the proposed method is superior to compared three methods in monotonicity and trendability. The polynomial decaying learning rate can improve the training efficiency of deep neural network and save a lot of time. The value of the constructed HI is in the interval [0, 1], which solves the problem that the failure threshold is difficult to determine in RUL prediction.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper is about one way to address a troubling aspect of our use of software-intensive systems in complex environments: information flow across the system boundary. Everything we (and any other organism or embedded system) can know about the world around us is limited and largely determined by our sensor data, and what we can deduce or otherwise learn about the regularities they can be expected to exhibit. As system designers, we can program some of this knowledge into our systems, but we are almost always wrong in important and unforeseen ways. We need to help the systems we design mitigate these problems by actively participating in creating this knowledge. We want our systems to observe their environments, make critical inferences about their present and future behaviors, and use the resulting models to inform their own decisions. Moreover, we expect these systems to do the same analyses on their own internal structure and behavior, and on their interactions with the environment, to help them react more effectively to unexpected partial system failures and environmental surprises. In this paper, we show how the Wrappings integration infrastructure applies to this class of problems, by describing the architecture of self-modeling systems, which have models of their own behavior that are used to generate and manage that behavior.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the stringent demand of high data transmission, the scenario of the next generation network has changed drastically. Among various techniques of reducing the latency data of transmission and providing better quality-of-service, the deployment of ultra dense small cells is deemed as the most promising one which reduces the average distance from a mobile user to the nearest base station. However, the overhead of energy consumption introduced by the network densification needs to be carefully addressed using dynamic switching technique, in which underutilized base stations are put into lower power sleep mode. On the other hand, the new user association scheme needs to be developed to synergize well with novel techniques and improve energy efficiency further. In this paper, we propose to jointly investigate user association and dynamic switching with a goal of achieving a reduced energy consumption and a higher energy efficiency in the whole system. In this paper, a message passing based distributed algorithm is proposed. Experimental results show that the energy consumption is by 84.7% over greedy algorithm and energy efficiency can be improved up to 2.4X compared to baselines.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Discrimination of food-contaminating microorganisms is an essential technology to secure the safety in manufacturing of foods and beverages. Conventionally, discrimination of the microorganisms has been performed by morphological observation, genetic analysis, and more recently, biochemical fingerprinting using mass spectrometry. However, several drawbacks exist in these methods, such as long assay time, cumbersome operations, and expensive equipment. To address these issues, we have proposed a novel method for discrimination of food-contaminating microorganisms, termed \u201ccolony fingerprinting\u201d, based on bioimage informatics. In colony fingerprinting, growth of bacterial colonies were monitored using a lens-less imaging system. The characteristic images of colonies, referred to as colony fingerprints (CFPs), were obtained over time, and subsequently used to extract discriminative parameters. We demonstrated to discriminate 20 bacterial species by analyzing the extracted parameters with machine learning approaches, namely support vector machine and random forest. Colony fingerprinting is a promising method for rapid and easy discrimination of food-contaminating microorganisms.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A Neural-Network-biased Genetic Algorithm is employed to design all dielectric waveguide with periodic structures with preferential band-pass frequencies. Preliminary results indicate that an optimum periodic structure design exist for target frequency transmission and highlights the need for robust optimization scheme such as the one described here.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Although deep learning pathology diagnostic algorithms are proving comparable results with human experts in a wide variety of tasks, they still require a huge amount of well annotated data for training. Generating such extensive and well labelled datasets is time consuming and is not feasible for certain tasks and so, most of the medical datasets available are scarce in images and therefore, not enough for training. In this work we validate that the use of few shot learning techniques can transfer knowledge from a well defined source domain from Colon tissue into a more generic domain composed by Colon, Lung and Breast tissue by using very few training images. Our results show that our few-shot approach is able to obtain a balanced accuracy (BAC) of 90% with just 60 training images, even for the Lung and Breast tissues that were not present on the training set. This outperforms the finetune transfer learning approach that obtains 73% BAC with 60 images and requires 600 images to get up to 81% BAC.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A joint communication and state estimation problem in a Gaussian multiple access channel with common additive state is considered. The state process is assumed to be IID Gaussian, and known non-causally at both the transmitters. The receiver not only has to decode the messages from the transmitters, but also needs to estimate the state process to within some prescribed squared error distortion. We provide a complete characterization of the optimal sum-rate versus distortion performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is envisaged that the future cellular networks (5G) will be able to meet the promising capacity and quality of experience requirements through extreme network densification and conglomeration of diverse technologies. It is easy to fathom that efficient management of such a convoluted network will be one of the big challenges faced by 5G. To cope with this challenge, Self-Organizing Networks (SONs) that were originally designed for legacy networks with reactive approach needs to be transformed to proactive paradigm. This radical transformation is possible only if the future network state can be predicted beforehand by harnessing historical network data. Mobility prediction is one of the key enablers of Proactive SON which enables efficient resource management. In this paper, we perform comparative analysis of four mobility predictors: Deep Neural Network (DNN), Extreme Gradient Boosting Trees (XGBoost), Semi-Markov, and Support Vector Machine (SVM). Our investigation is based on realistic synthetic dataset of eighty-four mobile users generated through realistic Self-similar Least Action Walk (SLAW) mobility model. We evaluate the effectiveness of each model not only based on the model's ability to predict the future location of mobile users but also the time each algorithm takes to be fully trained and perform such prediction. XGBoost stands out as clear winner among all predictors considered with high accuracy of 90%. Its high prediction accuracy enables high energy saving gain of above 80% when it is employed for driving proactive energy saving SON solution.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, latent vector embedding has become a research hotspot, with its great representative ability to measure the latent relationships among different views. However, most researches utilize the inner product of latent vectors as the representation of relationships, and they develop some embedding models based on this theory. In this paper, we take deep insight into the existing embedding models and find that utilizing the inner product may increase several problems: 1) in latent space, the inner product among three vectors may violate triangle principle; 2) the inner product cannot measure the relationships between vectors in the same category, such as user and user and item and item; and 3) the inner product cannot catch the collaborative relationships (user-user and item-item) for collaborative filtering. Along with this line, we propose a latent vector embedding model for collaborative filtering: latent dual metric embedding (LDME), which utilizes the dual-Euclidean distance in latent space, instead of the inner product, to represent different types of relationships (user-user, item-item, and user-item) with a uniform framework. Specifically, we design an embedding loss function in LDME, which can measure the close and remote relationships between entities, tackle the above problems, and achieve a more clear, well-explained embedding result. Extensive experiments are conducted on several real-world datasets (Amazon, Yelp, Taobao, and Jingdong), where the expiring results demonstrate that LDME can overperform some state-of-the-art user-item embedding models and can benefit the existing collaborative filtering models.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The statistical analysis of the examination results emerges as an important key in the field of education as the quality of the education is revealed by the evaluation process. The conclusions which follow the analysis can be a feedback of the quality of examination papers or the supplementary of the students' feedback of teachers teaching process. Therefore, it provides a summary of the prevailing standard of education which lead to educational research and reform. These reforms contribute to the positive changes applied to the examination system and to the learning and teaching process continuously. In year 2011 Information and Communication Technology was introduced to the G.C.E Advanced Level as a main subject. The main objective of this research is to identify problems relating to the G.C.E Advanced Level Examination ICT subject and provide suggestions for enhance performances in ICT results among students and provide suggestions for the teaching process of the education system. In this issue Association Rule technique of data mining is used to identify certain patterns in the examination results. The analysis exposure that, better the performance in mathematics and ICT subjects in G.C.E. Ordinary level, higher the probability of performing well at G.C.E. Advanced level examination ICT subject. Other analysis was to find whether there is an impact on gender difference for his/her G.C.E. Advanced level ICT subject result. It was found that when considering overall factors, female students performed well in the exam than male students.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The National Aeronautics and Space Administration (NASA) is in the midst of defining and developing the future space and ground architecture for the coming decades to return science and exploration discovery data back to investigators on Earth. Optimizing the data return from these missions requires planning, design, standards, and operations coordinated from formulation and development throughout the mission. The use of automation enhanced by cognition and machine learning are potential methods for optimizing data return, reducing costs of operations, and helping manage the complexity of the automated systems. In this article, we discuss the potential role of machine learning in the linkto- link aspect of the communication systems. An experiment using NASA's Space Communication and Navigation Testbed onboard the International Space Station and the ground station located at NASA John H. Glenn Research Center demonstrates for the first time the benefits and challenges of applying machine learning to space links in the actual flight environment. The experiment used machine learning decisions to configure a space link from the ISS-based testbed to the ground station to achieve multiple objectives related to data throughput, bandwidth, and power. Aspects of the specific neural-network-based reinforcement learning algorithm formation and on-orbit testing are discussed.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we study the problem of distributed multi-agent optimization over a network, where each agent possesses a local cost function that is smooth and strongly convex. The global objective is to find a common solution that minimizes the average of all cost functions. Assuming agents only have access to unbiased estimates of the gradients of their local cost functions, we consider a distributed stochastic gradient tracking method. We show that, in expectation, the iterates generated by each agent are attracted to a neighborhood of the optimal solution, where they accumulate exponentially fast (under a constant step size choice). More importantly, the limiting (expected) error bounds on the distance of the iterates from the optimal solution decrease with the network size, which is a comparable performance to a centralized stochastic gradient algorithm. Numerical examples further demonstrate the effectiveness of the method.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the explosive growth of multimedia applications, heterogeneous cellular networks (HetNets) are widely deployed to meet the increasingly impressing demands on communication capacity. However, supporting the massive real-time traffic generated by always-on multimedia applications in HetNets causes enormous energy consumptions. The tradeoff between energy consumptions and profits of service providers while maintaining satisfied quality-of-service (QoS) has become a significant objective. To this end, this paper proposes a novel framework of power rationing in HetNets to achieve maximal profit and guaranteed service performance. A dynamic power rationing strategy that employs Tullock contest is developed to model the power control of multiple cells into a game and solve the contradiction of profits and energy consumption as the tradeoff between QoS and cost. Two principal challenges in gaming, incomplete information, and the curse of dimensionality are resolved by the designed virtual repeated game that adopts the Monte-Carlo method and particle swarm optimization (PSO) to obtain the Nash equilibrium. The equilibrium of power rationing balances the energy consumptions and profits of cells, which ensures the optimal solution for the multimedia service providers and HetNets operators. The experimental results demonstrate that the developed model can serve as an efficient tool for power rationing in multimedia HetNets.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The aim of this paper is to evaluate the distribution system reliability including weather dependency of component failure rate (FR) and restoration time (RT) while an efficient Multilevel Monte Carlo (MLMC) method is used. The effects of different weather conditions such as high winds, lightning can be incorporated in the reliability evaluation by considering their impacts on FT and RT. MLMC method is a recently developed variance reduction technique for Monte Carlo simulation (MCS) which is proposed for reducing reliability indices computation time. Application of the MLMC method is carried out on a well-known test distribution system. The effects of the normal and adverse weather conditions on reliability estimation are determined. The results are compared with those obtained by using MCS.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we consider resource allocation for edge computing in internet of things (IoT) networks. Specifically, each end device is considered as an agent, which makes its decisions on whether offloading the computation tasks to the edge devices or not. To minimize the long-term weighted sum cost which includes the power consumption and the task execution latency, we consider the channel conditions between the end devices and the gateway, the computation task queue as well as the remaining computation resource of the end devices as the network states. The problem of making a series of decisions at the end devices is modelled as a Markov decision process and solved by the reinforcement learning approach. Therefore, we propose a near optimal task offloading algorithm based on \u03f5-greedy Q-learning. Simulations validate the feasibility of our proposed algorithm, which achieves a better trade-off between the power consumption and the task execution latency compared to these of edge computing and local computing modes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Multicast is widely deployed in data centers for point-to-multi-point communications. It has an established set of control protocols such as IGMP and PIM that has limitations around the lack of bandwidth awareness when establishing the multicast trees. This could lead to over-subscription of network links and packet loss impacting user quality of experience. Other existing multicast issues are related to path setup time as delay translates to deterioration of user experience. In this paper, we present and implement the DiRP algorithm utilizing distributed decision making architecture to optimize multicast tree formation while maintaining low path setup time. The system is implemented using off-the-shelve commercially available switches. the proposed DiRP algorithm maintains the creation and removal of source trees based on bandwidth requirements. We test the algorithm using Cisco's Nexus commercially available switches. Testing results confirm that the DiRP Algorithm is able to setup multicast tree paths based on available bandwidth while maintaining distributed decision making in the fabric to lower path setup time. The system offers substantially lower path setup time compared to centralized systems while maintaining bandwidth awareness when setting up the fabric.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Ensuring protection against side channel attacks (SCAs) is a crucial requirement in the design of modern secure embedded systems. Profiled SCAs, the class to which template attacks and machine learning attacks belong, derive a model of the side channel behavior of a device identical to the target one, and exploit the said model to extract the key from the target, under the hypothesis that the side channel behaviors of the two devices match. We propose an architectural countermeasure against cross-device profiled attacks which differentiates the side channel behavior of different instances of the same hardware design, preventing the reuse of a model derived on a device other than the target one. In particular, we describe an instance of our solution providing a protected hardware implementation of the advanced encryption standard (AES) block cipher and experimentally validate its resistance against both Bayesian templates and machine learning approaches based on support vector machines also considering different state-of-the-art feature reduction techniques to increase the effectiveness of the profiled attacks. Results show that our countermeasure foils the key retrieval attempts via profiled attacks ensuring a key derivation accuracy equivalent to a random guess.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We introduce a game-theoretic framework to explore revenue sharing in an Edge-Cloud computing system, in which computing service providers at the edge of the Internet (edge providers) and computing service providers at the cloud (cloud providers) collectively provide computing resources to clients (e.g., end users or applications) at the edge. Different from traditional cloud computing, the providers in an Edge-Cloud system are independent and self-interested. To achieve high system-level efficiency, the manager of the system adopts a task distribution mechanism to maximize the total revenue received from clients and also adopts a revenue sharing mechanism to split the received revenue among computing servers (and hence service providers). Under those system-level mechanisms, service providers attempt to game with the system in order to maximize their own utilities, by strategically allocating their resources (e.g., computing servers). Our framework models the competition among the providers in an Edge-Cloud system as a non-cooperative game. We have shown the existence of Nash equilibrium in the game both theoretically and practically through simulations and experiments on an emulation system that we have developed. We find that revenue sharing mechanisms have a significant impact on the system-level efficiency at Nash equilibria, and surprisingly the revenue sharing mechanism based directly on actual contributions can result in significantly worse system performance than Shapley value sharing mechanism and Ortmann proportional sharing mechanism. Our framework provides an effective economics approach to the understanding and designing of efficient Edge-Cloud computing systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The current discrete-time (e.g., hourly) modeling and prediction methods fall short in capturing and anticipating the sub-interval variations of electricity load. This leads to inability of power system operators to appropriately utilize the available resources to follow and compensate the load variations. This paper takes a novel and different approach on modeling electricity load, and proposes a continuous-time model for characterizing the uncertainty and variability of load. More specifically, the electricity load is modeled as a continuous-time stochastic process that is projected on a reduced-order function space spanned by Bernstein polynomials, which ensures the continuity of the process over the estimation and forecasting horizons. We assume a Gaussian process (GP) prior on the load process and design a covariance function that reflects the periodicity and smoothness of electricity load. We develop a computationally efficient method for estimating the hyperparameters of the model using the solution of a maximum likelihood estimation problem and form the posterior GP process. The proposed method is utilized to model and predict the load of California Independent System Operator (CAISO). The proposed model uniquely predicts the continuous-time mean value and uncertainty envelopes of future CAISO load, which inherently embeds information on the continuous-time variations and the associated ramping requirements of the load.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper develops an approach for household appliance identification and classification of household activities of daily living (ADLs) using residential smart meter data. The process of household appliance identification, i.e., decomposing a mains electricity measurement into each of its constituent individual appliances, is a very challenging classification problem. Recent advances have made deep learning a dominant approach for classification in fields, such as image processing and speech recognition. This paper presents a deep learning approach based on multilayer, feedforward neural networks that can identify common household electrical appliances from a typical household smart meter measurement. The performance of this approach is tested and validated using publicly available smart meter data sets. The identified appliances are then mapped to household activities, or ADLs. The resulting ADL classifier can provide insights into the behavior of the household occupants, which has a number of applications in the energy domain and in other fields.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: New IoT applications are demanding for more and more performance in embedded devices while their deployment and operation poses strict power constraints. We present the security concept for a customizable Internet of Things (IoT) platform based on the RISC-V ISA and developed by several Fraunhofer Institutes. It integrates a range of peripherals with a scalable computing subsystem as a three dimensional System-in-Package (3D-SiP). The security features aim for a medium security level and target the requirements of the IoT market. Our security architecture extends given implementations to enable secure deployment, operation, and update. Core security features are secure boot, an authenticated watchdog timer, and key management. The Universal Sensor Platform (USeP) SoC is developed for GLOBALFOUNDRIES' 22FDX technology and aims to provide a platform for Small and Medium-sized Enterprises (SMEs) that typically do not have access to advanced microelectronics and integration know-how, and are therefore limited to Commercial Off-The-Shelf (COTS) products.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The use of unmanned aerial vehicles (UAVs) in future wireless networks is gaining attention due to their quick deployment without requiring the existing infrastructure. Earlier studies on UAV-aided communication consider generic scenarios, and very few studies exist on the evaluation of UAV-aided communication in practical networks. The existing studies also have several limitations, and hence, an extensive evaluation of the benefits of UAV communication in practical networks is needed. In this paper, we proposed a UAV-aided Wi-Fi Direct network architecture. In the proposed architecture, a UAV equipped with a Wi-Fi Direct group owner (GO) device, the so-called Soft-AP, is deployed in the network to serve a set of Wi-Fi stations. We propose to use a simpler yet efficient algorithm for the optimal placement of the UAV. The proposed algorithm dynamically places the UAV in the network to reduce the distance between the GO and client devices. The expected benefits of the proposed scheme are to maintain the connectivity of client devices to increase the overall network throughput and to improve energy efficiency. As a proof of concept, realistic simulations are performed in the NS-3 network simulator to validate the claimed benefits of the proposed scheme. The simulation results report major improvements of 23% in client association, 54% in network throughput, and 33% in energy consumption using single UAV relative to the case of stationary or randomly moving GO. Further improvements are achieved by increasing the number of UAVs in the network. To the best of our knowledge, no prior work exists on the evaluation of the UAV-aided Wi-Fi Direct networks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a student project dedicated to the Internet of Things (IoT). The objective is to design and deploy a network of connected devices using LoRa technology to monitor the 20.000 m2 of our engineering school, ENSEIRB-MATMECA. Supervising the school using connected device came to the requirements to control our energy consumption. We choose a deployment in LoRa technology (open model compared to Sigfox) in order to master the deployment of the network from the beginning to the end. Modtronix LoRa modules inAIR9 and inAIR4 operating respectively in the 868MHz and 433MHz frequency bands were used as key components. There are 3 main blocks: sensor, gateway and server. All three are fully designed in the scope of this project by a team composed of 18 students.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The challenge of running complex physics code on the largest computers available has led to dataflow paradigms being explored. While such approaches are often applied at smaller scales, the challenge of extreme-scale data flow computing remains. The Uintah dataflow framework has consistently used dataflow computing at the largest scales on complex physics applications. At present Uintah contains two main dataflow models. Both are based upon asynchronous communication. One uses a static graph-based approach with asynchronous communication and the other uses a more dynamic approach that was introduced almost a decade ago. Subsequent changes within the Uintah runtime system combined with many more large scale experiments, has necessitated a reevaluation of these two approaches, comparing them in the context of large scale problems. While the static approach has worked well for some large-scale simulations, the dynamic approach is seen to offer performance improvements over the static case for a challenging fluid-structure interaction problem at large scale that involves fluid flow and a moving solid represented using particle method on an adaptive mesh.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Large-scale scientific facilities provide a broad community of researchers and educators with open access to instrumentation and data products generated from geographically distributed instruments and sensors. This paper discusses key architectural design, deployment, and operational aspects of a production cyberinfrastructure for the acquisition, processing, and delivery of data from large scientific facilities using experiences from the National Science Foundation's Ocean Observatories Initiative. This paper also outlines new models for data delivery and opportunities for insights in a wide range of scientific and engineering domains as the volumes and variety of data from facilities grow.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There is an increasing interest to identify users and behaviour profiling from network traffic metadata for traffic engineering and security monitoring. Network security administrators and internet service providers need to create the user behaviour traffic profile to make an informed decision about policing, traffic management, and investigate the different network security perspectives. Additionally, the analysis of network traffic metadata and extraction of feature sets to understand trends in application usage can be significant in terms of identifying and profiling the user by representing the user's activity. However, user identification and behaviour profiling in real-time network management remains a challenge, as the behaviour and underline interaction of network applications are permanently changing. In parallel, user behaviour is also changing and adapting, as the online interaction environment changes. Also, the challenge is how to adequately describe the user activity among generic network traffic in terms of identifying the user and his changing behaviour over time. In this paper, we propose a novel mechanism for user identification and behaviour profiling and analysing individual usage per application. The research considered the application-level flow sessions identified based on Domain Name System filtering criteria and timing resolution bins (24-hour timing bins) leading to an extended set of features. Validation of the module was conducted by collecting Net Flow records for a 60 days from 23 users. A gradient boosting supervised machine learning algorithm was leveraged for modelling user identification based upon the selected features. The proposed method yields an accuracy for identifying a user based on the proposed features up to 74%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The recent development of electronics and technological advances in wireless networks have led to the development of miniaturized wireless sensors, that can be used in the field of remote healthcare patients monitoring. When interconnected in a Wireless Body Area Network (WBAN), these tinny sensors, can be positioned on or implanted inside the human body to continuously monitor several physiological parameters (temperature, blood pressure, ECG, etc. ...) of the patient. The purpose of this paper is to compare the impact of the two modulation schemes specific to the NB physical layer at 2.4Ghz when the scheduled access is considered, for a WBAN model consisting of 11 sensor nodes (one of which is the hub) based on the IEEE 802.15.6 standard. The numerical study focuses on three metrics: packet reception, latency, and node-level energy consumption, and is performed using the Castalia 3.3 framework, based on the OMNeT ++ platform (4.6). Castalia simulator supports the IEEE 802.15.6 standard, which justifies the choice of its use in the simulations carried out.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: E-Iearning is the process of learning, educating, or training using web-based technologies. With the extensive use of such means, a tremendous amount of data are generated. The traditional methods of manual features engineering to collect, manage and process those data are very limited and time-consuming. Deep learning is one of the modern approaches that could automate this process in order to achieve effective smart e-learning. In this paper, we aim to introduce our personalized e-learning model that associate deep learning with process mining., in order to provide the learners with learning resources that fit their individual preferences, after giving an overview about both e-learning as an online educational system, and deep learning as a broader family of artificial intelligence.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Algebraic thinking is a very important skill that should be mastered by students at an early stage before learning algebra. However, algebraic thinking is not emphasized in learning algebra. The traditional approach for teaching algebra, which involved writing and solving equations according to the rules of mathematics, is taught procedurally and in isolation from other mathematical domains, hence towards real-world application of learning mathematics. In this paper, we conducted a study that integrated the students' algebraic thinking into the problem-based learning (PBL) process. Qualitative data from a learning task, namely which consisted of teaching notes, self-readings, reflections, self-evaluations, evaluation of scenario problems, task-based interview transcripts, and task-based interview notes were used to explore the acquisition of algebraic thinking. Accordingly, the framework that integrates students' algebraic thinking with the PBL process is expected to assist teachers in enhancing the effectiveness of teaching and learning of algebra and can potentially serve as a basis for developing algebraic thinking among school students, particularly at lower secondary level.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Inter-subject variabilities in brain signals can significantly deteriorate the accuracy of brain-computer interface (BCI) systems. Every subject has different brain signals; also, the performance of a subject varies widely between sessions, within a session, between on-line and off-line settings, and even from epoch to epoch. Such variabilities arise in measurements obtained during different BCI sessions require subject and session specific decoder models. This work proposes three different deep learning models for subject-independent decoding of event-related potentials in electroencephalographic signals: (1) shallow convolutional neural network, (2) gated recurrent neural network, and (3) CNN-RNN-Net: a hybrid one-dimensional convolution and a gated recurrent unit model. Experimental results demonstrate that the proposed models outperform the conventional baseline models in decoding subject-independent data. Moreover, among the three models, the CNN-RNN-Net has shown improved classification results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Web 2.0 is a trend in the use of World Wide Web (www) technology and web design. This includes sharing information among others such as reviews on certain products or services. These days, many travelers depend on hotel reviews as one of the sources in planning their travels. Travelers usually read the reviews whenever they wish to go for vacations. Summarization is a way to shorten all the reviews without change the actual meaning of the sentence. In order to summarise the hotel reviews, a method named Featured Noun Pairing has been chosen. This method associates features (nouns) and adjectives to represent a whole review sentence. This tool has been evaluated by experts and achieved 70% accuracy of noun, 80% accuracy of adjective, and 60% accuracy of pairing.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The algorithms of a reference model adaptive control, static control, and adaptive control with disturbances estimation are considered. A mobile object is described by equations of kinematics and dynamics in three-dimensional space. The novel structure of the control system is proposed, which allows separating the circuits of astaticism, estimation, and parametric adaptation. Asymptotic stability of the closed-loop adaptive system is proved by the Lyapunov method. It is proposed to perform the parametric adaptation by adjustment of the geometric mean root. The analysis of errors of disturbances estimation is performed. The simulation and comparative study of the reference model adaptive control, astatic control, and adaptive control with disturbances estimation are carried out.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Long Short-Term Memory is a recurrent neural network that can be trained to remember long sequences of data and acts as a generative model. As a generative model, Long Short-Term Memory has the ability to reproduce the trained sequences for arbitrary length. We train Long Short-Term Memory with sequential motion data of Remo Dance, a traditional dance from East Java. Motion data is acquired from a motion capture system from real dancer as sequences of bone rotation. Training sequential data on Long Short-Term Memory is time consuming even by using current GPU technology. We found that applying feature scaling and how data are grouped to be trained together are useful strategies to achieve optimal training. Our experiments show that the scale factor in feature scaling depends on how many sequences are trained together. Single sequence trainings need value range from -8 to +8. Multiple sequences need a lower value range accordingly. We also found that sequences with small variances can be trained better when combined with large variances sequences. Trained Long Short-Term Memory is able to reproduce the dance moves with some variations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep learning (DL) algorithms have substantially improved in terms of accuracy and efficiency. Convolutional Neural Networks (CNNs) are now able to outperform traditional algorithms in computer vision tasks such as object classification, detection, recognition, and image segmentation. They represent an attractive solution for many embedded applications which may take advantage from machine-learning at the edge. Needless to say, the key to success lies under the availability of efficient hardware implementations which meet the stringent design constraints. Inspired by the way human brains process information, this paper presents a method that improves the processing efficiency of CNNs leveraging their repetitiveness. More specifically, we introduce (i) a clustering methodology that maximizes weights/activation reuse, and (ii) the design of a heterogeneous processing element which integrates a Floating-Point Unit (FPU) with an associative memory that manages recurrent patterns. The experimental analysis reveals that the proposed method achieves substantial energy savings with low accuracy loss, thus providing a practical design option that might find application in the growing segment of edge-computing.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: How to reduce the content placement cost of cloud content delivery networks (CCDNs) is a hot topic in recent years. Traditional content placement methods mainly reduce the cost of content placement by constructing delivery trees, but they cannot adapt to the dynamic deployment of cloud proxy servers in the CCDNs. In addition, the traditional content placement method only provides delivery paths according to local decision-making without considering global dynamics of the congestion in the CCDNs, which is also one of the main factors causing high cost of content placement. To solve these problems, we propose a content placement model based on Q-learning for the dynamic CCDNs, called Q-content placement model (Q-CPM). This Q-learning approach can lead to better routing decisions due to up-to-date and more reliable congestion values. Then, based on the Q-CPM model, an algorithm is proposed to construct the Q-adaptive delivery tree (Q-ADT). In this algorithm, local and nonlocal congestion information is propagated over network learning packets. Through this algorithm, the paths with low congestion cost will be selected and can adapt to the dynamic cloud delivery environment. The experimental results show that the method can adapt to the dynamic changes of the CCDNs flexibly and reduce the overall congestion cost of content placement effectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In paper the problem of estimation the degree of wetlands on basis of remote sensing data is considered. For this purpose digital image processing is used. The satellite images are used as the main data. In paper set of local and global features is selected for segmentation area on two classes: water surface and land surface. For each point of color satellite image segment is determined whether the corresponding portion of the area to the water surface. Since the task set is described in general terms and not be formalized, segmentation method is based on machine learning. For processing of remote sensing data neural network is used. The using of neural networks for processing of remote sensing data can automate the task of analysis and assessment of areas of water bodies. The proposed solutions have been successfully used to assess the Oka river basin in Nizhny Novgorod and Vladimir regions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Opinion maximization in social networks is an optimization problem, which targets at determining some influential individuals (i.e., seed nodes), propagating the desired opinion to their neighbors, and eventually obtaining maximum opinion spread. Previous studies assume that influence power of one individual is mainly calculated by using some network structure properties and once the opinion of one individual is determined, its opinion usually keeps unchanged. However, in the real scenario, the influence power of one individual may be unknown and should be closely associated with the dynamic opinion formation process. In this paper, we propose a novel Influence Power-based Opinion Framework (IPOF) to solve the opinion maximization problem, which is composed of two phases: 1) influence power estimation, and 2) elimination of influence overlapping (EIO). Specifically, we design the exponential influence power and estimate the unknown parameter of influence power through maximum likelihood estimation due to its simplicity, practicability, and superior convergence in large samples. To generate the opinion series dynamically, the weighted voter model is proposed by leveraging influence power and intimate degree. Moreover, we also prove that the likelihood function is concave by using Hessian matrix. To determine the initial seed nodes and facilitate large opinion propagation, influence power-based EIO algorithm is proposed. Experimental results in six social networks demonstrate that the proposed approach outperforms the state-of-the-art benchmarks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurate evaluation of Ultra Low Power Systems on Chip (ULP SoC) is a huge challenge for designers and developers. In embedded applications, especially for Internet of Things end-node devices, ULP SoCs have to interact with their environment, but modelling a complete SoC and the peripheral components, their interaction and low-power policies, can be very complex in terms of developments and benchmarking. In order to cope with this challenge, an approach is to implement the desired system on FPGA with a monitoring infrastructure dedicated to fast and accurate evaluation. This paper presents a reconfigurable prototyping platform used for SoC architecture exploration and real-time application evaluation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a C-band Hybrid Semiconductor Integrated Circuit (HySIC) rectifier with a Gallium Nitride (GaN) Shottky barrier diode and silicon (Si) matching circuit for RF energy harvesting. The HySIC rectifier is used for supplying driving power to wireless sensor network systems in spacecraft. A high-functional, small-size, and low-cost rectifier was fabricated by combining GaN, which exhibits high resistivity to cosmic rays, and Si, which provides excellent integration technology. Based on the measurement results of the GaN diode, the HySIC rectifier was fabricated. The size of rectifier was 3.9 mm \u00d7 8.1 mm and its maximum conversion efficiency was obtained with 21.7% in the C-band.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a Model Predictive Control (MPC) framework for control of a quarter car semi-active suspension system based on reachable sets approach. The proposed approach provides the flexibility of systematically including the information on the bounds of future road disturbances over the prediction horizon onto the MPC problem formulation by means of reachable sets. This inclusion of future bounds into control design leverages the efficiency of the operation of suspension system. Firstly, a unique Linear Parameter Varying (LPV) model is formulated for control design, which implicitly accounts for the dissipativity constraint of semi-active suspension system. Secondly, an efficient online computation of the reachable sets with the available disturbance information at the current instant for the MPC problem is developed. The proposed methodology is an integration of the preceding steps into a single MPC problem. The effectiveness of the proposed methodology is validated through simulations and the results exhibit better performance of the proposed controller compared to skyhook controller in terms of satisfaction of objective and constraint requirements.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Even though modern industry has been widely digitalized and automated, many printed matters aren't digitalized. And even printed matters are often scanned upside down. To check whether scanned matters is upside down or not, matters should be checked by human. This study will suggest an algorithm that checks scanned matter's direction automatically. With the help of Tesseract OCR engine and extract information of a scanned matter, it could be determined if it is scanned in right direction.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The operation of the electric grid in systems with a large hydroelectric component is often cast as a dynamic programming problem, in which state variables are reservoir levels. To avoid the curse of dimensionality in discretizing such states, the SDDP technique has been successfully applied. However, new demands are being placed on the optimization, with the penetration of renewable sources of faster variability, and the possible incorporation of shorter term energy storage.In this paper, we present preliminary work on extending the SDDP framework to such two time-scale problems. We apply the method to a stylized model of the Uruguayan system, relying on new open source implementations of SDDP to carry out the computations. Our results indicate that the method remains tractable despite the increased problem dimension.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Fifth generation (5G) core network is serviceoriented. It uses service-based interfaces between the control plane functions which improve modularity and align better with cloud networking. In this paper we propose an approach to service oriented radio access network (RAN), where the functions of Next Generation Application Protocol (NGAP) are designed as services of small size that are easily to debug and test. The focus is on handover procedures, where the handover control is exposed as a service. Following the Representational State Transfer (REST) style, we define service resources and the supported methods. Service implementation is considered in terms of modeling the handover state as seen by the core network, source and target radio nodes. The models are formally described and it is proved that they expose equivalent behavior.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We adopted actual intelligent production requirements and proposed a tiny part defect detection method to obtain a stable and accurate real-time tiny part defect detection system and solve the problems of manually setting conveyor speed and industrial camera parameters in defect detection for factory products. First, we considered the important influences of the properties of tiny parts and the environmental parameters of a defect detection system on its stability. Second, we established a correlation model between the detection capability coefficient of the part system and the moving speed of the conveyor. Third, we proposed a defect detection algorithm for tiny parts that are based on a single short detector network (SSD) and deep learning. Finally, we combined an industrial real-time detection platform with the missed detection algorithm for mechanical parts based on intermediate variables to address the problem of missed detections. We used a 0.8 cm darning needle as the experimental object. The system defect detection accuracy was the highest when the speed of the conveyor belt was 7.67 m/min.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, when the stress and strength are two independent Kumaraswamy random variables, we derive the point and interval estimate of the stress-strength parameter, from both frequentist and Bayesian viewpoints, under the Type-II hybrid progressive censoring scheme. In fact, the problem is solved in two cases. First, with the assumption that the stress and strength have different first shape parameters and the common second shape parameter, we attain maximum likelihood estimate (MLE), approximation MLE, and two Bayesian approximation estimates, Lindley's approximation and the Markov chain Monte Carlo (MCMC) method, due to lack of closed forms. Also, the asymptotic confidence interval of R is constructed by asymptotic distribution of it. Moreover, by using the MCMC method, we achieve the highest posterior density credible intervals. Second, with the assumption that the common shape parameter is known, we attain the MLE, the exact Bayes estimate, and the uniformly minimum-variance unbiased estimate of R. Also, we construct the asymptotic and Bayesian intervals for the stress-strength parameter. Furthermore, to compare the performance of various methods, we apply the Monte Carlo simulation. Finally, one real dataset is analyzed for demonstrative aims.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Robotic exoskeletons have emerged as effective rehabilitation and ability-enhancement tools, by mimicking or supporting natural body movements. The control schemes of exoskeletons are conventionally developed based on fixed torque-ankle state relationship or various human models, which are often lack of flexibility and adaptability to accurately address personalized movement assistance needs. This paper presents an adaptive control strategy for personalized robotic ankle exoskeleton in an effort to address this limitation. The adaptation was implemented by applying the experience-based fuzzy rule interpolation approach with the support of a muscle-tendon complex model. In particular, this control system is initialized based on the most common requirements of a \u201cstandard human model,\u201d which is then evolved during its performance by effectively using the feedback collected from the wearer to support different body shapes and assistance needs. The experimental results based on different human models with various support demands demonstrate the power of the proposed control system in improving the adaptability, and thus applicability, of robotic ankle exoskeletons.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distribution networks are important components of power and energy infrastructure. Today's distribution networks are vulnerable to different types of faults. To make the electric grid more resilient, it is important to detect and locate faults in distribution lines quickly and accurately. Impedance methods are the most common ways used to locate the faults. In this paper, first possible fault locations are identified using the improved impedance method. Then, its natural frequencies are determined. Since there are more than one possible fault locations, it is necessary to identify the fault section in order to determine the fault location accurately. The fault section and its main location are determined through simulating the fault in each section of possible locations. This is done by small steps and through accommodating the determined frequency to the existing natural frequency registered in the original fault voltage form. The proposed method is applied in Salim's 11-node network in which effect of fault inception angles, effect of different resistors and fault type are tested. Accuracy and efficiency of the proposed method has been verified.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper studies underdetermined direction of arrival (DOA) estimation accuracy by sum and difference composite co-array. We have studied to further enhance the DOF by combining the concepts of the sum co-array and the difference co-array, which is called \u201csum and difference composite coarray\u201d, and evaluated its beamforming performance. We aim at investigating if the sum and difference composite co-array also works well for underdetermined DOA estimation problems. Performance of the proposed array configuration is evaluated through computer simulation of DOA estimation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the e-commerce model that has online customer service, such as email or live chat, customers mostly use live chat because it is fast and comfortable. Thus, a company needs to hire and pay for admins. However, this incurs the problem that admins need to spend an extensive amount of time for writing an answer and customers have to wait for the answers. Several chatbots are available, but they require users to set up key phrases manually. In this article, we propose and develop a Frequently Asked Questions (FAQs) Chatbot which automatically responds to customers by using a Recurrent Neural Network (RNN) in the form of Long Short-Term Memory (LSTM) for text classification. The experimental results have shown that chatbot could recognize 86.36% of the questions and answer with 93.2% accuracy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet of things (IoT) devices come in various operating form factors. Some are operated on unconstrained resources by directly connecting to the electrical grid with Cloud Compute driven memory and processing capacities; others, operated on constrained resources by connecting to finite battery sources and limited memory and compute. Whatever the form factors are, importantly, the expectations from consumers are the IoT devices must be secured - both in terms of data and in terms of safety and efficiency. For securing IoT devices with unconstrained resources, there are many tools and compute technologies are available. On the other hand, Securing IoT devices with constrained resources, the options are few and pose huge challenges in terms of price, performance, and service costs. In this research paper, we propose machine learning enabled cognitive secure shield that secures the Dairy IoT devices operating under constrained resources. Our innovation is in the design of Secure shield framework that enhances security posture of our Dairy IoT device without affecting Useful Life of the device (ULD). Finally, the paper presents Secure shield ML prototyping.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a sparse direction of arrival (DOA) estimation method for directive coprime arrays. The complex radiation patterns are extracted using an electromagnetic simulator and then incorporated in the DOA estimation. Three DOA algorithms are compared including Capon, MUSIC, and Lasso based on compressive sensing (CS). It is shown that when the DOA increases in the elevation plane, the performance degrades faster when using real antenna patterns.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Online Teachers' Professional Development (TPD) has become an important way to enhance their professional capacity. However, it is difficult to evaluate teachers' performances in online TPD. Due to the complexity of online activities for teachers, there is no single standard to evaluate performance. However, online professional development platforms accumulate detailed data about teachers' online behavior, and this provides an important opportunity to find a solution to the problem of evaluation. In this paper, we suggest a model-building process to build a data model for online TPD. We describe the structure and process of building a teacher model for Ourteachers, one of the largest Online TPD platforms in China. Important issues critical to the model-building methodology are also discussed in this paper.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Space network which has important significance for the realization of national security strategy, consists of several satellites and ground stations. The satellites communicate with each other through high-speed laser links. One of the core components of satellite network is routers, which affects the overall performance of space network directly. After investigating the technology of high-performance router, we come up with topological structure of space network router. The main modules are optimized to reduce the latency in the router. In accordance to space harsh environment, fault tolerance and self-recovery design for space router are carried out. The space router with low-latency and high reliability meets the application requirements of space network. The main features of the router include: packet forwarding delay is less than 10us; data frames can be transmitted in multi-channels concurrently; single-channel fault does not spread and can be recovered autonomously.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We introduce a framework for dynamic evaluation of the fingers movements: flexion, extension, abduction and adduction. This framework estimates angle measurements from joints computed by a hand pose estimation algorithm using a depth sensor (Realsense SR300). Given depth maps as input, our framework uses Pose-REN [1], which is a state-of-art hand pose estimation method that estimates 3D hand joint positions using a deep convolutional neural network. The pose estimation algorithm runs in real-time, allowing users to visualise 3D skeleton tracking results at the same time as the depth images are acquired. Once 3D joint poses are obtained, our framework estimates a plane containing the wrist and MCP joints and measures flexion/extension and abduction/adduction angles by applying computational geometry operations with respect to this plane. We analysed flexion and abduction movement patterns using real data, extracting the movement trajectories. Our preliminary results show this method allows an automatic discrimination of hands with Rheumatoid Arthritis (RA) and healthy patients. The angle between joints can be used as an indicative of current movement capabilities and function. Although the measurements can be noisy and less accurate than those obtained statically through goniometry, the acquisition is much easier, non-invasive and patient-friendly, which shows the potential of our approach. The system can be used with and without orthosis. Our framework allows the acquisition of measurements with minimal intervention and significantly reduces the evaluation time.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, 3D action recognition has received more attention of research and industrial communities thanks to the popularity of depth sensors and the efficiency of skeleton estimation algorithms. Accordingly, a large number of methods have been studied by using either handcrafted features with traditional classifiers or recurrent neural networks. However, they cannot learn high-level spatial and temporal features of a whole skeleton sequence exhaustively. In this paper, we proposed a novel encoding technique to transform the pose features of joint-joint distance and joint-joint orientation to color pixels. By concatenating the features of all frames in a sequence, the spatial joint correlations and temporal pose dynamics of action appearance are depicted by a color image. For learning action models, we adopt the strategy of end-to-end fine-tuning a pre-trained deep convolutional neural networks to completely capture multiple high-level features at multi-scale action representation. The proposed method achieves the state-of-the-art performance on NTU RGB+D, the largest and most challenging 3D action recognition dataset, for both the cross-subject and cross-view evaluation protocols.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A Protocol for smart grid has been created for managing demand and supply as per customer needs and use of renewable energy. Markov process and Poisson's method are used for two-way communication and calculating average cost of energy production at main grid. The electricity generation at consumer end from renewable sources, is stored in batteries and used when required and send back to main grid if excess. The average cost of power production is calculated, and appropriate action is suggested to increase the efficiency of the entire system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Narrow Band Internet of Things (NB-IoT) is built on cellular network. It has the advantages of low power consumption, low cost, super coverage and large connection. Cell search is an essential component of NB-IoT and its main purpose is to obtain the Physical-layer Cell Identity. Since the cells of NB-IoT are not grouped, NB-IoT cell search is more complex than LTE systems. In this paper, we propose an effective method to reduce the complexity of NB-IoT cell search. Simulation results show that the detection accuracy reaches 80% with 10 radio frames, with a signal-to-noise ratio of -12.6dB.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In wireless sensor networks, DV-Hop localization algorithm which uses average hop distance to represent the actual distance is a commonly used range-free localization technology. But this algorithm has great error and node energy consumption in practical applications. Evolutionary algorithm has a branch which is differential evolution algorithm. DE algorithm has been widely used in a large number of fields, as a result of DE algorithm has simple structure and can combine with other methods easily. To solve the disadvantages of DV-Hop algorithm, an advanced DV-Hop localization algorithm on basic of differential evolution in this paper has been proposed. To reduce the hop distance error, the improved algorithm advances weighted process in the second step of DV-hop algorithm by leading the average hop distance error correction value. The differential evolution algorithm is used to optimize the positioning result of the unknown node in the third step of DV-hop algorithm. From the advanced localization algorithm's simulation, the positioning accuracy has improved.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distinguishing between classes of time series sampled from dynamic systems is a common challenge in systems and control engineering, for example in the context of health monitoring, fault detection, and quality control. The challenge is increased when no underlying model of a system is known, measurement noise is present, and long signals need to be interpreted. In this paper we address these issues with a new non parametric classifier based on topological signatures. Our model learns classes as weighted kernel density estimates (KDEs) over persistent homology diagrams and predicts new trajectory labels using Sinkhorn divergences on the space of diagram KDEs to quantify proximity. We show that this approach accurately discriminates between states of chaotic systems that are close in parameter space, and its performance is robust to noise.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a loop antenna for deep implant powering in an intracranial pressure (ICP) monitoring application. This work is continuation of our on-going development of a wirelessly powered implantable system for ICP monitoring. Experimental results indicate that the implant can be activated until the depth of 19 mm from the skin. This is 8 mm more than our previous work where we were able to activate the implant until 11 mm depth from the skin. Moreover, we have also presented pressure readout experimental results with the proposed antenna.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents the real time voltage sag monitoring system for voltage sag based on S-Transform analysis. The monitoring system is initially work on the extracting and analyzing the real time signal features which was conducted in LabVIEW software. The data acquisition module for voltage measurement has been used to acquire real time voltage sag event which generated by Three Phase Chroma Programming AC Source. From this data acquisition module, the S-transform provides the significant features of voltage sag for detecting the voltage sag. This technique is compared and validated with the theoretical types of voltage sag event in terms of magnitude and phase angle jump. The result indicates that S-transform algorithm is superior in providing an accurate analysis for voltage sag event for monitoring activity.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The prospects of achieving a trillion connected internet of things (IoT) devices by 2020 has created the urgency for effective intrusion detection systems (IDS) for these devices. Although it has been argued that the most effective technique used in such systems is anomaly detection, there exist no mechanisms to determine their performance in real-life deployment. In this paper, we report the results of applying asymptotic analysis to evaluate the performance of an anomaly detection algorithm which is designed using logic reasoning through fuzzy logic methodologies. In order to achieve this, the IDS was included as part of intrusion detection software for ZigBee Wireless Sensor Networks (WSNs). In particular, the solution is targeted to address the ZigBee protocol's vulnerability to flood attacks during node discovery and association to the network. The intrusion detection software is hosted external to the WSNs in pursue of a light solution mindful of resource preservation in sensor nodes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, there has been growing research into the field of Wi-Fi radar signal processing for dual radar-communication purposes. Combining these two features enables cost savings to vehicle manufacturers by lowering the design complexity, while also saving precious radio frequency spectrum. This work presents a feasibility study and a hardware implementation of a Doppler radar that operates on the IEEE 802.11p Wi-Fi packets. A 5-MHz OFDM modem that adheres closely to the 802.11p PHY was implemented on two Universal Software Defined Peripherals (USRP) via MATLAB's USRP toolbox. By applying the estimation of signal parameters via rotational invariance technique (ESPRIT) to a collection of received Wi-Fi symbols, the real-time Doppler radar was achieved, which demonstrated an average accuracy of sub-0.64 m/s in measuring a vehicle's velocity.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a novel method to detect unusual crowd behavior in a video sequence using probability models of speeds and directions. Thus, optical flow is used to extract velocities at each image frame, which are then reduced to speed and motion orientations. Using expectation maximization algorithm, we construct a mixture model of von Mises distribution describing the set of directions, and a mixture model of normal distribution related to the speed set. Each frame is compared with a collection of reference frames using distance of probability densities. This distance is then used to indicate changes in the crowd motion. Unlike the speed based detection, using the direction model is not yet adapted to the case of unstructured crowds. The proposed method was tested on various publicly available crowd datasets.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Human activity recognition is possible using small top K compressed personalized training dataset. However, unavailability of personalized dataset weakens the personalization of human activity. To address the shortage of personalized dataset an integrated training and testing algorithm was implemented. The algorithm extract and label top K time domain feature used to train the implemented hybrid Na\u00efve Bayes Nearest Neighbor algorithm to detect human activity in real time. The hybrid algorithm was implemented using HTML5, JavaScript and Cordova multi-platform for Android. Three subjects were recruited and requested to put in the Samsung Smartphone their front pocket as they train and test the model. Each subject was requested to perform human activities during training and testing. During testing each subject was requested to perform each human activity three times, therefore 21 comma separated values files were collected and analyzed using confusion matrix based on precision, recall and accuracy. Overall the results reveal a balanced precision, recall, f-measure and accuracy of 79%, 75%, 75% and 70% respectively for all subjects. Also the results indicate that it possible to train and test the data intensive algorithm with small training dataset.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is critical to obtain stability certificate before deploying reinforcement learning in real-world mission-critical systems. This study justifies the intuition that smoothness (i.e., small changes in inputs lead to small changes in outputs) is an important property for stability-certified reinforcement learning from a control-theoretic perspective. The smoothness margin can be obtained by solving a feasibility problem based on semi-definite programming for both linear and nonlinear dynamical systems, and it does not need to access the exact parameters of the learned controllers. Numerical evaluation on nonlinear and decentralized frequency control for large-scale power grids demonstrates that the smoothness margin can certify stability during both exploration and deployment for (deep) neural-network policies, which substantially surpass nominal controllers in performance. The study opens up new opportunities for robust Lipschitz continuous policy learning.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep neural networks (DNNs) have useful applications in machine learning tasks involving recognition and pattern analysis. Despite the favorable applications of DNNs, these systems can be exploited by adversarial examples. An adversarial example, which is created by adding a small amount of noise to an original sample, can cause misclassification by the DNN. Under specific circumstances, it may be necessary to create a selective untargeted adversarial example that will not be classified as certain avoided classes. Such is the case, for example, if a modified tank cover can cause misclassification by a DNN, but the bandit equipped with the DNN must misclassify the modified tank as a class other than certain avoided classes, such as a tank, armored vehicle, or self-propelled gun. That is, selective untargeted adversarial examples are needed that will not be perceived as certain classes, such as tanks, armored vehicles, or self-propelled guns. In this study, we propose a selective untargeted adversarial example that exhibits 100% attack success with minimum distortions. The proposed scheme creates a selective untargeted adversarial example that will not be classified as certain avoided classes while minimizing distortions in the original sample. To generate untargeted adversarial examples, a transformation is performed to minimize the probability of certain avoided classes and distortions in the original sample. As experimental datasets, we used MNIST and CIFAR-10, including the Tensorflow library. The experimental results demonstrate that the proposed scheme creates a selective untargeted adversarial example that exhibits 100% attack success with minimum distortions (1.325 and 34.762 for MNIST and CIFAR-10, respectively).", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Proteins are an important part of the organism and are involved in almost every process in the cell. Many proteins are biochemical enzymes and important to the process of metabolism. This paper proposes protein localization sites in cells. Comparison of learning algorithms such as Logitboost, Tree Decision, Binary Decision Tree, Bayesian Classifier methods. To use as a guideline to improve the best data analytic model. The efficiency and accuracy of the yeast data set are used. The learning algorithms are used to measure performance and finding the value of the data set. This paper used the principle of data mining in segmentation. In this case, there are five classes and nine attributes, respectively, of 921 records. The analysis system of the data is done by correctly classified instances and incorrectly classified instances. The experimental results show that the Logitboost technique gave the highest accuracy rate at 65.65%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a method for pose estimation of unknown motion target by integrating the improved iterative closest point (ICP) algorithm and the adaptive extended Kalman filter (AEKF). First, the point cloud of scene is obtained by stereo vision. Then, the target is extracted based on color and depth information. Last, the point clouds of adjacent frames are registered by the improved ICP-AEKF in closed loop configuration in order to obtain relative pose parameters between the target and stereo camera. The physical experiment is also conducted under the ZED binocular camera, which shows that the performance of the improved ICP-AEKF is much better than the traditional ICP in accuracy, convergence speed and robustness.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Existing techniques for machine learning and data mining have shortcomings in handling data of different types, data without a priori knowledge of data dependence, and data with variable relations varying in different value ranges. This paper illustrates how the Partial-Value Association Discovery (PVAD) algorithm overcomes shortcomings of existing machine learning and data mining techniques. The paper also demonstrates how the PVAD algorithm was used to analyze engineering student data and computer network data for identifying characteristics of engineering retention and network traffic normalcy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose a novel inverse tone mapping network, called \u201ciTM-Net.\u201d For training iTM-Net, we also propose a novel loss function that considers the non-linear relation between low dynamic range (LDR) and high dynamic range (HDR) images. For inverse tone mapping with convolutional neural networks (CNNs), we first point out that training CNNs with a standard loss function causes a problem due to the non-linear relation between the LDR and HDR images. To overcome the problem, the novel loss function non-linearly tone-maps target HDR images into LDR ones on the basis of a tone mapping operator, and the distance between the tone-mapped images and predicted ones are then calculated. The proposed loss function enables us not only to normalize the HDR images but also to reduce the non-linear relation between LDR and HDR ones. The experimental results show that the HDR images predicted by the proposed iTM-Net have higher-quality than the HDR ones predicted by conventional inverse tone mapping methods, including the state of the art, in terms of both HDR-VDP-2.2 and PU encoding + MS-SSIM. In addition, compared with loss functions that do not consider the non-linear relation, the proposed loss function is shown to improve the performance of CNNs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapid development of intelligent communication systems, classical problems, such as automatic modulation classification (AMC), have gained extensive research interest. This is due to the significant role that AMC plays in many civilian and military applications. In this letter, we consider AMC for millimeter wave-over-fiber (MMWoF) communication. This type of communication is of practical interest because it enables centralized analysis and processing, taking the advantages of low transmission loss of MMW signals over fiber optic channels. In this letter, we use autoencoder neural networks for automatic features extraction and classification, preceded by a pre-processing step applied to the samples of the input signal. The performance of the system under consideration has been thoroughly investigated by simulation and verified experimentally under different impairments, including fiber chromatic dispersion and amplified spontaneous emission noise. The results are presented in terms of the probability of correct classification for different values of optical signal-to-noise ratio and different lengths of fiber channels. The results from simulation are in good match to those obtained experimentally.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently cyber-bullying and online harassment have become two of the most serious issues in many public online communities. In this paper, we use data from Wikipedia talk page edits to train multi-label classifier that detects different types of toxicity in online user-generated content. We present different data augmentation techniques to overcome the data imbalance problem in the Wikipedia dataset. The proposed solution is an ensemble of three models: convolutional neural network (CNN), bidirectional long short-term memory (LSTM) and bidirectional gated recurrent units (GRU). We divide the classification problem into two steps, first we determine whether or not the input is toxic then we find the types of toxicity present in the toxic content. The evaluation results show that the proposed ensemble approach provides the highest accuracy among all considered algorithms. It achieves 0.828 F1-score for toxic/non-toxic classification and 0.872 for toxicity types prediction.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Dialogue Act classification is a relevant problem for the Natural Language Processing field either as a standalone task or when used as input for downstream applications. Despite its importance, most of the existing approaches rely on supervised techniques, which depend on annotated samples, making it difficult to take advantage of the increasing amount of data available in different domains. In this paper, we briefly review the most commonly used datasets to evaluate Dialogue Act classification approaches and introduce the Optimum-Path Forest (OPF) classifier to this task. Instead of using its original strategy to determine the corresponding class for each cluster, we use a modified version based on majority voting, named M-OPF, which yields good results when compared to k-means and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN), according to accuracy and V-measure. We also show that M-OPF, and consequently OPF, are less sensitive to hyper-parameter tuning when compared to HDBSCAN.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The wireless body area network (WBAN) has attracted considerable attention and becomes a promising approach to provide a 24-h on-the-go healthcare service for users. However, it still faces many challenges on the privacy of users' sensitive personal information and the confidentiality of healthcare center's disease models. For this reason, many privacy-preserving schemes have been proposed in recent years. However, the efficiency and accuracy of those privacy-preserving schemes become a big issue to be solved. In this paper, we propose an efficient and privacy-preserving priority classification scheme, named PPC, for classifying patients' encrypted data at the WBAN-gateway in a remote eHealthcare system. Specifically, to reduce the system latency, we design a non-interactive privacy-preserving priority classification algorithm, which allows the WBAN-gateway to conduct the privacy-preserving priority classification for the received users' medical packets by itself and to relay these packets according to their priorities (criticalities). A detailed security analysis shows that the PPC scheme can achieve the priority classification and packets relay without disclosing the privacy of the users' personal information and the confidentiality of the healthcare center's disease models. In addition, the extensive experiments with an android app and two java server programs demonstrate its efficiency in terms of computational costs and communication overheads.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An improved wood texture feature description algorithm of Local Binary Pattern operator is proposed. Firstly, the linear LBP operator is used to extract the texture features of the image. Then, the similarity between textures is calculated for the sub-region of the wood image. Because of the extremely high texture similarity of the striped wood, this method can be used to quickly and accurately distinguish the two types of images of stripes and patterns. Experimental results show that the algorithm has strong ability to describe wood texture features, is more robust than traditional methods, and has higher recognition accuracy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Quantum particle swarm optimization (QPSO), inspired from the basic concept of PSO algorithm and quantum theory, is a stochastic searching algorithm. However, the algorithm may encounter a premature convergence when dealing with multimodal and complex inverse problems. Thus, some improvements are introduced. More especially, one will randomly select the best particle to take part in the current search domain. Also, a mutation strategy is added to the mean best position, and an enhancement factor (EF) is incorporated to enhance the global search capability to find the global optimum solution and to avoid premature convergence. Moreover, some parameter updating strategy is proposed to tradeoff the exploration and exploitation searches. Experiments have been conducted on well-known multimodal functions and an inverse problem. The numerical results showcase the merit and efficiency of the proposed modified quantum inspired particle swarm optimizer (MQPSO).", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Emotion Classification using lexicons has vast number of applications ranging from social media analysis to pervasive computing. Lexicons are usually hand-crafted and cost a lot of time and effort to generate. The major research challenge in this area is the creation of a generalized lexicon which serves well for every domain. This work focuses on automatic expansion of emotion lexicons to ease the process of domain adaption. Our proposed approach - CB-Lex - relies on a seed lexicon and an unlabeled corpus from the target domain. In experimental results, our expanded lexicons show an improvement of over 6% in F-Measure.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Information in the form of text can be found in abundance in the web today, which can be mined to solve multifarious problems. Customer reviews, for instance, flow in across multiple sources in thousands per day which can be leveraged to obtain several insights. Our goal is to extract cases of a rare event e.g., recall of products, allegations of ethics or, legal concerns or, threats to product-safety, etc. from this enormous amount of data. Manual identification of such cases to be reported is extremely labour-intensive as well as time-sensitive, but failure to do so can have fatal impact on the industry's overall health and dependability; missing out on even a single case may lead to huge penalties in terms of customer experience, product liability and industry reputation. In this paper, we will discuss classification through Positive and Unlabeled data, PU classification, where the only class, for which instances are available, is a rare event. In iCASSTLE, we propose a two-staged approach where Stage I leverages three unique components of text mining to procure representative training data containing instances of both classes in the right proportion, and Stage II uses results from Stage I to run a semi-supervised classification. We applied this to multiple datasets differing in nature of Product Safety as well as nature of imbalance and iCASSTLE is proven to perform better than the state-of-the-art methods for the relevant use-cases.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is crucial to implement an effective and accurate fault diagnosis of a gearbox for mechanical systems. However, being composed of many mechanical parts, a gearbox has a variety of failure modes resulting in the difficulty of accurate fault diagnosis. Moreover, it is easy to obtain raw vibration signals from real gearbox applications, but it requires significant costs to label them, especially for multi-fault modes. These issues challenge the traditional supervised learning methods of fault diagnosis. To solve these problems, we develop an active learning strategy based on uncertainty and complexity. Therefore, a new diagnostic method for a gearbox is proposed based on the present active learning, empirical mode decomposition-singular value decomposition (EMD-SVD) and random forests (RF). First, the EMD-SVD is used to obtain feature vectors from raw signals. Second, the proposed active learning scheme selects the most valuable unlabeled samples, which are then labeled and added to the training data set. Finally, the RF, trained by the new training data, is employed to recognize the fault modes of a gearbox. Two cases are studied based on experimental gearbox fault diagnostic data, and a supervised learning method, as well as other active learning methods, are compared. The results show that the proposed method outperforms the two common types of methods, thus validating its effectiveness and superiority.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The security system in the room requires remote monitoring, one of the implementations is using the web that can be accessed via a smartphone or laptop. In order to realize a web server, the raspberry pi is needed as the low power consume device. In this study, monitoring system a local network based on the IoT technology (Internet of Things) where an object has the ability to send data over the network without requiring human-to-human or human interaction to the computer. Raspberry pi is used as a web server and is installed by Motion eye as a web interface that is integrated with a webcam that functions as a sensor. Furthermore, the client monitor in the room through the web browser by streaming with define the URL according to the IP address obtained by the raspberry pi from the server. The observations of the systems have been addressed on the number users, number of frames per second and the resolution of the object related to the delay, the throughput, the delay and the CPU performance. The simulations have been defined when the number of user 30 and 70, the resolution of the object was 320 \u00d7 176, 320 \u00d7 240, 640 \u00d7 480 and 1024 \u00d7 786, the number of frames per second were 10 fps, 20 fps and 30 fps. The results show that IoT technology have possibility to be implemented for CCTV monitoring using Raspberry Pi. The examining on the CPU usage shows that higher both the resolution and fps on the video results affect the CPU usage performances. However, the video quality becomes better.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A natural basis for the detection of a wireless random reactive jammer (RRJ) is the perceived violation by the detector [typically located at the access point (AP)] of the carrier sensing protocol underpinning many wireless random access protocols (e.g., Wi-Fi). Specifically, when the wireless medium is perceived by a station to be busy, a carrier sensing compliant station will avoid transmission, while an RRJ station will often initiate transmission. However, hidden terminals (HTs), i.e., activity detected by the AP but not by the sensing station, complicate the use of carrier sensing as the basis for RRJ detection since they provide plausible deniability to a station suspected of being an RRJ. The RRJ has the dual objectives of avoiding detection and effectively disrupting communication, but there is an inherent performance tradeoff between these two objectives. In this paper, we capture the behavior of both the RRJ and the compliant stations via a parsimonious Markov chain model and pose the detection problem using the framework of the Markov chain hypothesis testing. Our analysis yields the receiver operating characteristic (ROC) of the detector and the optimized behavior of the RRJ. While there has been extensive work in the literature on jamming detection, our innovation lies in leveraging carrier sensing as a natural and effective basis for detection.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Mobile Ad-hoc Networks (MANETs) are often visible to the exposed terminal problem and hidden terminal problem, which exist due to non-transitivity in media access control schemes. This affects the utilization of channel and throughput in media access control protocols, e.g., dual busy tone multiple access protocol (DBTMA). Hence, to improve the fairness and throughput performance of the DBTMA it is very necessary to address the problems associated with hidden and exposed terminals. Hence, in this paper, the quality of service (QoS) is improved by enhancing the capability of the DBTMA for better network service in the MANETs. The proposed method uses an improved DBTMA called Retransmission Dual Busy Tone Multiple Access (RDBTMA) protocol. This is based on two elements namely: busy tones and Ready To Send/Clear to Send (RTS/CTS) dialogues. In addition to this fast retransmission, a strategy is used further to improve its effectiveness. The retransmission strategy is adopted using negative acknowledgment after the collision occurred by the hidden nodes. A hidden node, where the collision occurs at access point, listens to the NACK signal and uses the signal to determine the requirement fast retransmission scheme. The proposed method is simulated and compared against existing methods in terms of various network parameters. The results show that the proposed RDBTMA protocol is effective in terms of the improved QoS in terms of network throughput (21.9%), packet delivery ratio (17.8%), 14.9% less packet loss, and 38% less route discovery delay than the existing methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The ever-growing proliferation of mobile devices equipped with accelerometers has provided new opportunities to capture the semantic meanings of human activities and improve user experience with behavior-based recommendations, which heavily rely on the accuracy of the recognition of daily human activities. Acceleration-based human activity recognition (HAR) is a challenging problem because each accelerometer records multi-dimensional signals in both spatial and temporal domains that have different attributes for representing different activities or even the same activity. Thus we cannot directly compare these signals with each other, because they are embedded in a non-metric space. In this paper, we present a Personalized Recurrent Neural Network (PerRNN) to dynamically segment and recognize the human activities based on accelerometer data. Enlightened by the idea of spatiotemporal predictive learning, the proposed architecture is capable of memorizing different acceleration signals' appearances and temporal variations in a unified memory pool. We evaluate the performance of the proposed framework on a commonly used dataset, WISDM. Experiment results show that compared with state-of-the-art schemes, our proposed PerRNN system recognizes 6 different human activities with the highest overall accuracy of 96.44%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The identification of individual household appliances in the residential power grid can provide better consumption control and detection of anomalies present in some of these appliances. This identification is only possible if each electric appliance has an Electric Load Signature (ELS). The generation of ELS occurs through the Internet of Things (IoT) equipment, such as Smart Meter (SM) or Smart Plugs (SPs), which provides information necessary for this purpose. The proposed work allows the reading and detection of residential household appliances in the network, through the individual ELS of each load, using SPs together with the Machine Learning Algorithm. Some important electrical parameters will be analyzed and detected individually. With the aid of the Decision Tree and Naive Bayes algorithms, the creation data of each ELS is stored in a centralized database present in the Home Energy Management System and trained so that the identification in each SP is possible. A visual application is provided to the consumer at the HEM to be able to see which appliances are operating, consumption history, as well as anomalies and unwanted changes, present in the residential network.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper describes an approach of creating a system identifying fruit and vegetables in the retail market using images captured with a video camera attached to the system. The system helps the customers to label desired fruits and vegetables with a price according to its weight. The purpose of the system is to minimize the number of human computer interactions, speed up the identification process and improve the usability of the graphical user interface compared to existing manual systems. The hardware of the system is constituted by a Raspberry Pi, camera, display, load cell and a case. To classify an object, different convolutional neural networks have been tested and retrained. To test the usability, a heuristic evaluation has been performed with several users, concluding that the implemented system is more user friendly compared to existing systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Objective: Deep learning has recently been applied to electrical impedance tomography (EIT) imaging. Nevertheless, there are still many challenges that this approach has to face, e.g., targets with sharp corners or edges cannot be well recovered when using circular inclusion training data. This paper proposes an iterative-based inversion method and a convolutional neural network (CNN) based inversion method to recover some challenging inclusions such as triangular, rectangular, or lung shapes, where the CNN-based method uses only random circle or ellipse training data. Methods: First, the iterative method, i.e., bases-expansion subspace optimization method (BE-SOM), is proposed based on a concept of induced contrast current (ICC) with total variation regularization. Second, the theoretical analysis of BE-SOM and the physical concepts introduced there motivate us to propose a dominant-current deep learning scheme for EIT imaging, in which dominant parts of ICC are utilized to generate multi-channel inputs of CNN. Results: The proposed methods are tested with both numerical and experimental data, where several realistic phantoms including simulated pneumothorax and pleural effusion pathologies are also considered. Conclusions and Significance: Significant performance improvements of the proposed methods are shown in reconstructing targets with sharp corners or edges. It is also demonstrated that the proposed methods are capable of fast, stable, and high-quality EIT imaging, which is promising in providing quantitative images for potential clinical applications.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Currently, there is a lack of tools for real validation of 5G scenarios. The increasing traffic demand of 5G networks is pushing network operators to find new cost-efficient solutions. The selected solution is a multi-tenancy approach that, together with user mobility will impose some architectural changes. This approach increases service dynamism making it necessary to have tools that provide these new capabilities to be able to validate each development. This work presents a novel experimentation framework for the emulation of 5G scenarios providing them with real-time user mobility and multi-tenancy. The functionality of this novel framework has been validated through different experiments.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Honeypots have become an important tool for capturing attacks. Hybrid honeypots, including the front end and the back end, are widely used in research because of the scalability of the front end and the high interactivity of the back end. However, traditional hybrid honeypots have some problems that the flow control is difficult and topology simulation is not realistic. This paper proposes a new architecture based on SDN applied to the hybrid honeypot system for network topology simulation and attack traffic migration. Our system uses the good expansibility and controllability of the SDN controller to simulate a large and realistic network to attract attackers and redirect high-level attacks to a high-interaction honeypot for attack capture and further analysis. It improves the deficiencies in the network spoofing technology and flow control technology in the traditional honeynet. Finally, we set up the experimental environment on the mininet and verified the mechanism. The test results show that the system is more intelligent and the traffic migration is more stealthy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Edge detection is a fundamental problem in computer vision community. In this paper, we propose a novel concept for edge detection called Structural Edge. The Structural edges include occluding contours of objects as well as orientation discontinuities in surfaces that define the 3D structure of objects and their environments. This contrasts the semantic edge which is only the boundary between semantic areas. While existing edge detection methods focus on either semantic boundaries or low-level gradients, we focus on the structural edge. To achieve that, in this paper, we propose the structural edge dataset along with a benchmark. The structural edge dataset contains 600 images of natural indoor and outdoor scenes. The structural edges are labeled manually and validated by eye-tracking data from 10 participants with overall 20 trials. Later, we use the dataset to benchmark the existing edge detection methods. We benchmark both the learning based and non-learning based methods and draw the conclusion that existing methods cannot fully solve the structural edge detection. We encourage new research to exploit the proposed task.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper considers the transceiver design for uplink massive multi-input multi-output (MIMO) systems with channel sparsity. Recent progress has shown that sparsity learning-based blind signal detection is able to retrieve the channel and data by using message passing based noisy matrix factorization. We propose a semi-blind signal detection scheme in which a short pilot sequence is introduced to each user packet and the knowledge of pilots is integrated into the message passing algorithm for noisy matrix factorization. We show that our semi-blind signal detection scheme substantially outperforms the state-of-the-art blind detection and training-based schemes in the short-pilot regime.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless Sensor Network (WSN) is an emerging next-generation sensor network that has a wide range of application prospects. The localization technology is one of the most important key technologies for WSN. However, in a complex indoor environment, fluctuations in received signal strength can seriously degrade positioning accuracy. In this paper, we propose a fingerprint localization method based on received signal strength (RSS) distance and improved weighted k-Nearest Neighbor (KNN) algorithm. The fingerprint database is established in the off-line phase. The real-time RSS values of the on-line measurement points are measured, and the two-stage RSS distance is calculated using the Euclidean distance. Finally, in order to solve the problem of non-Gaussian distribution of measurement noise, we use an improved weighted KNN algorithm to calculate the final position coordinates of the measurement point. Simulation results show that this method can reduce the influence of signal strength fluctuations and improve the positioning accuracy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a sensorless control strategy using sine-wave high-frequency (HF) voltage injection for three-phase four-switch (TPFS) inverter fed interior permanent magnet synchronous motors (IPMSMs). The principle of TPFS inverter fed IPMSM drive system is studied, and then a nonorthogonal-nonlinear k-l axis coordinate system is proposed for sector identification and voltage projection. Besides, the TPFS inverter fed sensorless control strategy is proposed, where the position estimation scheme is deduced, and the equation of the injected HF voltage in the proposed k-l axis coordinate system is given. The effectiveness of the proposed scheme is verified by simulation results in Matlab/Simulink and experimental results on a 5kW IPMSM motor prototype, which shows that estimated values track the real ones well in different working conditions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents the algorithm and VLSI architecture of a highly efficient Motion Estimation (ME) for High Efficiency Video Coding (HEVC) systems. To be specific, this paper proposes an Adaptive Fast Search Algorithm where the search space is adaptive to the characteristics of the video and the number of search candidates is greatly reduced. The experimental results show that, compared to the conventional approaches, this algorithm reduces the computational complexity by 54% with a marginal 2.01% performance degradation. Furthermore, the VLSI architecture and circuit implementation of the proposed ME engine is presented in this paper. The architectural and circuit-level optimizations for enhancing the throughput and reducing the complexity are illustrated. The proposed ME system is underwent the ASIC design flow and realized based on the Xilinx Zynq UltraScale+ FPGA platform. The results show that the, for ASIC and FPGA, the implemented ME achieves 60 frames per second (fps) and 30 fps with resolution of 3840\u00d72160 respectively. The efficiency is also significantly enhanced.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To tackle the problem of target scale changed, too monotonous target feature, or track cumulative errors in Kernelized correlation filters(KCF), the paper proposes a self-adaptive KCF tracking algorithm employed multi-feature fusion. KCF tracking algorithm is improved based on location prediction, multi-feature fusion and bilinear interpolation. Among them, to facilitate better representation of the target's appearance model, make target tracking more robust, the multi-feature fusion is integrated (Hue Saturation Value, HSV) color features, grayscale features and improved (Histogram of Oriented Gradient, HOG) features. Both qualitative and quantitative evaluations on some object tracking benchmark show that the proposed tracking method achieves superior performance compared with other state-of-the-art methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep neural networks can obtain effective hierarchical representations, which make them perform well in image recognition and speech recognition. However, the disadvantages of deep learning relying on large-scale annotation data also limit its development. To this end, a multi-layer convolutional neural network based on knowledge graph prior knowledge is proposed in this paper, which acquires text features from finer granularity, and using the existing prior knowledge which increases granularity information to enhance the semantic information of the text, and to reduce the dependence of the model on large-scale samples. At the same time, it can reduce the over-fitting problem of the model, and improve classification accuracy. This model performs well in several large data sets. By introducing prior knowledge to TextCNN, a classical deep neural model, it has also verified the validity of using prior knowledge through experiments.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As Internet of Things(IoT) is actively researched and used, it is also emphasizing the need for IoT standards. IoT is an essential element to establish Cyber Physical System(CPS) of Smart Factory. Therefore, the IoT system of smart factory should be established so that the IoT system complies with the IoT standard. Container is a virtualization concept that benefits from rapid deployment, flexibility, stability and manageability. This paper proposes an architecture that utilizes container concepts in smart factory environments that comply with IoT standards.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In real-time applications, a fast and robust visual tracker should generally have the following important properties: 1) feature representation of an object that is not only efficient but also has a good discriminative capability and 2) appearance modeling which can quickly adapt to the variations of foreground and backgrounds. However, most of the existing tracking algorithms cannot achieve satisfactory performance in both of the two aspects. To address this issue, in this paper, we advocate a novel and efficient visual tracker by exploiting the excellent feature learning and classification capabilities of an emerging learning technique, that is, extreme learning machine (ELM). The contributions of the proposed work are as follows: 1) motivated by the simplicity and learning ability of the ELM autoencoder (ELM-AE), an ELM-AE-based feature extraction model is presented, and this model can provide a compact and discriminative representation of the inputs efficiently and 2) due to the fast learning speed of an ELM classifier, an ELM-based appearance model is developed for feature classification, and is able to rapidly distinguish the object of interest from its surroundings. In addition, in order to cope with the visual changes of the target and its backgrounds, the online sequential ELM is used to incrementally update the appearance model. Plenty of experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Benefit estimation is one of the key components for introducing Advanced Process Control (APC) / Multi-variable Predictive Control (MVPC) / Model Predictive Control (MPC) to a process, as the cost associated with it have to be justified in economic terms. The conventional approach to estimate benefit is based on the assumption of percentage reduction in the standard deviation of key controlled process variables, which comes from the experience and process knowledge. The conventional technique is found to be ineffective because of the uncertainty associated with it. In this research, an approach was made to develop a novel method to numerically estimate percentage reduction in the standard deviation of key controlled process variables by modelling the disturbance(s) with application to APC such that we need not assume the reduction in standard deviation while basic equation remains same as in conventional approach. The effectiveness of the proposed method is justified by implementing it in MATLAB on the real process plant data of Delayed Coker Unit (DCU) in Petrochemical refinery which experiences cyclic disturbance(s). The simulation is done by two ways, one is directly injecting the disturbance data & other is by characterizing/modelling the disturbance pattern and both the results are found to be very close. These two results were further verified by comparing with real plant data after APC has been implemented.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An experimental data analysis to investigate the output performance of thermoelectric generator (TEG) is presented in this paper. Thermally conducting materials, Al, Fe and matte finish metal paint, are tested to study the increase in heat absorbance by TEG. An experimental thermoelectric set-up comprising Peltier cell (TEC1-12706), commercially available, is designed to convert the ambient solar energy from sun into electrical energy suitable for powering micro/small power devices. The analysis of open circuit output DC voltages shows that paint coated Al sheet increases the output voltage considerably. A matte finish paint coated Al sheet mounted on hotter side of TEG is generating a maximum voltage of 0.76 V. An array of three such TEG modules generated a maximum voltage value 2.28 V, sufficient to power up low and ultra low power wireless sensor device and modules. The present set-up provides promising alternative to power low power devices like WSN, hearing aids, wireless temperature sensor etc. instead of using battery thereby enhancing their lifetime.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes an outliers-robust constant false-alarm rate (OR-CFAR) detector of Gaussian clutter based on the truncated-maximum-likelihood estimator (TMLE) in SAR imagery. The proposed method aims at elevating the detection performance in multiple-target environment, where the sea clutter samples are often contaminated by the interfering target pixels, the azimuth ambiguities, and the breakwater. As a consequence, the parameters used for statistical modeling are over-estimated, resulting in a degradation of the CFAR detection rate. Inspired by the traditional two-parameter CFAR (TP-CFAR) detector of Gaussian clutter, OR-CFAR designs an adaptive threshold-based clutter truncation method to eliminate the high-intensity outliers from the clutter samples in the local reference window, and the probability density function (PDF) of the sea clutter can be accurately modeled through the newly raised TMLE. Furthermore, the optimal truncation depth used for clutter truncation and PDF modeling is evaluated and selected properly to get the best detection results. The OR-CFAR greatly enhances the CFAR detection rate in multiple-target environment, and it is computationally simple and efficient, which has a great application value. The Chinese Gaofen-3 SAR data are used for experiments to show the better detection performance of OR-CFAR.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, data-aided sensing as a cross-layer approach in Internet-of-Things (IoT) applications is studied, where multiple IoT nodes collect measurements and transmit them to an access point (AP). It is assumed that measurements have a sparse representation (due to spatial correlation) and the notion of compressive sensing (CS) can be exploited for efficient data collection. For data-aided sensing, a node selection criterion is proposed to efficiently reconstruct a target signal through iterations with a small number of measurements from selected nodes. Together with compressive random access (CRA) to collect measurements from nodes, compressive transmission request is proposed to efficiently send a request signal to a group of selected nodes. Error analysis on compressive transmission request is carried out and the impact of errors on the performance of data-aided sensing is studied. Simulation results show that data-aided sensing allows to reconstruct the target information with a small number of active nodes and is robust to nodes' decision errors on compressive transmission request.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a progressive leaning approach to separating child speech from signals with mixed adult speech in a speaker-independent manner based on a densely connected long short-term memory (LSTM) architecture to deal with limited training data issue in child speech. First, by measuring the speech dissimilarities between children and adults using i-vectors, we demonstrate that distances between child and adult speech are large enough to warrant a possible separation through establishing child and adult speech groups. Accordingly, we present a novel LSTM design with densely connected hidden layers and stacked inputs containing progressively obtained intermediate targets that are learnt via multiple-target learning for speech separation between child and adult groups. Experimental results on a simulation corpus show that the proposed framework can yield consistent and significant gains of objective measures over the LSTM baseline for child speech separation. Further-more, our preliminary results on the SeedLing corpus with realistic recordings for child language acquisition show that our approach can achieve better overall separation performances than LSTM baseline when comparing spectrograms of separated speech, implying a potential for speaker diarization involving child speech.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing is transforming some biological systems by providing asset combinations that incorporate ownership, simple layout, configuration, quantification, and computerization. This shift in perspective raises many security and protection issues that need to be addressed. These are some of the main challenges of the cloud computing environment: measuring trust, multi-tenure, loss of administration. This document reviews current advances and offers a broad range of cloud security and assurance solutions. In addition, this article discusses information security issues in distributed computing and addresses these security issues.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposed a method for motion deblurring by estimating parameters length and angle in Point Spread Function (PSF) with the gradient descent method based on an absolute gradient magnitude. These parameters are used to restore the blurred image to be sharp by using wiener filter. The experimental results show that the proposed method is effective for estimating parameters and improves a quality of the blurred image. Peak Signal to Noise Ratio (PSNR) and Structural Similarity index (SSIM) are applied to test the efficiency of the method.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapid development of information technology, video surveillance system has become a key part in the security and protection system of modern cities. Especially in prisons, surveillance cameras could be found almost everywhere. However, with the continuous expansion of the surveillance network, surveillance cameras not only bring convenience, but also produce a massive amount of monitoring data, which poses huge challenges to storage, analytics and retrieval. The smart monitoring system equipped with intelligent video analytics technology can monitor as well as pre-alarm abnormal events or behaviours, which is a hot research direction in the field of surveillance. This paper combines deep learning methods, using the state-of-the-art framework for instance segmentation, called Mask R-CNN, to train the fine-tuning network on our datasets, which can efficiently detect objects in a video image while simultaneously generating a high-quality segmentation mask for each instance. The experiment show that our network is simple to train and easy to generalize to other datasets, and the mask average precision is nearly up to 98.5% on our own datasets.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Data Science has assembled researchers from multiple fields, such as computer science and marketing, to develop methods, algorithms and techniques to discover knowledge from large amounts of data and solve complex problems. The main focus is to find patterns hidden within the data. In general, these patterns must appear frequently to be learned using machine algorithms and reflect the standard, even if tacit, knowledge that guides decisions in the world. The uncovered knowledge may also reflect societal prejudice that will be enforced by the machines. Nevertheless, in many domains, specially when the negative impacts of bad decisions have a high cost, a few instances of patterns within the dataset suffice to warrant further investigation. Considering that decisions will be based on knowledge learned from the data, the challenge lies in determining when fewer appearances of a sequence of events are more important than very frequent patterns. In this context, we have devised an approach that uses a domain ontology to boost these infrequent, but relevant, events. In addition to guiding the search for relevant knowledge, the ontology helps users accept results and further investigate the data. It enables users to create data subsets and views, deriving new attributes from existing ones, guiding the data mining process, and providing a background layer from which even not so frequent patterns stand out and become meaningful. In this paper, we present our ontology-based data discovery approach, a system developed according to it, and preliminary results of a real life application in the oil production domain.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The last few years have seen a radical shift in the cyber defense paradigm from reactive to proactive, and this change is marked by the steadily increasing trend of Cyber Threat Intelligence (CTI) sharing. Currently, there are numerous Open Source Intelligence (OSINT) sources providing periodically updated threat feeds that are fed into various analytical solutions. At this point, there is an excessive amount of data being produced from such sources, both structured (STIX, OpenIOC, etc.) as well as unstructured (blacklists, etc.). However, more often than not, the level of detail required for making informed security decisions is missing from threat feeds, since most indicators are atomic in nature, like IPs and hashes, which are usually rather volatile. These feeds distinctly lack strategic threat information, like attack patterns and techniques that truly represent the behavior of an attacker or an exploit. Moreover, there is a lot of duplication in threat information and no single place where one could explore the entirety of a threat, hence requiring hundreds of man hours for sifting through numerous sources - trying to discern signal from noise - to find all the credible information on a threat. We have made use of natural language processing to extract threat feeds from unstructured cyber threat information sources with approximately 70\\% precision, providing comprehensive threat reports in standards like STIX, which is a widely accepted industry standard that represents CTI. The automation of an otherwise tedious manual task would ensure the timely gathering and sharing of relevant CTI that would give organizations the edge to be able to proactively defend against known as well as unknown threats.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a novel approach is presented to solve the trajectory tracking problem for autonomous vehicles. This approach is based on the use of a cascade control where the external loop solves the position control using a novel Takagi Sugeno-Model Predictive Control (TS-MPC) approach and the internal loop is in charge of the dynamic control of the vehicle using a Takagi Sugeno-Linear Quadratic Regulator technique designed via Linear Matrix Inequalities (TS-LMI-LQR). Both techniques use a TS representation of the kinematic and dynamic models of the vehicle. In addition, a novel Takagi-Sugeno estimator-Moving Horizon Estimator-Unknown Input Observer (TS-MHE-UIO) is presented. This method estimates the dynamic states of the vehicle optimally as well as the force of friction acting on the vehicle that is used to reduce the control efforts. The innovative contribution of the TS-MPC and TS-MHE-UIO techniques is that using the TS model formulation of the vehicle allows us to solve the nonlinear problem as if it were linear, reducing computation times by 10-20 times. To demonstrate the potential of the TS-MPC, we propose a comparison between three methods of solving the kinematic control problem: Using the nonlinear MPC formulation (NL-MPC) with compensated friction force, the TS-MPC approach with compensated friction force, and TS-MPC without compensated friction force.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: While context information of a smartphone user such as his/her current location, the number of companions, mental conditions, and health conditions can be fundamental information towards smarter services, it is not easy to estimate these information with built in smartphone sensors. This paper develops a user context estimation model based on mobile application usage histories. As user application history and context are assumed to be time series data that change over time, a recurrent neural network was employed to learn temporal relationship between the history and context. The experimental results confirmed that the proposed model was effectively able to estimate user context from application usage history.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: At present, the backbone communication network construction, management and operation and maintenance system has been relatively perfect, and the communication access network has more access methods and communication network service quality differences. The distribution service is closely related to power production, and communication reliability and real-time High performance requirements, strict requirements for data transmission delay and loss, but data traffic (except video services) is generally small. The distribution service directly serves the production scheduling, which has strict requirements on communication failure rate and failure repair time, and has higher requirements on operation and maintenance management. The distribution service is located in the production control area and has high requirements for information security. The communication channel is required to have a high level of security protection.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud manufacturing (CM) is an open and service-oriented platform that virtualizes distributed design, machining, and assembly resources together in order to provide a seamless, adaptive, and high quality transaction of manufacturing procedures. Despite the results in resource discovery and planning, the research in robustness and security of the systems falls behind in many aspects. The lack of such knowledge puts a serious challenge for the wide deployment and adoption of cloud manufacturing which is naturally connected to the Internet and exposed to cyber attacks. To bridge this gap, we study a specific type of equipment contention attack in cloud manufacturing. Through requesting extra shares of scarce resources, an attacker can gain advantage in competition with other users and reduce the efficiency of the overall system. We design a mechanism to mitigate such attacks through measuring the remaining machining capabilities in the cloud. The cloud administrator can control cost difference between the attacker and subsequent service requesters. Simulations of a mid-size city manufacturing cloud are conducted and the results show that our approach will incur low increases in cost for benign users while discouraging the equipment contention attacks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, unmanned aerial vehicle (UAV) networks have been a focus area of the academic and industrial research community. They have been used in many military and civilian applications. Emergency communication is one of the essential requirements for first responders and victims in the aftermath of natural disasters. In such scenarios, UAVs may configure ad hoc wireless networks to cover a large area. In UAV networks, however, localization and routing are challenging tasks owing to the high mobility, unstable links, dynamic topology, and limited energy of UAVs. Here, we propose swarm-intelligence-based localization (SIL) and clustering schemes in UAV networks for emergency communications. First, we propose a new 3-D SIL algorithm based on particle swarm optimization (PSO) that exploits the particle search space in a limited boundary by using the bounding box method. In the 3-D search space, anchor UAV nodes are randomly distributed and the SIL algorithm measures the distance to existing anchor nodes for estimating the location of the target UAV nodes. Convergence time and localization accuracy are improved with lower computational cost. Second, we propose an energy-efficient swarm-intelligence-based clustering (SIC) algorithm based on PSO, in which the particle fitness function is exploited for intercluster distance, intracluster distance, residual energy, and geographic location. For energy-efficient clustering, cluster heads are selected based on improved particle optimization. The proposed SIC outperforms five typical routing protocols regarding packet delivery ratio, average end-to-end delay, and routing overhead. Moreover, SIC consumes less energy and prolongs network lifetime.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Security and consistency of smart grids is one of the main issues in the design and maintenance of highly controlled and monitored new power grids. Bad data injection attack could lead to disasters such as power system outage, or huge economical losses. In many attack scenarios, the attacker can come up with new attack strategies that couldn't be detected by the traditional bad data detection methods. Adaptive Partitioning State Estimation (APSE) method [3] has been proposed recently to combat such attacks. In this work, we evaluate and compare with a traditional method. The main idea of APSE is to increase the sensitivity of the chi-square test by partitioning the large grids into small ones and apply the test on each partition individually and repeat this procedure until the faulty node is located. Our simulation findings using MATPOWER program show that the method is not consistent where it is sensitive the systems size and the location of faulty nodes as well.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a new class-optimal algorithm for the distributed computation of Wasserstein Barycenters over networks. Assuming that each node in a graph has a probability distribution, we prove that every node reaches the barycenter of all distributions held in the network by using local interactions compliant with the topology of the graph. We provide an estimate for the minimum number of communication rounds required for the proposed method to achieve arbitrary relative precision both in the optimality of the solution and the consensus among all agents for undirected fixed networks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accelerating deep neural networks on resource-constrained embedded devices is becoming increasingly important for real-time applications. However, in contrast to the intensive research works on specialized neural network inference architectures, there is a lack of study on the acceleration and parallelization of deep learning inference on embedded chip-multiprocessor architectures, which are favored by many real-time applications for superb energy-efficiency and scalability. In this work, we investigate the strategies of parallelizing single-pass deep neural network inference on embedded on-chip multi-core accelerators. These methods exploit the elasticity and noise-tolerance features of deep learning algorithms to circumvent the bottleneck of on-chip inter-core data moving and reduce the communication overhead aggravated as the core number scales up. The experimental results show that the communication-aware sparsified parallelization method improves the system performance by 1.6\u00d7-1.1\u00d7 and achieves 4\u00d7-1.6\u00d7 better interconnects energy efficiency for different neural networks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes an autoregressive approach to harness the power of deep learning for multi-speaker monaural speech separation. It exploits a causal temporal context in both mixture and past estimated separated signals and performs online separation that is compatible with real-time applications. The approach adopts a learned listening and grouping architecture motivated by computational auditory scene analysis, with a grouping stage that effectively addresses the label permutation problem at both frame and segment levels. Experimental results on the WSJ0-2mix benchmark show that the new approach can achieve better signal-to-distortion ratio and perceptual evaluation of speech quality scores than most of the state-of-the-art methods for both closed-set and open-set evaluations, even methods that exploit whole-utterance statistics for separation. It achieves this while requiring fewer model parameters.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This chapter gives details of Synchronous Optical NETworking (SONET)/synchronous digital hierarchy (SDH) as they still exist in most of the legacy networks and more so because many of their protocols, specifically with fault tolerance and survivability, are used in the current networks with some variations. The SONET/SDH protocol had many good features of fault tolerance, availability, etc., but it lacked many key features required in wavelength division multiplexing (WDM) large area optical networks. In large area optical transport networks (OTNs) WDM architecture is the norm. These networks are operated by multi\u2010carriers and their many interconnected domains are operated by multiple operators across the network boundaries. OTN is an important networking standard for metro networks because of its many distinguishing characteristics: it provides transport for any digital signal independent of client\u2010specific aspects; it can adapt to ever\u2010changing customer requirements and provide more effective optical network management.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Most of the approaches used for Landslide inventory mapping (LIM) rely on traditional feature extraction and unsupervised classification algorithms. However, it is difficult to use these approaches to detect landslide areas because of the complexity and spatial uncertainty of landslides. In this letter, we propose a novel approach based on a fully convolutional network within pyramid pooling (FCN-PP) for LIM. The proposed approach has three advantages. First, this approach is automatic and insensitive to noise because multivariate morphological reconstruction is used for image preprocessing. Second, it is able to take into account features from multiple convolutional layers and explore efficiently the context of images, which leads to a good tradeoff between wider receptive field and the use of context. Finally, the selected PP module addresses the drawback of global pooling employed by convolutional neural network, FCN, and U-Net, and, thus, provides better feature maps for landslide areas. Experimental results show that the proposed FCN-PP is effective for LIM, and it outperforms the state-of-the-art approaches in terms of five metrics, $Precision$ , $Recall$ , $Overall~Error$ , $F$ -$score$ , and $Accuracy$ .", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a computation offloading scheme for precedence-constrained tasks in a base station-assisted device-to-device (D2D) scenario for the information-centric Internet of Things (IC-IoT). When specified precedence among subtasks cannot be described as simple sequential or parallel relations in a task, the selection of task execution helper for subtasks offloading becomes complex due to the constraints of latency and resources. We define this type of precedence and aim to minimize the time and financial cost of computation task offloading for the user by optimizing subtask-helper pairs. This problem is modeled as a dynamic generalized multi-resource-constrained assignment problem. The optimal offloading policy is offered by searching minimum weight matchings in a bipartite graph. Computer simulations indicate the effectiveness of the proposed approach compared with the random helper selection and priority-based offloading scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We are considering a system capable of providing a route of minimum driving risk to reduce accidents of elderly drivers. In this system, mapping the driving risk is an important issue so that it is necessary to estimate the risk level of intersection without traffic signal where accidents occur frequently. To estimate the risk level of intersections, the viewing environment, especially the viewing angle, has a big influence. We plan the automatic generating method to predict the viewing angle from the Google Street View using latest computer vision approach. We try to predict the viewing angle to recognize the shield objects by using Semantic Segmentation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, multihop wireless networks have been playing a key role in many Internet of Things applications. Due to the limited resources of wireless nodes, extending the network lifetime is one of the most crucial issues, which needs to be concerned. This paper aims to maximize the network lifetime by appropriately distributing the tasks of the applications for each node in the network. First, a centralized optimal task allocation algorithm for multihop wireless networks (COTAM) is proposed by modeling the problem of maximizing the network lifetime as a linear programming (LP) problem. As the centralized algorithm requires knowing all the network parameters in advance, COTAM is mostly restricted to the off-line optimization in known environments. To extend the usability of the approach, this paper further proposes a distributed optimal task allocation algorithm (DOTAM) based on Dantzig-Wolf decomposition. DOTAM divides the centralized large-sized LP problem into small-sized subproblems, which are independently executed by each node. The proposed COTAM and DOTAM are tested by applying both the artificially generated applications and a realistic application. The extensive results demonstrate that DOTAM achieves the same performance as COTAM. Comparing with existing methods, they provide significant improvements on extending the network lifetime.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Although person re-identification (ReID) has drawn increasing research attention due to its potential to address the problem of analysis and processing of massive monitoring data, it is very challenging to learn discriminative information when the people in the images are occluded, in large pose variations or from different perspectives. To address this problem, we propose a novel joint attention person ReID (JA-ReID) architecture. The idea is to learn two complementary feature representations by combining a soft pixel-level attention mechanism and a hard region-level attention mechanism. The soft pixel-level attention mechanism learns a discriminative embedding for the fine-grained information by exploring the salient parts in the feature maps. The hard region-level attention mechanism conducts uniform partitions on the convolutional feature maps for learning local features. We have achieved competitive results in three popular benchmarks, including Market1501, DukeMTMC-reID, and CUHK03. The experimental results verify the adaptability of the joint attention mechanism to non-rigid deformation of the human body, which can effectively improve the accuracy of ReID.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cluster Heads (CHs) in cluster based Wireless Sensor Networks (WSNs) are considered as critical nodes since they are responsible of collecting data from cluster members, aggregating and then sending them to a sink node. The failure of CHs makes disconnected partitions in the network and hence will degrade the network performances, which makes the fault tolerance of CHs is a critical issue in WSNs. The existing fault tolerant mechanisms either consume important extra energy and time to detect and recover from the failures or need to use additional hardware and software resources. In this paper, we propose a Centralized Fault Tolerant Mechanism (CFTM) that deals efficiently with permanent and transient failures. The proposed mechanism is able to maintain the continuity of the network operation despite of CHs failures. The performance of the proposed mechanism was tested by means of simulations and compared against the Low-Energy Adaptive Clustering Hierarchy (LEACH) and Informer Homed Routing (IHR) protocols. The results show that our mechanism has better performance than its counterparts in terms of energy and time costs needed to tolerate failures as well as data received at the sink.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Completely soft robots are emerging as a compelling new platform for exploring and operating in unstructured, rugged, and dynamic environments. Unfortunately, the very properties which make soft robots so appealing also make them difficult to accurately model, scalably design, and robustly control. One of the outstanding obstacles to exploring these challenges is the relative lack of low-cost entry-level investigative model systems. In this paper we describe the design and implementation of a low-cost entry-level soft robotics platform based upon modular tensegrity structures. This modular platform can scale across a variety of shapes and sizes and is capable of untethered control. We then demonstrate how unsupervised learning algorithms can be used to produce vibration-based locomotion.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the development of cloud server, reversible data hiding in encrypted domain has received widespread attention for management of encrypted media. Most of existing reversible data hiding methods are based on stream ciphers, which are mainly concerned with data storage security. While homomorphic ciphers focus on data processing security, which is popular in cloud and other third-party platforms. Reversible data hiding with homomorphic cryptography has been an active topic in the research filed. In this paper, we propose a new lossless data hiding method based on homomorphic cryptosystem. After a content owner generates encrypted media with homomorphic ciphers, a data hider embeds secret data by establishing a mapping between secret bits and losslessly modifying encrypted media. At the receiver side, the embedded data is extracted from encrypted domain with a little auxiliary message. And original media is recovered by directly decrypting marked encrypted media. During the embedding process, the modification of encrypted media does not change the corresponding original media owing to homomorphic and probabilistic properties. Thus, no distortion is introduced in the whole procedures of data embedding. And directly decrypted media containing embedded data is the same as the original media. Meanwhile, the proposed method achieves a high embedding rate through efficient mapping and skillful use of expanded pixel values. The experimental results show that compared with state-of-the-art methods, the proposed method has higher embedding rate without distortion.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As the demand for flexible learning increases, it is important to explore and expand online learning opportunities, especially in student supported learning. Peer Assisted Study Sessions (PASS) is a student led academic support program designed to help students transition into university and increase student retention. PASS is offered in traditionally challenging first year core subj ects. Due to increased popularity of PASS, along with limited space and time availability, a synchronous online format (Blackboard Collaborate) was piloted in three first year University of Wollongong (UOW) subjects in the faculties of Business, Nursing, and Psychology. The aim was to test the effectiveness of the online delivery of PASS by comparing student final grade outcomes from online cohorts with face-to-face (F2F) modes, and those students who had not attended PASS. Results demonstrated that students who attended PASS obtained significantly higher marks compared to students that did not attend PASS. Final grade outcomes for F2F versus online also varied between subjects. The different result profiles for the three subjects suggests there may be different drivers for student success in the online space. This paper presents these findings providing consideration of different factors that may influence student success, with directions for future research.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The e-healthcare management system could be enchanced extremelyby connecting tocurrent trend technology. This paper suggested a model of grouping adaptable e-healthcare services administration framework dependent on Cloud Computing. Along these lines, this paper prescribed a model of planning adaptable e-healthcare services administration framework dependent on distributed computing. The recommended framework has been enhanced and incorporates different divisions to create healthcare services framework. Health Data Management System based on the client side, simple healthcare cloud and application side creates a readily attainable network. Biometric based confirmation system is appropriate in this condition since it defeats the constraints of nominalcrime and forgetpasswords in the regular nominal id secret key instrument utilized for giving security. It additionally has high correctness rate for secure information access and recovery. At long last, the framework proposed enhances cost administration, time, cost, putting away patient profile.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: IEEE 802.11ah protocol aims at providing network connectivity to a large number of end points while maintaining a satisfactory network performance. The RAW scheme of MAC access technique enables STAs to contend in different RAW slots and thereby reducing the chance of collision. In this paper we evaluate the performance metrics of the RAW scheme and observe that RAW scheme may be suitable for very small traffic condition. At higher traffic load, some of the nodes need to wait longer to transmit packets and network becomes unstable very quickly as the traffic for each node is increased beyond two packets per second in our test condition. The highest normalized network throughput achieved in this network is around 40%. Some important performance metrics like idle probability, backoff time and transmission probability are evaluated and studied in detail in this paper.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This research provides academic and practical contributions. From a theoretical standpoint, a hybrid harmony search (HS) algorithm, namely the oppositional global-based HS (OGHS), is proposed for solving the multi-objective flexible job-shop scheduling problems (MOFJSPs) to minimize makespan, total machine workload and critical machine workload. An initialization program embedded in opposition-based learning (OBL) is developed for enabling the individuals to scatter in a well-distributed manner in the initial harmony memory (HM). In addition, the recursive halving technique based on opposite number is employed for shrinking the neighbourhood space in the searching phase of the OGHS. From a practice-related standpoint, a type of dual vector code technique is introduced for allowing the OGHS algorithm to adapt the discrete nature of the MOFJSP. Two practical techniques, namely Pareto optimality and technique for order preference by similarity to an ideal solution (TOPSIS), are implemented for solving the MOFJSP. Furthermore, the algorithm performance is tested by using different strategies, including OBL and recursive halving, and the OGHS is compared with existing algorithms in the latest studies. Experimental results on representative examples validate the performance of the proposed algorithm for solving the MOFJSP.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As Internet of Things (IoT) applications are getting attention these days, the importance of firmware security is also growing. However, it is not straightforward to analyze the bugs or vulnerabilities that reside in firmware. One of the major challenges is to detect information about hardware architectures of compressed firmware. Traditional analysis tools make use of static signatures embedded in the compressed binary code of firmware. However, signature extraction needs the careful elaboration of experts, and it is not always even possible. In this paper, we introduce our experience in analyzing the hardware information of compressed firmware. Since it is not possible to use the semantic information of compressed binary code, we adopt machine learning technologies for this purpose. Despite various difficulties, we have positive experimental results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Space-Ground Integrated Network (SGIN) plays an important role for the future development of the country. The cyber-attacks against it are the focus of the research. In this paper, an association analysis algorithm based on knowledge graph of cyber security attack events is proposed to present the attack scenario for the Space-Ground Integrated Network. The construction of knowledge graph and association analysis can show the scene of cyber-attacks in the form of graphs. During the build process, the construction of an event ontology is an important part of it. Event ontology is used to represent various relationships in the network attack procedure. At last, we present a space-ground integration network security analysis system based on the knowledge graph of cyber security attack events, and uses the association analysis algorithm to analyze the attack scenario.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Environmental conditions have a significant impact on health and all human activities. On the one hand, laboratories are spaces characterized by various sources of pollution that can lead to unhealthy indoor environments. On the other hand, laboratory activities, especially in the case of experiments using thermography, require the supervision of several parameters. The proliferation of the Internet of Things paradigm (IoT) shows great potential for the creation of automatic solutions for monitoring indoor environments. In general, people spend about 90% of their time inside buildings so it is significant to use realtime monitoring systems in order to identify bad ambient conditions. The timely detection of poor ambient conditions allows you to plan interventions in the building for enhanced living environments and occupational health. This paper aims to present a laboratory environment conditions supervision solution based on IoT architecture called iLabM+. The solution is composed of prototype hardware for the collection of the data environment and a Web portal for data consulting and analysis. This system allows monitoring of temperature, relative humidity, barometric pressure and air quality. The results obtained are promising and represent a significant contribution to IoT-based environmental quality monitoring systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, from the perspectives of defenders, we consider the detection problems of false data-injection attacks in cyber-physical systems (CPSs) with white noise. The false data-injection attacks usually modify the sensor data to make CPSs unstable and keep stealth for the \u03c72 detector. To guarantee system security, a novel detector, that is, the summation (SUM) detector, is proposed to detect the false data-injection attacks. Different from the \u03c72 detector, the SUM detector not only utilizes the current compromise information but also collects all historical information to reveal the threat. Its evaluation value also satisfies \u03c72 distribution when no attacks compromise the systems, and the false alarm rate can be restricted to less than any given value by choosing the proper threshold value. Furthermore, an improved false data-injection attack with a time-variable increment coefficient is introduced based on the existing approaches. The effects of the SUM detector are also verified for the traditional and the improved false data-injection attacks, respectively. Finally, some simulation results are given to demonstrate the effectiveness and superiority of the SUM detector.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Optical Character Recognition (OCR) of Arabic text from manuscripts images is virtually impossible, this is due to the semi-cursive nature of Arabic script which is very difficult to explore by algorithms and image processing methods.In this paper, we present a method of exploration and research in content of Arabic manuscript images. It's about characterizing segmented handwritten words with a set points of interest by providing a means of identification and research in these manuscripts. Each word segmented and described by key features will be compared to query words.The results of this method are encouraging in comparison with other methods, especially when it comes to the same of handwritten style used in the text.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We focus on the problem of crosstalk-aware lightpath provisioning in a dynamic, spectrally and spatially flexible optical network, in which spectral super-channels are carried over multi-core fibers with distance-adaptive transmission. The problem involves establishing a lightpath for a connection such that the inter-core crosstalk (XT) between the new lightpath and any exiting lightpaths does not cause the quality of transmission of any of these light-paths to be below an acceptable level. To this end, we analyze and compare two broad classes of schemes that aim to ensure acceptable XT levels in the network. The first is based on static/worst-case XT, whereas the second uses dynamic/ precise XT estimation. These methods are implemented in a dynamic XT-aware routing, spatial mode, and spectrum allocation algorithm that solves the lightpath-provisioning problem efficiently with the goal of minimizing bandwidth blocking. Extensive simulation experiments performed in realistic network scenarios indicate that the utilized XT estimation methodology has a significant impact on network performance, with the precise XT approach favored over the worst-case XT strategy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Convolutional Neural Network (CNN) has achieved dramatic developments in emerging Machine Learning (ML) services. Compared to online ML services, offline ML services that are full of diverse CNN workloads are common in small and medium-sized enterprises (SMEs), research institutes and universities. Efficient scheduling and processing of multiple CNN-based tasks on SME clusters is both significant and challenging. Existing schedulers cannot predict the resource requirements of CNN-based tasks. In this paper, we propose GENIE, a QoS-guided dynamic scheduling framework for SME clusters that achieves users' QoS guarantee and high system utilization. Based on a prediction model derived from lightweight profiling, a QoS-guided scheduling strategy is proposed to identify the best placements for CNN-based tasks. We implement GENIE as a plugin of Tensorflow and experiment with real SME clusters and large-scale simulations. The results of the experiments demonstrate that the QoS-guided strategy outperforms other baseline schedulers by up to 67.4% and 28.2% in terms of QoS-guarantee percentage and makespan.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Visual Odometry (VO) is a key component in modern driver assistance systems and robotics. Meeting the real-time requirements is mandatory for VO in such applications. Previous works have primarily focused on improving accuracy at the cost of longer runtime. In this work, we propose novel strategies for feature correspondence setup, outlier removal and robust pose optimization in the VO pipeline to achieve real-time performance of close to 30 frames-per-seconds (fps) on a dual-core 3.5 GHz CPU while maintaining high accuracy. In particular, computationally efficient strategies are introduced to obtain an initial set of good features and rapidly filter out the outliers to minimize the computational overhead in later stages. In addition, we propose a depth based weighting and saturated-residual scheme during pose optimization to increase the robustness of VO. Experimental results show that the proposed VO achieves the fastest speed among all the top-ranked OV and SLAM systems on KITTI leader-board. Specifically, the proposed VO is 47% faster than state-of-the-art ORB-SLAM2 with comparable accuracy on KITTI dataset.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes an efficient equivalent model of patch antenna for the fast prediction of its installed radiation pattern. More CPU time and memory cost are required for accurate prediction of installed radiation pattern of patch antenna on different platforms. However, a fast and efficient prediction can save CPU time and memory cost when constructing an equivalent model of patch antenna that can reproduce a similar radiation pattern to that of the patch antenna. A code is developed to determine the electric field of a magnetic dipole based on Green function derivation. The result of the radiation pattern for the far-field and near-field are computed and validated with the result using commercial software tool (FEKO). The magnetic dipole is used to construct the equivalent model of patch antenna based on the radiation mechanism to predict its installed radiation pattern. The numbers of design parameters needed to be optimized are reduced to only two parameters which are the spacing distance between the dipoles in the x- and y-directions. The height of the dipole is kept at a fixed value above the same ground plane as that of the patch antenna. This makes it more computational efficient by reducing the CPU time and memory cost. After the equivalent model is optimized with FEKO optimization tool, it is further installed on a platform to compute the installed radiation pattern. The simulation results show that the proposed equivalent model based on a magnetic dipole with only two design parameters can obtain a fast prediction of installed radiation pattern of patch antenna when mounted on a platform. The equivalent model does not require detailed geometry and material information of the patch antenna.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Information-centric networking (ICN) is of high interest to the Internet of Things (IoT) community, since the dissemination of massive data continuously produced by IoT devices can be easily handled by ICN\u2019s data naming scheme and inherent multipath delivery. Providing optimal multipath-oriented transmission control is crucial for ICN-IoT data delivery, but yet remains challenging because of the randomness of request arrival, dynamic link condition, and on-path caching. More prominently, the resource limitation and scalability issues in IoT require the control scheme to be lightweight and distributed. In this paper, we propose a distributed stochastic optimization framework for multipath transmission control in ICN-IoT. The transmission control, including request scheduling and data rate regulation, is formulated as a stochastic concave optimization problem, which aims to accommodate the randomness, unpredictability, and multipath delivery of ICN-IoT and maximize the overall throughput. This problem is linearly separated into two subproblems: 1) a request scheduling problem and 2) a data rate control problem, which can be individually solved per time slot. A distributed alternating descent method (DADM) is designed to optimally control the transmission by solving the aforementioned problems at client sides. DADM enables each client to sequentially update the request schedule and rate regulation via communicating the links and providers they use, which asymptotically converges to optimality while allowing low-complexity and decentralized implementation. Validated by simulations, our DADM significantly improves throughput, delay reduction, and energy efficiency, in comparison with other state-of-the-art solutions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The integration of satellite and terrestrial network(ISTN) is an ongoing trend in prospect. Recently many commercial companies such as SpaceX, OneWeb plan to launch tens of thousands satellites which promotes the scale of satellite networks rapidly. While the scale of the satellite network is still small relatively compared with the terrestrial network. IP-routing ability in satellite network is becoming more and more important for ISTN. Suffering from high topology dynamics, long propagation delay, and onboard processing limitations on satellite networks, routing in ISTN faces many challenges worth analyzing in-depth. This paper builds an IP-based routing usability theoretical analytical model with calculable formula, which formulates different types of networks under a unified framework that accommodates various configurations and characteristics of multiple networks. Moreover, massive experiments based on real satellite motion data are designed and conducted to analyze the routing performance and challenges in ISTN deeply, combining various ground stations, satellite processing ability, number of satellites per orbit, etc. At last, with the analysis of routing usability for ISTN, some instructive conclusions are drawn. Although the number of routing nodes in satellite networks is relatively small compared with terrestrial networks, but the influence is big. With the network scale extending, high topology dynamics makes the IP-based routing performance of ISTN decrease dramatically. Routing usable time in one day period decreases from 23.36h to 2.99h when the number of satellites grows from 60 to 960. To keep routing usability, the number of satellites in space is not the more the better in ISTN. The scale of network and the onboard processing ability should do trade off.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: During disasters, discover people's concerns dynamically is crucial to disaster rescue and relief. In this paper, we propose a social media based framework to analyze people's concerns, to access the importance and to track the dynamic changes of these concerns. To better understand people's concerns across platforms and to monitor the dynamics, we make comparisons between Tweets and news on the mentioned aspects and disclosed some interesting findings. Specifically, we take 2018 Camp Fire, the most destructive wildfire on record in history of California as a case study. We find that despite their keen attentions towards the disaster, social media and news media focus on different aspects of the disaster, so are the contents and dynamic changes of their concerns.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Fast and accurate detection of vehicles on road traffic scenes captured by traffic surveillance cameras, is essential for large-scale deployment of automated traffic surveillance systems. The state-of-the-art techniques typically employ background modeling for low-complexity foreground detection. However, this is a challenging problem as these methods need to be robust to varying road scene conditions (such as illumination changes, camera jitter, stationary vehicles, and heavy traffic) leading to huge computation cost. In this paper, we propose a highly accurate yet low-complexity foreground (i.e., vehicle) detection technique, which can effectively deal with the varying road scene conditions, and generate accurate pixel-level foreground masks in real-time. We propose a novel robust block-based feature suitable for modeling road background and detecting vehicles as foreground, and employ Bayesian probabilistic modeling on these features. The experimental evaluations on widely used traffic datasets demonstrate that the proposed method can achieve comparable accuracy to the existing state-of-the-art techniques but at a much higher processing frame rate (40x speedup over PAWCS). The real-time performance of the proposed system has also been demonstrated by implementing it on a low-cost embedded platform, Odroid XU-4, that still achieves a frame rate of over 80 frames/s, thereby enabling the real-time detection of foreground objects in road scenes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Solving a large-scale system of linear equations is a key step at the heart of many algorithms in scientific computing, machine learning, and beyond. When the problem dimension is large, computational and/or memory constraints make it desirable, or even necessary, to perform the task in a distributed fashion. In this paper, we consider a common scenario in which a taskmaster intends to solve a large-scale system of linear equations by distributing subsets of the equations among a number of computing machines/cores. We propose a new algorithm called Accelerated Projection-based Consensus, in which at each iteration every machine updates its solution by adding a scaled version of the projection of an error signal onto the nullspace of its system of equations, and the taskmaster conducts an averaging over the solutions with momentum. The convergence behavior of the proposed algorithm is analyzed in detail and analytically shown to compare favorably with the convergence rate of alternative distributed methods, namely distributed gradient descent, distributed versions of Nesterov's accelerated gradient descent and heavy-ball method, the block Cimmino method, and Alternating Direction Method of Multipliers. On randomly chosen linear systems, as well as on real-world data sets, the proposed method offers significant speed-up relative to all the aforementioned methods. Finally, our analysis suggests a novel variation of the distributed heavy-ball method, which employs a particular distributed preconditioning and achieves the same theoretical convergence rate as that in the proposed consensus-based method.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unknown input estimation have a major importance in real-time monitoring, diagnosis and fault-tolerance control. Consequently, this paper concentrates on state estimation for nonlinear system where a part of its inputs is unknown. In particular, the problem is oriented towards an experiential application for laboratory three tank system affected by actuator faults. Our objective is to perform the fault detection (FE) by the synthesis and the implementation of the unknown input observer (UIO). Simulation and experimental results confirm the performance of the suggested observer.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The development of cognitive radio and radar electronic reconnaissance has put forward an important demand for improving the recognition ability of modulated signals in complex electromagnetic environment. In this paper, we propose a valid radar signal modulation recognition technology under low signal-to-noise ratio (SNR). The recognition technology can recognize 12 different modulation signals, including Costas, LFM, NLFM, BPSK, P1-P4, and T1-T4 codes. First, we propose the image fusion algorithm of non-multi-scale decomposition to fuse images of a single signal with different time-frequency (T-F) methods. Specifically, weights are designed by the principal component analysis, which could combine significative details of T-F images. Second, we adopt transfer learning-based convolutional neural networks and self-training-based stacked autoencoder, which extract the effective information on fusion image, furthermore guarantee the recognition performance. Moreover, multi-feature fusion algorithm is used to fuse features, which reduces redundant information on features and enhances computing efficiency. Finally, the classifier is performed by a classical algorithm called support vector machine. Simulation results show that the average recognition success rate is 95.5% at SNR of -6dB. It is testified that proposed recognition technology possesses good robustness and superiority in RSR with a wide range of SNR.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In order to improve the edge caching efficiency of the fog radio access network (F-RAN), this paper put forward a distributed deep Q-learning-based content caching scheme based on user preference prediction and content popularity prediction. Given that the constraint that the storage capacity of each device is limited, and the optimization problem is formulated so as to maximize the caching hit rate. Specifically, by taking users' selfishness into consideration, user preference is predicted in an offline manner by applying popular topic models. Then, the online predicted content popularity is achieved by combining the network topology relationship together with the obtained user preference. Finally, with the predicted user preference and content popularity, the deep Q-learning network (DQN)-based content caching algorithm is proposed to achieve the optimal content caching strategy. Moreover, we further present a content update policy with user preference and content popularity prediction, so that the proposed algorithm can handle the variations of contents popularity in a timely manner. Simulation results demonstrate that the proposed scheme achieves better caching hit rate compared with existing algorithms.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To address the drastic growth of data traffic dominated by streaming of video-on-demand files, mobile edge caching/computing (MEC) can be exploited to develop intelligent content caching at mobile network edges to alleviate redundant traffic and improve content delivery efficiency. Under the MEC architecture, content providers (CPs) can deploy popular video files at MEC servers to improve users\u2019 quality of experience (QoE). Designing an efficient content caching policy is crucial for CPs due to the content dynamics, unknown spatial-temporal traffic demands, and limited service capacity. The knowledge of users\u2019 preference is very useful and important for efficient content caching, yet often unavailable in advance. Under this circumstance, machine learning can be used to learn the users\u2019 preference based on historical demand information and decide the video files to be cached at the MEC servers. In this paper, we propose a multi-agent reinforcement learning (MARL)-based cooperative content caching policy for the MEC architecture when the users\u2019 preference is unknown and only the historical content demands can be observed. We formulate the cooperative content caching problem as a multi-agent multi-armed bandit problem and propose a MARL-based algorithm to solve the problem. The simulation experiments are conducted based on a real dataset from MovieLens and the numerical results show that the proposed MARL-based cooperative content caching scheme can significantly reduce content downloading latency and improve content cache hit rate when compared with other popular caching schemes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Compressed Sensing (CS) based channel estimation techniques have recently emerged as an effective way to acquire the channel of millimeter-wave (mmWave) systems with a small number of measurements. These techniques, however, are based on prior knowledge of transmit and receive array manifolds, and assume perfect antenna arrays at both the transmitter and the receiver. In the presence of antenna imperfections, the geometry and response of the arrays are modified. This distorts the CS measurement matrix and results in channel estimation errors. This paper studies the effects of both transmit and receive antenna imperfections on the mmWave channel estimate. A relay-aided solution which corrects for errors caused by faulty transmit arrays is then proposed. Simulation results demonstrate the effectiveness of the proposed solution and show that comparable channel estimates can be obtained when compared to systems with perfect antennas without the need for additional training overhead.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Information Systems (ISs) are fundamental to streamline operations and support processes of any modern enterprise. Being able to perform analytics over the data managed in various enterprise ISs is becoming increasingly important for organisational growth. Extract, Transform, and Load (ETL) are the necessary pre-processing steps of any data mining activity. Due to the complexity of modern IS, extracting data is becoming increasingly complicated and time-consuming. In order to ease the process, this paper proposes a methodology and a pilot implementation, that aims to simplify data extraction process by leveraging the end-users' knowledge and understanding of the specific IS. This paper first provides a brief introduction and the current state of the art regarding existing ETL process and techniques. Then, it explains in details the proposed methodology. Finally, test results of typical data-extraction tasks from four commercial ISs are reported.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a robust optimal neuro-adaptive controller for nonlinear systems with unstructured uncertainties. This work is also the first step towards employing control barrier functions (CBFs) for such systems to create safety constraints in the presence of disturbances. The proposed controller consists of three parts: feedforward term, adaptive term, and optimal term. The unknown dynamics of the system are estimated by a joint neural network and concurrent learning adaptation mechanism (NNCL) to inform the adaptive term. The optimal term uses an online quadratic program (QP) formulated to generate the optimal signal while providing system stability via a control Lyapunov function (CLF). The CBFs and control bounds (CBs) constraints are incorporated into the QP structure to create safety conditions on the system and bound the control effort. A robust term robustifies the proposed controller to disturbances and uncancelled uncertainty. The end result is a QP-RCLBF-NNCL controller for which uniformly ultimately boundedness of all system signals is proven using Lyapunov synthesis. The proposed controller is validated on an inverted pendulum. Simulation results show that the controller achieves good tracking performance and model identification. Two safety tests are performed to show that the proposed controller is able to bound the control signal and velocity by their predefined values when a disturbance acts on the system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Most of existing community detection algorithms group nodes with more connections into the same community, and they are more concerned with links within the community. However, the weak ties between different communities are also important, because they can reflect the relationships between different communities, including helpful, friendly or negative, and adverse. Few studies focus on weak ties, although they are important. In this paper, we propose a novel sign prediction model based on the nodes features in the network, including the Jaccard similarity and the ratio of the negative degrees of all nodes, and the autoencoder technology that self-defines its loss function with the features of the communities. The proposed model maps the original network to a low-dimensional space so that the weak ties can be represented by low-dimensional vectors. We conduct experiments on the Epinions and Slashdot datasets and find that the proposed model outperforms the challenging state-of-the-art graph embedding methods in the sign prediction of weak ties in terms of accuracy and F1 score measurement.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Radiomics is a rapidly growing field that deals with modeling the textural information present in the different tissues of interest for clinical decision support. However, the process of generating radiomic images is computationally very expensive and could take substantial time per radiological image for certain higher order features, such as, gray-level co-occurrence matrix(GLCM), even with high-end GPUs. To that end, we developed RadSynth, a deep convolutional neural network(CNN) model, to efficiently generate radiomic images. RadSynth was tested on a breast cancer patient cohort of twenty-four patients(ten benign, ten malignant and four normal) for computation of GLCM entropy images from post-contrast DCE-MRI. RadSynth produced excellent synthetic entropy images compared to traditional GLCM entropy images. The average percentage difference and correlation between the two techniques were 0.07 \u00b1 0.06 and 0.97, respectively. In conclusion, RadSynth presents a new powerful tool for fast computation and visualization of the textural information present in the radiological images.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Adaptive channel coding and power control for practical free-space optical communication systems are proposed in this paper. Particularly, we first assume that the channel state information (CSI) is perfectly known at the transmitter, and propose adaptive transmission schemes in which coding rate is adjusted either independently or jointly with transmit power according to the channel conditions. Moreover, an optimization problem is developed to attain power consumption minimization under free space optical (FSO) practical constraints, i.e., target bit-error rate, target outage probability, and maximum transmit power. We then solve the optimization problem and derive closed-form expressions for throughput and average transmit power for both adaptive schemes over the Gamma-Gamma atmospheric turbulence channels. Our results demonstrate the superiority of the proposed adaptive schemes over non-adaptive schemes, especially under strong turbulence conditions. In the second part of this paper, we move toward a more realistic scenario where the channel is not perfectly known to Tx/Rx and is estimated by the sequence of received signals. Therefore, we investigate the effect of channel estimation inaccuracies on the performance of the proposed schemes and show that the accuracy of the channel estimator depends on the length of the sequence and, for a sufficiently large length of the observation window, it achieves performance close to the receiver with known CSI. Extensive analytical derivations are provided to investigate the performance of the proposed adaptive schemes under channel estimation error. Our analytical results can be used for calculating and tuning of the optimum length of the observation window without resorting to Monte Carlo simulations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To improve the diagnostic accuracy rate of multiple parameter faults in widely used power electronic circuits, solving for the overlapping parameter fault characteristic signal is crucial. This paper proposes a novel approach for diagnosing the multiple parameter faults of power electronic circuits that can adaptively adjust the weights of each fault feature parameter according to the diagnostic accuracy rate to enhance their discriminability. According to the properties of multiple parameter faults in power electronic circuits, the measured signal that has been decomposed via the variational mode decomposition and wavelet packet energy entropy (VMD-WPEE) is used to extract the feature parameters. Furthermore, a classifier is selected and the threshold of the diagnostic accuracy rate is set. Then, the weight of each fault feature parameter is adjusted until the classifier yields the desired results. Finally, the proposed approach is applied to other classifiers and feature extraction methods. The high accuracy, high robustness, and advantages of the multiple-parameter-fault diagnosis method are demonstrated by the simulation and experimental results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose a deep multiple description coding framework, whose quantizers are adaptively learned via the minimization of multiple description compressive loss. Firstly, our framework is built upon auto-encoder networks, which have multiple description multi-scale dilated encoder network and multiple description decoder networks. Secondly, two entropy estimation networks are learned to estimate the informative amounts of the quantized tensors, which can further supervise the learning of multiple description encoder network to represent the input image delicately. Thirdly, a pair of scalar quantizers accompanied by two importance-indicator maps is automatically learned in an end-to-end self-supervised way. Finally, multiple description structural dissimilarity distance loss is imposed on multiple description decoded images in pixel domain for diversified multiple description generations rather than on feature tensors in feature domain, in addition to multiple description reconstruction loss. Through testing on two commonly used datasets, it is verified that our method is beyond several state-of-the-art multiple description coding approaches in terms of coding efficiency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The behavior of APT attack has been the hot topic in recent network security study. It is critical to understand the implementation principle of APT attack. In this paper, we analyze the behavior of APT attack in the Ngay campaign from two aspects: network traffic and code implementation. We first set up the attack chain by using network traffic analysis. Then, the vulnerability exploitation process is detailed through reverse code analysis. After that, we illustrate the process of building back door. Lastly, we introduce the obfuscation technology applied in the APT malware samples.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Melanoma skin cancer is one of the most deadly forms of cancer which are responsible for thousands of deaths. The manual process of melanoma diagnosis is a time taking and difficult task, therefore researchers introduced several computerized methods for recognition. Through computational methods, improves the accuracy of diagnostics process which is helpful for dermatologists. In this paper, we proposed an automated system for skin lesion classification through transfer learning based deep neural network (DCNN) features extraction and kurtosis controlled principle component (KcPCA) based optimal features selection. The pre-trained ResNet deep neural network such as RESNET-50 and RESNET-101 are utilized for features extraction. Then fused their information and selects the best features which later fed to supervised learning method such as SVM of radial basis function (RBF) for classification. Three datasets name HAM10000, ISBI 2017, and ISBI 2016 are utilized for experimental results and achieved an accuracy of 89.8%, 95.60%, and 90.20%, respectively. The overall results show that the performance of the proposed system is reliable as compared to existing techniques.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Given a tiny face image, existing face hallucination methods aim at super-resolving its high-resolution (HR) counterpart by learning a mapping from an exemplary dataset. Since a low-resolution (LR) input patch may correspond to many HR candidate patches, this ambiguity may lead to distorted HR facial details and wrong attributes such as gender reversal and rejuvenation. An LR input contains low-frequency facial components of its HR version while its residual face image, defined as the difference between the HR ground-truth and interpolated LR images, contains the missing high-frequency facial details. We demonstrate that supplementing residual images or feature maps with additional facial attribute information can significantly reduce the ambiguity in face super-resolution. To explore this idea, we develop an attribute-embedded upsampling network, which consists of an upsampling network and a discriminative network. The upsampling network is composed of an autoencoder with skip-connections, which incorporates facial attribute vectors into the residual features of LR inputs at the bottleneck of the autoencoder, and deconvolutional layers used for upsampling. The discriminative network is designed to examine whether super-resolved faces contain the desired attributes or not and then its loss is used for updating the upsampling network. In this manner, we can super-resolve tiny (16\u00d716 pixels) unaligned face images with a large upscaling factor of 8\u00d7 while reducing the uncertainty of one-to-many mappings remarkably. By conducting extensive evaluations on a large-scale dataset, we demonstrate that our method achieves superior face hallucination results and outperforms the state-of-the-art.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Due to densification of wireless networks, there exist abundance of idling computation resources at (network) edge helpers (e.g., base stations and handheld computers). These resources can be scavenged by offloading heavy computation tasks from small Internet-of-Things (IoT) devices (e.g., sensors and wearable computing devices) in proximity, thereby overcoming their limitations and lengthening their battery lives. However, unlike dedicated servers, the spare resources offered by edge helpers are random and intermittent. Thus, it is essential to intelligently control a user (IoT device) the amounts of data for offloading and local computing so as to ensure that a computation task can be finished in time-consuming minimum energy. In this paper, we design energy-efficient control policies in a computation offloading system with a random channel and a helper with a dynamically loaded CPU (due to the primary service). Specifically, the policy adopted by the helper aims at determining the sizes of offloaded and locally computed data for a given task in different slots such that the total energy consumption for transmission and local CPU is minimized under a task-deadline constraint. As the result, the polices endow an offloading user robustness against channel-and-helper randomness besides balancing offloading and local computing. By modeling the channel and helper CPU as Markov chains, the problem of offloading control is converted into a Markov decision process. Though dynamic programming (DP) for numerically solving the problem does not yield the optimal policies in closed form, we leverage the procedure to quantify the optimal policy structure and apply the result to design optimal or sub-optimal policies. For three cases ranging from zero, small to large helper buffers, the low complexity of the policies overcomes the \u201ccurse of dimensionality\u201d in DP arising from joint consideration of channel, helper CPU, and buffer states.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a novel data-driven information hiding scheme called \u201cgenerative steganography by sampling\u201d (GSS) is proposed. Unlike in traditional modification-based steganography, in our method the stego image is directly sampled by a powerful generator: no explicit cover is used. Both parties share a secret key used for message embedding and extraction. The Jensen-Shannon divergence is introduced as a new criterion for evaluating the security of generative steganography. Based on these principles, we propose a simple practical generative steganography method that uses semantic image inpainting. The message is written in advance to an uncorrupted region that needs to be retained in the corrupted image. Then, the corrupted image with the secret message is fed into a Generator trained by a generative adversarial network (GAN) for semantic completion. Message loss and prior loss terms are proposed for penalizing message extraction error and unrealistic stego image. In our design, we first train a generator whose training target is the generation of new data samples from the same distribution as that of existing training data. Next, for the trained generator, backpropagation to the message and prior loss are introduced to optimize the coding of the input noise data for the generator. The presented experiments demonstrate the potential of the proposed framework based on both qualitative and quantitative evaluations of the generated stego images.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Local binary descriptor constitutes power visual cues for feature representation. They provide discriminative information about small appearance details in local neighbourhoods. So, they are robust to local changes databases such as illumination, identity, and expression. Unlike existing local descriptors is not discriminatory enough to estimate the relationship between two people. This is mainly due to the learning feature code individually and the hand-crafted features which previous knowledge is required. In this paper, we propose an effective Context-Aware Local Binary Feature Learning (CA-LBFL)for kinship verification in order to solve the proposed problem. (CA-LBFL)a method has applied to learn contextual features from raw pixels directly and to eliminates the dependence on hand-crafted features. Experimental results demonstrate that the proposed method achieves competitive results compared with other states-of-the-art.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In order to identify a specific system (module) of interest embedded in a dynamic network, one typically has to formulate a multi-input single-output (MISO) identification problem which requires to identify all modules in the MISO structure, and determine their model order. While the former task poses the problem of estimating a large number of parameters that are of no interest to the experimenter, the latter task may result computationally challenging in large-size networks. To avoid these issues and increase the accuracy of the identified module of interest, we use regularized kernel-based methods. Keeping a parametric model for the module of interest, we model the impulse response of the remaining modules in the MISO structure as zero mean Gaussian vectors with covariance matrix (kernel) given by the first-order stable spline kernel, accounting also for the noise model affecting the output of the target model. Using an Empirical Bayes (EB) approach, the target-module parameters are estimated by maximizing the marginal likelihood of the module output. The related optimization problem is solved using the Expectation-Maximization (EM) algorithm. Numerical experiments illustrate the potentials of the introduced method in comparison with the state-of-the-art techniques for local identification.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Spell Checker is an important part of language specific text processing applications. In this paper, we propose a novel hybrid approach for spell checker for Punjabi language. Available spell checkers use traditional method for error detection such as dictionary lookup technique and minimum edit distance for error correction. The proposed spell checker aims to improve performance as well accuracy. In this paper, we present the use of trie data structure to store Punjabi words dictionary and then use tree based algorithm along with n-gram analysis to detect misspelled words. To correct misspelled words, best possible suggestion is listed using Long-short term memory (LSTM) recurrent neural network along with rule based approach and minimum edit distance. Error detection using trie-based dictionary improves the performance and Error correction using LSTM improves the accuracy of proposed spell checker. In addition to error detection techniques and error correction techniques, proposed spell checker uses handcrafted rules, language syntax rules and rules regarding tokenization.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is generally conceded that, due to security and privacy concerns, enterprises and users are reluctant to embrace the cloud computing paradigm and hence benefit from the cost reductions and the increased flexibility or business agility that this paradigm brings about. These concerns stem mainly from the significantly-expanded attack surfaces that result from the heterogeneous nature of cloud services and the dynamicity inherent in cloud environments. In order to alleviate these concerns, effective and flexible access control approaches are required to consider the contextual parameters that characterise data access requests in the cloud. In this respect, this work presents PaaSword: a novel holistic access control framework\u2014essentially a PaaS offering\u2014that extends the popular XACML standard with semantic reasoning capabilities that support the federation of effective context-aware access control policies and their infusion into cloud applications with minimal manual intervention and effort. To determine the performance of our solution, a comparative evaluation test is presented and discussed, against a well-known reference implementation of the XACML standard, namely the open source WSO2 Balana engine.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The gateway selection is an essential issue in hybrid mobile ad hoc networks (MANETs). Current independent and random selections without supporting routing negotiation protocol may cause links or gateways overloaded when they are selected by multiple nodes simultaneously. Therefore, the network performance may not be in a proper way. Furthermore, the ad hoc nature of MANETs makes the topology change dynamically, so that the gateway selection becomes even more difficult. This paper presents a mathematical model for gateway capability and a novel approach, called bio-inspired gateway selection, where the gateways are selected according to the network status and associated with a cooperative mechanism for optimization. The novelty includes the use of attractor selection model, the self-adaptability and the autonomy of the biological system. The performance of the proposed approach is evaluated by simulation with different scenarios and compared with the conventional approaches being currently used in hybrid MANETs. The illustrated numerical results present the performance of the proposed approach in terms of packet delivery ratio, average delivery latency, normalized routing overhead, and gateway load balance under different network conditions. Furthermore, the numerical results are also able to demonstrate the significant performance gain compared with the conventional gateway selection approaches.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper explores Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to deal with unseen entities without annotation. A compromised solution is to build a global entity-unspecific model for all entities without respect to the relationship information among entities, which cannot guarantee achieving a satisfactory result for each entity. Moreover, most previous methods can not adequately exploit prior knowledge embedded in entities or documents due to considering all kinds of features indifferently. In this paper, we propose a novel entity and document class-dependent discriminative mixture model by introducing one intermediate layer to model the correlation between entity-document pairs and hybrid latent entity-document classes. The model can better adjust to different types of entities and documents, and achieve better performance when dealing with a broad range of entity and document classes. An extensive set of experiments has been conducted on two offical datasets, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Nowadays, more attention has been placed on cost reductions and yield enhancement in the semiconductor industry. During the manufacturing process, a considerable amount of sensor data called status variables identification (SVID) is collected by sensors embedded in advanced machines. This data is a valuable source for data-driven automatic fault detection and diagnosis at an early manufacturing stage to maintain competitive advantages. However, wafer processing times vary slightly from wafer to wafer, resulting in variable-length signal data. The conventional approaches use much condensed data called fault detection and classification (FDC) data made by manually designed feature extraction. Or, recent deep learning approaches assume that all wafers have the same processing time, which is impotent to the variable-length SVID. To detect and diagnose faults directly from the variable-length SVID, we propose a self-attentive convolutional neural network. In experiments using real-world data from a semiconductor manufacturer, the proposed model outperformed other deep learning models with less training time and showed robustness at different sequence lengths. Compared to FDC data, SVID data showed better fault detection performance. Without manually investigating the lengthy sensor signals, abnormal sensor value patterns were found at the time specified by the model.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Prediction of protein subcellular location has currently become a hot topic because it has been proven to be useful for understanding both the disease mechanisms and novel drug design. With the rapid development of automated microscopic imaging technology in recent years, classification methods of bioimage-based protein subcellular location have attracted considerable attention for images can describe the protein distribution intuitively and in detail. In the current study, a prediction method of protein subcellular location was proposed based on multi-view image features that are extracted from three different views, including the four texture features of the original image, the global and local features of the protein extracted from the protein channel images after color segmentation, and the global features of DNA extracted from the DNA channel image. Finally, the extracted features were combined together to improve the performance of subcellular localization prediction. From the performance comparison of different combination features under the same classifier, the best ensemble features could be obtained. In this work, a classifier based on Stacked Auto-encoders and the random forest was also put forward. To improve the prediction results, the deep network was combined with the traditional statistical classification methods. Stringent cross-validation and independent validation tests on the benchmark dataset demonstrated the efficacy of the proposed method.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose an optimization approach for determining both hardware and software parameters for the efficient implementation of a (family of) applications called dense stencil computations on programmable general purpose computing on graphics processing units. We first introduce a simple, analytical model for the silicon area usage of accelerator architectures and a workload characterization of stencil computations. We combine this characterization with a parametric execution-time model and formulate a mathematical optimization problem that seeks to maximize a common objective function of all the hardware and software parameters. The solution to this problem, therefore, \u201csolves\u201d the codesign problem: simultaneously choosing software-hardware parameters to optimize total performance. We validate this approach by proposing architectural variants of the NVIDIA Maxwell GTX-980 (respectively, Titan X) specifically tuned to a predetermined workload of four common 2-D stencils (Heat, Jacobi, Laplacian, and Gradient) and two 3-D ones (Heat and Laplacian). Our model predicts that performance would potentially improve by 28% (respectively, 33%) with simple tweaks to the hardware parameters, such as tuning the number of streaming multiprocessors, the number of compute cores each contains, and the size of shared memory. We also develop a number of insights about the optimal regions of the design landscape.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An adaptive neural output consensus control issue is considered for stochastic nonlinear strict-feedback multi-agent systems (MASs). The traditional backstepping framework is employed combing with the graph theory, as well as neural networks (NNs) technology. NNs are utilized for the approximation of unknown functions, and the It\u00f4\u2019s lemma is used to deal with stochastic dynamics of the system. It is proved that all signals remain bounded in probability and that the tracking errors of all followers converge to a small neighborhood of the origin in the sense of mean quartic value by suitable choice of parameters. A simulation example is provided.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The proliferation of various mobile devices equipped with cameras results in an exponential growth of the amount of images. Recent advances in the deep learning with convolutional neural networks (CNN) have made CNN feature extraction become an effective way to process these images. However, it is still a challenging task to deploy the CNN model on the mobile sensors, which are typically resource-constrained in terms of the storage space, the computing capacity, and the battery life. Although cloud computing has become a popular solution, data security and response latency are always the key issues. Therefore, in this paper, we propose a novel lightweight framework for privacy-preserving CNN feature extraction for mobile sensing based on edge computing. To get the most out of the benefits of CNN with limited physical resources on the mobile sensors, we design a series of secure interaction protocols and utilize two edge servers to collaboratively perform the CNN feature extraction. The proposed scheme allows us to significantly reduce the latency and the overhead of the end devices while preserving privacy. Through theoretical analysis and empirical experiments, we demonstrate the security, effectiveness, and efficiency of our scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Power-over-fibre (POF) has been demonstrated for remotely powering microelectronic devices in hazardous environments [1] and in telecommunication and smart grid applications [2]. The technique can also be used for supplying power to surveillance cameras which reduces their vulnerability to tampering [3]. Ultimately, POF will have significant applications in the development and implementation of all-optical sensor networks. However, there is a limit to the amount of power that can be transmitted via the optical fibre due to processes such as stimulated Brillouin scattering (SBS) [4]. As such, the aim of this study is to optimise the power delivered to different parts of an optical fibre network, in order to minimise the effects of processes such as SBS. This can be achieved by enhancing the performance of the photovoltaic microcell (also called a photovoltaic power converter, PPC) and tailoring it to match the power requirements of sensors or actuators scattering throughout the network.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Telecommunications Engineering degree at Macquarie is undergoing renewal, simultaneously with a transformation in pedagogy by the School of Engineering and also a change in curriculum structure by Macquarie University. This work-in-progress paper reports a study of the effect of changes in Telecommunications Engineering education. These include updated technical content imparted through an educational approach which includes project-based learning (PBL), project ownership, replacement of traditional lectures, virtual laboratories and an emphasis on software tools and programming skills.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the Internet of Things (IoT), billions of physical devices are connected and provide a near real-time state of the world. By adopting a service-oriented computing paradigm, the capabilities of these devices (whether mobile or static) can be abstracted as IoT services and delivered to users in a demand-driven way. Service providers can have a comprehensive competitive edge by tailoring their services to match users' requests through a negotiation process, with the particular service provisioning specified in a service level agreement (SLA), which can be further used to monitor and guarantee the quality of service (QoS). The challenges for SLA negotiation in the IoT include the scale and the dynamics of the environment. Existing SLA negotiation approaches are focused on cloud computing, which do not consider the domain-specific properties of IoT services. In this paper, we designed a context-based negotiation strategy to evaluate offers and generate counteroffers, and integrate it with the WS-Agreement Negotiation standard. The evaluation results demonstrate that our proposal can produce a higher utility and maintain a good success rate compared to other negotiation strategies.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The prodigious data generated by all kinds of smart devices creates the need for an efficient and reliable platform which should be capable of managing huge data traffic. To overcome the acute load on the cloud and achieve low latency for real-time applications, the computation has been moved to distributed edge devices, i.e., fog/edge computing. However, the capability of end devices is not on par with the cloud. To address performance and reliability issues of these edge nodes, it is important to have one management platform that can solve reliability challenges and improve the overall service availability. In this paper, we propose to design a distributed edge computing platform by providing an easy management solution for remote distributed edge nodes. The major functions of our platform include service auto discover, lightweight container provisioning, GPU container support for deep learning applications, manage all active end nodes without a fixed IP address, secure remote deployment of edge analytic applications, and joint edge/cloud data communication and processing. To minimize the service downtime and guarantee high availability of edge services, we propose a real-time internal and external container migration scheme to achieve cooperative processing, load balancing, data backup, and service failover. Comparing with existing edge platforms, our solution achieves lightweight service management, low overhead, flexible and secure networking, and the significant reduction in latency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The requirements of the Certified Accountant (CC) profession are legally defined and the profession is recognized as being of public utility. The increasing challenges to which these professionals are subject have implied new requirements in terms of training, which are reflected in the training offered by Higher Education Institutions (HEI). In this sense, the SimEmp - Contabilidade web application is presented, which allows an approach to student-centered accounting teaching and \u201clearning by doing\u201d. This web-based computer application allows the use of active teaching methodologies based on Project Based Learning (PBL), and the students simulate a virtual business environment previously prepared for the acquisition of the skills and competencies that are required to these professionals in accounting, financial and tax. In addition, other characteristics of the students are put to the test, such as how they deal with stress, time management and teamwork.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Management of increasing amount of the electricity information provided by the smart meters is becoming more valuable and a very challenging issue in modern era, especially in residential sector for maintaining the records of consumers' consumption patterns. It becomes the necessity of retailers and utilities to provide the consumers more effective demand response programs for handling the uncertainties of their consumption patterns. In order to deal with the unceratian behaviours of the consumers and their unprecedented high volume of data, this work introduces the deep neuro-fuzzy optimizer for effective load and cost optimization. Three premises parameters: energy consumption, price and time of the day and two consequents parameters: peak and cost reduction are used for the opti-mization process of the optimizer. The dataset is taken from the Pecan Street Incorporation site and Takagi Sugeno fuzzy inference system is used for the evaluation of the rules developed from the memebership functions of the parameters. Membership Functions (MFs) are chosen as Guassian MFs for continuously monitoring the consumers' behaviours. Performance of this proposed energy optimizer is validated through the simulations which shows the robustness of optimizer in cost optimization and energy efficiency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The leader-following attitude consensus problem of multiple rigid body systems over switching networks was solved by the distributed observer approach. The resulting control law requires that the control of every rigid body system know the dynamics of the exosystem that produces the desired angular velocity. In this paper, we further consider solving the same problem by the adaptive distributed observer approach. The latter approach is able to provide for each rigid body system the estimation of the dynamics of the exosystem, thus leading to a fully distributed control law for solving the leader-following attitude consensus problem of multiple rigid body systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As part of ongoing research for agarwood oil quality classification, this paper presents the non-linear SVM modelling with polynomial as kernel parameter. The work involves of 96 agarwood oil collection, from different high qualities. The input for SVM modeling is the abundances (%) of volatile and the output is agarwood oil qualities either low or high. The experimental works are carried out automatically via MATLAB software version R2016a. The result showed that polynomial tuning kernel parameter has its capability in classifying agarwood oil volatile to high and low qualities. It was supported by 100 % obtained for accuracy, confusion matrix, sensitivity, precision and specificity. The finding in this study is important and benefits other future work focusing on agarwood oil research area.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: While the initial aim of smart meters is to provide energy readings for billing purposes, the availability of these measurements could open new opportunities for the management of future distribution grids. This paper presents a multilevel state estimator that exploits the smart meter measurements for monitoring both low and medium voltage grids. The goal of this paper is to present an architecture that is able to efficiently integrate smart meter measurements and to show the accuracy performance achievable if the use of real-time smart meter measurements for state estimation purposes was enabled. The design of the state estimator applies the uncertainty propagation theory for the integration of the data at different hierarchical levels. The coordination of the estimation levels is realized through a cloud-based infrastructure, which also provides the interface to auxiliary functions and the access to the estimation results for other distribution grid management applications. A mathematical analysis is performed to characterize the estimation algorithm in terms of accuracy and to show the performance achievable at different levels of the distribution grid when using the smart meter data. Simulations are presented, which validate the analytical results and demonstrate the operation of the multilevel estimator in coordination with the cloud-based platform.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Dynamic Hand Gesture Recognition is carried out in various studies to read patterns. Various sensors can be used to capture dynamic hand movement patterns. The results of the initial reading are usually in the form of raw data that must pass initial processing. Advanced processing is carried out to obtain features that will be trained using various classifiers. The recognition process without feature selection activities will reduce the accuracy of pattern recognition during the classification process. Seeing the many shortcomings in the implementation of the initial data processing, this study will present some initial processing examples to produce features that are relatively good for the data training process. The method used is the gaussian mixture model and the selection of predictors for the classification process. The sensor for recording dynamic hand movements used in this study is Leap-motion. There are three dynamic hand gestures were used in this study. The data used were 4609 coordinates spread in 30 features. The classifiers used are k-NN with Euclidean distance metric without feature selection process compare to the process with feature selection. The results obtained from this study are the availability of examples of feature selection models in the form of gaussian mixtures and some accuracy of the results of processing comparison. The lowest prediction, without and with feature prediction, has a slightly range from 99.7% to 100%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper studies a single-machine scheduling problem with a two competing agents in which the performance criteria of the first and second agents are to minimize the mean lateness and number of tardy jobs, respectively. Due to the non-deterministic polynomial-time hardness of this problem, we propose an effective and efficient algorithm, denominated as the SPT-M algorithm, to generate the non-dominated solutions of the Pareto set. Computational results conducted on a test problem set reveal that the proposed SPT-M algorithm can generate an efficient Pareto frontier in remarkably short computing time. The contribution of this paper could help practitioners to determine the tradeoffs between the jobs of two agents competing for a single resource.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the development of RGB-D sensors, the highquality color point cloud can be obtained conveniently. Besides the geometrical information of point cloud, the color has great potential to assist the point cloud registration. In this paper, we propose a registration method by adaptively combining with color moment information to improve the registration accuracy. Firstly, three kinds of central moments are used to characterize the color distribution of two point clouds. And then, we build the correspondence between point clouds by dynamically combining the geometric feature and color moment feature of each point, hence the corresponding points satisfy both geometric similarity and color similarity. Finally, for the partial registration problem in practice, we apply the trimmed ICP algorithm framework to calculate the rigid transformation. Experimental results demonstrate that our algorithm is more robust and accurate in dealing with point cloud geometry defects, missing data and poor initial position.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Integrating components of vehicular network with the Internet has emerged recently for fulfilling the ever-growing demand for a safe and comfortable driving experience. Road Side Unit (RSU) clouds help in serving requests arriving from vehicles but are limited by available resources. It is also required to reduce the overhead cost corresponding to Virtual Machine (VM) migrations caused by high mobility of vehicles. Also, requests need to be served within their stipulated delay constraint. In this work, we propose a novel algorithm to schedule requests at RSU clouds. Our algorithm efficiently schedules service requests from vehicles while taking care of their delay constraints and VM migrations. The simulation results confirm that our proposed scheduling technique helps in serving more number of requests within their stipulated delay constraint with lesser VM migrations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In systems subject to communication constraints, carefully scheduling the transmission of updated control values can greatly improve the trade-off between communication effort and control performance. In this letter, we consider a dynamical communication network together with a predictive controller that has explicit knowledge thereof. In the usual fashion of rollout strategies in networked control, the controller both schedules transmissions and computes the corresponding control values. Using tools from model predictive control, stability of the considered setup for nonlinear, constrained plants is established. The special case of linear plants is investigated in more detail. Furthermore, strict performance improvement over a feasible baseline control is established in case that the plant is additionally unconstrained. By means of a numerical example, effectiveness of the considered approach is demonstrated.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Effort has been made to find biomarkers for vascular dementia (VaD). Nevertheless, the current findings are typically obtained through statistical tests of group level differences. In clinical practice, however, it is more common to perform individual level inferences, e.g., to determine if a subject is suffering from VaD, which cannot be resolved with statistical analysis. The goal of this study is to develop a method to effectively discriminate early VaD patients from normal controls by combining EEG features with machine learning methods. The EEG signals were recorded from a total of 15 VaD patients and 21 controls during a visual oddball task. Interregional directed connectivity was derived from directed transfer function (DTF) analysis and used as features in classification. Three machine learning methods, including linear discriminant analysis (LDA), error back-propagation (BP) neural network, and support vector machine (SVM) were used as classifiers, and their classification performance was compared. It was found that VaD patients can be effectively identified using the BP and SVM classifiers with high accuracy. In particular, when the SVM classifier was combined with feature selection by Fisher score, it reached an accuracy 86.11%, sensitivity 86.67%, and specificity 85.71%. The area under the curve (AUC, 0.854) indicates a good identification of VaD patients from the normal controls. Since the EEG is noninvasive, inexpensive, and widely available to use, the current study presents a novel clinical application of machine learning methods and could facilitate automatic screening and diagnosis of the VaD at an early stage in future.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper is concerned with the sliding mode control (SMC) via finite-time stabilization (FTS) for a class of conic-type nonlinear semi-Markovian jumping systems (SMJSs). Comparing with the classical Markovian jumping systems, the transition rates of SMJSs are related to the random sojourn-time g. Based on this, a suitable SMC law for driving the state trajectories to the designed sliding surface within a finite-time interval is given. Then, the FTS over reaching phase and sliding motion phase is further proved to guarantee the FTS of the whole SMJSs. Finally, the effectiveness of the proposed method is demonstrated by a time-delayed Chua's circuit simulation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Human activity recognition substantiates itself as a useful tool applicable in many smart assistive technologies. Gathering, evaluating, and analyzing data in order to recognize human activities poses many issues, especially in healthcare applications. This paper seeks to answer the question what are the issues regarding the human activity recognition process in healthcare. After conducting a review of approaches used to implement each stage of a specific activity recognition process, the results indicated that issues mostly arise in data collection stage. Based on that, the applicability of related approaches (i.e., wearables, smartphones, and non-contact sensing) is discussed. Underlying issues are identified in order to serve as a starting point for future research activities.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: For the first time, the combination of mutual information analysis and correlation power analysis is proposed to enhance the accuracy and success rate of side channel analysis. Using the k-nearest-neighborhood (KNN) algorithm, correlation power analysis is combined with mutual information analysis to classify various possible keys to two classes of correct and wrong keys. The advantage of the combination of the distinguishers is two fold. First, the accuracy of the estimation is enhanced due to availability of multiple possible values for the correct key. Second, the number of measurements required to disclose the correct key is reduced by combining the distinguishers. The effectiveness of combined distinguisher is verified by extensive simulations. The number of measurements required to perform a side channel attack with a success rate of 90% is improved, respectively, by 20% and 49%, as compared to individual correlation power analysis and mutual information analysis.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In energy-from-waste plants, combustible waste is first carried into the process plant by garbage trucks, which unload the solid waste into a large waste pit. The solid waste collected in the waste pit is then carried into the incinerator using a waste crane. In order to stabilize the combustion process, it is important to keep the waste composition similar, by breaking the plastic containers and mixing the garbage well. Various constraints on crane operations including the limited space in the waste pit, time and energy conservation constraints, make fully automatic crane operation a very complex and difficult problem. In this paper, a fully automatic waste crane scheduler applying evolutionary computation is proposed. The proposed waste crane scheduler was successfully tested in an actual energy-from-waste plant, and it was verified that the waste deposited in the incinerator was mixed better using the proposed waste crane scheduler.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes fast convergent distributed algorithms for weighted average consensus of input data. For acyclic graphs, we give an algorithm that converges to the exact weighted average consensus in a finite number of iterations, equal to the graph diameter. For loopy (cyclic) graphs, we offer two remedies. In the first one, we give another distributed algorithm to enable our average consensus algorithm applicable to a loopy graph by converting it into a spanning tree. In the second one, we consider a slightly modified average consensus problem whose optimal solution approximates the consensus solution with arbitrary precision, and give a modified average consensus algorithm with guaranteed exponential convergence to the optimal solution. The proposed average consensus algorithms enjoy low complexities, robustness to transmission adversaries, and asynchronous implementation. Our algorithms are conceptually different from the popular graph Laplacian approach, and converge much faster than the latter approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Vision-to-language tasks aim to integrate computer vision and natural language processing together, which has attracted the attention of many researchers. For typical approaches, they encode image into feature representations and decode it into natural language sentences. While they neglect high-level semantic concepts and subtle relationships between image regions and natural language elements. To make full use of these information, this paper attempt to exploit the text-guided attention and semantic-guided attention (SA) to find the more correlated spatial information and reduce the semantic gap between vision and language. Our method includes two-level attention networks. One is the text-guided attention network which is used to select the text-related regions. The other is SA network which is used to highlight the concept-related regions and the region-related concepts. At last, all these information are incorporated to generate captions or answers. Practically, image captioning and visual question answering experiments have been carried out, and the experimental results have shown the excellent performance of the proposed approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Although the convolutional neural networks have obtained amazing performance in the area of image search, most of the existing methods are applied for natural images collected via normal lens without anamorphic distortion. Actually, there are great amounts of images collected with a circular fisheye lens to obtain larger field-of-view (FOV), especially in the area of natural science study. This paper aims to present a novel image search method with saliency deep features for those images, especially the aurora images used in solar-terrestrial space research. Our method exploits the advanced Mask R-CNN framework to extract semantic features. To utilize the unique physical characteristics of aurora and focus on the most informative local regions, we present a saliency proposal network (SPN) to take place in the region proposal network (RPN). In our SPN, different from the conventional rectangular gridding way, the proposed anchors show spherical distortion determined by imaging principle and magnetic information. In addition, instead of the horizontal directions, our anchor boxes direct perpendicular to the physical magnetic meridian, and thus ensure them to include the auroral structures within minimum areas. We perform numerous experiments on the big aurora image dataset, and the results prove the superiority of the proposed method over the state-of-the-art methods on both search accuracy and efficiency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The clustering technique is a kind of key method used to balance energy consumption in wireless sensor networks. It can increase the lifetime of the network and scalability. Energy-efficient clustering algorithms should be designed for the characteristic of homogeneous WSN. We propose and evaluate a new centralized energy-efficient clustering protocol for homogenous WSNs, which is called Distance energy evaluated DEE. In DEE, the cluster-heads (CHs), are elected by a probability based on the ratio between distance and residual energy of each node. The probability of being CH according to their initial and residual energy. Finally, the simulation results seemed that DEE achieves more effective messages and longer lifetime than current important clustering protocols in homogeneous environments.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we consider the usage of data mining techniques: statistical and spectral analysis and machine learning for analysis of wearable devises data what can be useful in task of Parkinsons's tremor detection task. The applied methods with the justification of their use are given. The scheme of their interaction is described. The study involved 8 patients with Parkinson's disease. Comparison of the considered algorithm and some of its modifications with the best method of tremor detection is made. According the research results the describing method has accuracy 0.98. That allows to use it in clinical practice.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Advertisements are an integral part of internet economics and culture, and video ads are the most popular and arguably the most entertaining form of advertisements. With the recent growth in digital marketing, video ads have seen unprecedented growth and are growing in importance as an advertising means. Video ads are expensive to create and are not always effective. The effectiveness of a video ad is usually not known before its deployment, which is non-ideal for creators, advertisers, and ad platforms. In this paper, we outline an idea to provide feedback before an ad is placed on its effectiveness based on the video along with the historical data about the effectiveness of other video ads. We propose a multi-modal mixture based algorithm to predict the effectiveness automatically. Specifically, we exploit rich textual information often found with an advertisement as well as visual information to learn a finite mixture model. Our experiments on a publicly available dataset show that our approach can outperform other baseline approaches.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper aims to study the Mittag-Leffler stabilization of an unstable time fractional hyperbolic partial differential equation by boundary control and boundary measurement. The backstepping method, the fractional Lyapunov method, and the semigroup theory are adopted in the investigation. A novel state feedback control via the Dirichlet boundary is designed to stabilize the controlled system. Based on the output signal, we first construct an observer that can recover the state of the original system, and then, we propose an observer-based stabilizing control law, under which the closed-loop system is shown to admit a unique solution and to be Mittag-Leffler stable. Finally, a benchmark example is presented to test the proposed theory.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Individual recognition of communication emitter can obtain the working mechanism, system parameters, and the usage model of the emitters based on the separation and interception of the specific emitter signals. In order to realize the individual recognition of communication emitter, a method of communication emitter recognition based on wavelet packet feature extraction and attribute reduction of granular computing is proposed. The method uses hierarchical and granularity to represent the subtle feature information of different emitter signals contained in the original data after the decomposition of wavelet packet, so that the feature attributes of different emitters are efficiently mapped to different levels of granular structure which can be used to search for an optimal reduced attributes operation for signal recognition. And then, the reduced attributes are used to distinguish the emitter signals correctly by label propagation method. The experimental results show that the proposed method based on wavelet packet feature extraction and granular computing attribute reduction has good classification performance in individual recognition of communication emitters.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper focuses on the finite-time control issue for a class of uncertain nonlinearly parameterized systems under arbitrary switching. By using the adding a power integrator technique (APIT), a new adaptive controller with a tuning parameter is designed. Different from the previous results, we develop a new common Lyapunov function (CLF) with a tuning parameter to achieve the global finite-time stability (GFTS) for the closed-loop switched systems. It is proved that all the states of the considered switched systems converge to an equilibrium state in finite time. Some simulations are given to illustrate the feasibility and advantages of the suggested approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Due to the growing complexities of Integrated Circuits (ICs) and the current trend of Intellectual Property (IP) based hardware designs, the possible threats of Hardware Trojans are inevitable. Trojans can be inserted during the design or fabrication phase of the IC design cycle by an untrustworthy third party. A Trojan once implanted in a system component can cause an unintended modification in the system functionality resulting in a considerable performance penalty or even a system crash. The malicious activity by a Trojan may include access to highly sensitive data, thus increasing the system vulnerability. A malicious Trojan once activated, can continue to inject memory transaction through the system interconnect. Such memory transactions may result in unnecessary invalidation broadcast as well as the eviction of valid cache lines and hence resulting in a huge degradation in system performance. Even a malicious activity by a Trojan can designate an upper level cache hit for a cache line to a cache miss resulting in a Denial-of-Service (DoS) attack. This type of attack in turn greatly increases lower level memory traffic. In this work, we have considered such stealthy Trojans and evaluate their effects on the performance of a real-time many-core system. We have used machine learning based techniques for run-time detection of such Trojans. Simulation results show more than 95% accuracy in detection.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Many mobile systems and wearable devices, such as Virtual Reality (VR) or Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID and password for signing into a virtual website. However, they are usually equipped with gesture capture interfaces to allow the user to interact with the system directly with hand gestures. Although gesture-based authentication has been well-studied, less attention is paid to the gesture-based user identification problem, which is essentially an input method of account ID and an efficient searching and indexing method of a database of gesture signals. In this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification framework that can generate a compact binary hash code from a piece of in-air-handwriting of an ID string. This hash code enables indexing and fast search of a large account database using the in-air-handwriting by a hash table. To demonstrate the effectiveness of the framework, we implemented a prototype and achieved \u226599.5% precision and \u226592.6% recall with exact hash code match on a dataset of 200 accounts collected by us. The ability of hashing in-air-handwriting pattern to binary code can be used to achieve convenient sign-in and sign-up with in-air-handwriting gesture ID on future mobile and wearable systems connected to the Internet.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In addition to controlling the influx of light to the retina, the pupil also reacts as a result of cognitive and emotional processing. This makes it possible to use pupil dilation as an index for cognitive effort and emotional arousal. We show how an extended version of a computational model of pupil dilation can account for pupillary contagion effects where the pupil of an observer dilates upon seeing another person with dilated pupils. We also show how the model can reproduce the effects of cognitive effort in a math exercise. Furthermore, we investigate how the model can account for different explanations for the abnormal pupil response seen in individuals with or at risk for autism spectrum disorder. The reported computer simulations illustrate the usefulness of system-level models of the brain in addressing complex cognitive and emotional phenomena.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With large-scale distributed generators (DGs) penetrated into the active distribution network (ADN), conventional load flow convergence failure is incurred by heavy power transmission. The Holomorphic Embedding Load Flow Method (HELM) has proven to be more robust than the Newton-Raphson method under heavy power transmission and is not sensitive to the initial points. At present, HELM is mainly designed for balanced transmission networks. In this study, we developed a three-phase HELM model to accommodate DGs, delta connection loads, and ZIP loads for ADN. The effectiveness and better performance of the proposed method under heavy load situations were validated using modified unbalanced IEEE 13, 34, 37, and 123 test feeders.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In general, aerial mapping is an image registration problem, i.e., the problem of transforming different sets of images into one coordinate system. Aerial mapping is one of the important capability of an unmanned aerial vehicle (UAV). Here, the images processed by the registration system is strongly influenced by the quality of the image captured by the UAV. To select the image that will be processed efficiently is not easy considering the ground truth in the mapping process is not given before the UAV flies and captures the image. On the other hand, generally, UAV will fly and take the image in sequence regardless of the quality. These will result in several issues, such as: 1) the quality of mapping results becomes bad, and 2) the computational cost of registration process becomes high. To tackle such issues, therefore, we need a recognition system that is able to recognize images that should be excluded from the registration process. In this paper, we define such image as an \u201cinclined image,\u201d i.e., images captured by UAV not perpendicular with the ground. Although we can calculate the inclination angle using a gyroscope attached to the UAV, our interest here is to recognize the images without the use of such sensor like human do. To realize that, we utilize a deep learning method to build an inclined image recognition system. We tested our proposed system with images captured by UAV. The results showed that the proposed system yielded accuracy rate of 86.4%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Full-Duplex (FD) and Device-to-Device (D2D) communications have been recognized as one of the successful solutions of spectrum scarcity in 5G networks. Significant advancements in self-interference-to-power-ratio (SIPR) reduction have paved the way for FD use to double the data rates and reduce the latency. This advantage can now be exploited to optimize dynamic spectrum sharing among different radio access technologies in cognitive networks. However, protecting the primary user communication has been a challenging problem in such coexistence. In this paper, we provide an abstract level analysis of protecting primary users reception based on secondary users FD enabled communication. We also propose optimal mode selection (Half-duplex, Full-duplex, or silent) for secondary D2D users depending on its impact on primary users. Our analysis presents the significant advantage of D2D mode selection in terms of efficient spectrum utilization while protecting the primary user transmission, thus, leading the way for FD enabled D2D setup. Depending on the location and transmit power of D2D users, the induced aggregate interference should not violate the interference threshold of primary users. For this, we characterize the interference from D2D links and derive the probability for successful D2D users for half-duplex and full-duplex modes. The analyses are further supported by theoretical and extensive simulation results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A fast time-delay estimator for linear frequency modulation signal is proposed. The new estimator, named by frFt-TELS, searches the maximal bin of fractional Fourier transform (frFt) as a coarse estimation. Then Taylor expansion of theoretical frFt power spectrum around the coarse estimation is deduced. Least squares approximation between the theoretical frFt power spectrum and the frFt power spectrum of the given sample data is applied to get the fine estimation. Simulations demonstrated that frFt-TELS approaches the performance of traditional estimator with less computational complexity.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a Three-Player Generative Adversarial Network to improve classification networks. In addition to the game played between the discriminator and generator, a competition is introduced between the generator and the classifier. The generator's objective is to synthesize samples that are both realistic and hard to label for the classifier. Even though we make no assumptions on the type of augmentations to learn, we find that the model is able to synthesize realistically looking examples that are hard for the classification model. Furthermore, the classifier becomes more robust when trained on these difficult samples. The method is evaluated on a public dataset for traffic sign recognition.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There are many onomatopoeias in Japanese language. Using onomatopoeias, Japanese people can communicate their own sense and image to others directly. Onomatopoeias have sound symbolism, so that the people who are familiar with Japanese can think metaphorically the image of onomatopoeias even if they are newly coined words for the people. However it is not easy for people who have different backgrounds from Japanese, to think metaphorically the meaning of semantic usage of Japanese onomatopoeias. To solve this problem, we have paid attention to phonological features and acoustic features of onomatopoeias. So far we have attempted to automatically classify some onomatopoeias into appropriate semantic usage categories by using these features. This paper adopted mimetic words regarding \"human action\" as example of test data for autoclassification. We investigated what features are useful among the phonological or acoustic features to classify the mimetic words.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Person re-identification is a technique that search the given target in the video surveillance network. This technique has been widely applied to security and surveillance system, and also become a research hotspot in computer vision. Person re-identification has been challenging due to the large number of cameras in the network and variation in camera angles, illumination, occlusion and poses. In this paper, we proposed a person re-id approach that can resist occlusions and variations based on a human pose guided convolution neural network framework with joint loss functions. We extract local features from body parts localized by landmarks, merge it with global features to learn the similarity metric. Identification loss and pose-constrained triplet loss function are jointly employed to train the model. Our approach outperforms most state-of-the-art methods on three large-scale datasets, with an accuracy of 83.31%, 86.1% and 72.6% on Cuhk03, Market1501 and Duke MTMC-reID respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A large number of mobile multimedia terminals are prominent features of smart cities. Device-to-device (D2D) communication takes advantage of the limited bandwidth resources of cellular networks to accommodate more mobile devices. However, when D2D pairs reuse cellular users channels, serious interference leads to energy consumption, which dissatisfies the requirements of green communication. This paper focuses on energy efficiency maximization of D2D communication under the constraints of both D2D pairs and cellular users quality of service. The formulated resource allocation problem is NP-hard, which is usually difficult to solve within polynomial time. To make the problem easy to handle, we divide it into power control and channel allocation sub-problems. In particular, we propose a power control algorithm based on the Lambert W function to maximize the energy efficiency of a single D2D pair. The preference values of D2D pairs and cellular users are calculated using the power control results, respectively. A channel allocation scheme based on the Gale-Shapley algorithm utilizes preference values to match two sides, which aims at maximizing the signal to interference plus noise ratio of cellular users and the energy efficiency of D2D pairs. The simulation results show that the proposed algorithm could not only guarantee the transmission rate of cellular users but also improve the system and D2D pairs energy efficiency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Trends in processor and system architecture, driven by power and complexity, point super-computing landscape toward very high and heterogeneous core count designs. As the number of cores inevitably increase traditional ways of performing large scale computations will also need to evolve from legacy models like OpenMP & MPI. However, such models are very much wedded to a control-flow vision of parallel programs, making it difficult to express asynchrony in programs. To address these challenges, codelet model was developed which is fine-grained, event-driven asynchronous program execution model. In-spite of its initial design goals, the Codelet model is rife with opportunities for further improvements to provide high-performance for both data and control regular applications. The major inspiration behind this work is to leverage the decades of research done to exploit instruction level parallelism (ILP) for the machine instructions inside codelet while build upon the dataflow software pipelining principals at codelet graph level to further enhance performance. In this paper, we propose hardware assisted extensions to the original codelet program execution model in order to implement efficient dataflow software pipelining and extend capabilities of the codelet model. This hardware-software co-design focuses on efficient implementation of data FIFO buffers leveraging proposed optimizations like - FIFO ring buffers and multiple-head FIFO buffers and single owner FIFO buffers to further exploit advantages of dataflow software pipelining. The wide range of scientific, machine learning and specially streaming applications should be able to take advantage of techniques proposed in this paper. Identifying these kernels and bench-marking them are the next anticipated steps for us.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The first generation of IoT was developed and deployed all over the world by connecting devices with common functionalities that were not sufficiently efficient or reliable for use in dynamic situations that require adaptive solutions. However, these fundamental IoT functions and services mainly targeted stable environments; there is consequently a strong need for the next generation of IoT services to be smarter, faster, and more reliable. We believe that the hyper-connected IoT ecosystem on fog platforms with contextual AI technologies is a promising solution. In this work, we introduce the EiF, a flexible fog computing framework that runs on IoT gateways with adaptive AI services fostered on the cloud. Our approach can be viewed as an integration of three emerging technologies, namely IoT, fog, and AI. Generally, EiF virtualizes an IoT service layer platform for fog nodes, and provides functions to manage and orchestrate various fog nodes; upon service virtualization and orchestration, AI services are fostered within both the federated cloud and distributed edge side and are deployed on fog nodes. We demonstrate the feasibility of EiF via the example of intelligent traffic flow monitoring and management.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet of Things (IoT) aims to address important challenges in our environment, industries, cities, homes and society by collecting, integrating and analysing data from potentially millions of sensors and other internet-connected devices. In this paper we propose a novel IoT-based solution that provides real-time detection of hydrocarbon pollution that can be generated by retail fuel outlets (which are also referred to as service or filling stations). Our solution includes a low-cost but highly accurate fibre optic sensor that can detect hydrocarbons in ground water and can be easily deployed in existing monitoring wells. These hydrocarbon sensors are a key part of an IoT sensor data collection and analysis platform that utilises commercially available sensor nodes to communicate via low power networks (e.g. LORA) to collect and analyse hydrocarbon pollution data in the cloud. This novel IoT platform that combines hydrocarbon sensing with cloud-based data analysis to produce continuously updated hydrocarbon pollution maps and alerts can detect and report hydrocarbon pollution in real-time. The platform could potentially collect hydrocarbon pollution levels from millions of such sensors deployed in thousands of service stations around the world and automatically analyse such data in the cloud to produce continuously and instantaneously updated hydrocarbon pollution maps and related alerts for individual service stations, corporate chains and environmental monitoring agencies.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With Von-Neumann computing struggling to match the energy-efficiency of biological systems, there is pressing need to explore alternative computing models. Recent experimental studies have revealed that Resistive Random Access Memory (RRAM) is promising alternative for DRAM. Resistive crossbar arrays possess many promising features that can not only enable high-density and low-power storage but also non Von-Neumann compute models. Most recent works focus on dot product operation with RRAM crossbar arrays, and therefore are not flexible to implement various logical functions. We propose a low-power dynamic computing in memory system which can implement various functions in Sum of Product (SOP) form in RRAM crossbar array architecture. We evaluate the proposed technique by performing simulation over wide range of MCNC benchmarks. Simulation results show 1.42X and 20X latency improvement as well as 2.6X and 12.6X power saving compared to static and MAGIC computing in memory methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Distribution system state estimation (DSSE) has recently been tested and experimentally deployed in some practical distribution networks. Distinct features of distribution systems, such as diverse and unsymmetrical configurations as well as limited real-time measurements, prohibit the direct application of mature state estimation methods for transmission systems. Targeting at three-phase four-conductor configured unsymmetrical medium-voltage distribution systems (MDS) with neutral conductors and ground resistances, this paper proposes a weighted least square-based DSSE approach, in which voltages are chosen as state variables and load pseudo measurements of low-voltage distribution systems (LDS) are considered to compensate insufficient real-time measurements in MDS. Both rectangular and polar coordinates are studied, and voltage variables of neutrals and zero-injection phases are eliminated to reduce the scale of the DSSE problem. Moreover, in order to enhance the load pseudo measurement accuracy of LDSs, a clustering and partial least square regression-based load estimation model is proposed to leverage the real-time communication ability of smart meters. Case studies on a modified IEEE 123-bus distribution system with actual smart meter data illustrate the effectiveness of the proposed approaches.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An accurate recognition of a dimensional variation pattern is very important for producing high-quality body-in-white (BIW). The wide application of optical coordination measurement machines (OCMM) in vehicle factory provided massive online dimensional data for the variation pattern recognition. However, the massive serially correlated or autocorrelated and 100% measurement data generated from the OCMM challenge the traditional statistical process control (SPC) technology and the common variation recognition approaches. This paper presents a novel deep-learning method, long short-term memory neural network (LSTM NN), to recognize the variation pattern of the BIW OCMM online measurement data. A comparative study between the backpropagation neural network (BP NN) and the LSTM NN was implemented, and the practicability of the proposed intelligent method was demonstrated by a case study. With the efficient use of time series information, the LSTM NN has a good performance in variation patterns\u2019 recognition and high practicability in improving the quality of the BIW.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper considers the problem of face sketch synthesis in the wild, which transforms a face photo into a face sketch. Face sketch synthesis is widely applied in law enforcement as well as digital entertainment fields. However, the existing methods either focus on hand-crafted techniques where prior human experience is relied on or adopt deep learning techniques as an end-to-end framework, where facial details cannot be well represented. In this paper, we propose a novel approach for face sketch synthesis in the wild via a deep patch representation-based probabilistic graphical model (DeepPGM). A Siamese network is constructed to extract deep patch representation from a raw facial patch, where the representative detail information for robust face sketch synthesis can be exploited. The generated deep patch representation and facial image patches are then optimally combined through a probabilistic graphical model. The proposed DeepPGM approach not only outperforms the state-of-the-art on public face sketch datasets but also can cope with forensic photos in the wild conditions, including varying lightings, poses, occlusions, skin colors, and ethnic origins. The superiority of the proposed method is demonstrated by extensive experiments on two public face sketch datasets and real-world forensic photos in the wild.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the introduction of autoscaling, clouds have strengthened their position as self-adaptive systems. Nevertheless, the reactive nature of the existing autoscaling solutions provided by major Infrastructure-as-a-Service (IaaS) cloud services providers (CSP) heavily limits the ability of cloud applications for self-adaptation. The major reason of such limitations is the necessity for the manual configuration of the autoscaling rules. With the evolution of monitoring systems, it became possible to employ the data-driven approaches to derive the parameters of scaling rules in order to enable the autoscaling in advance, i.e. the predictive autoscaling. The change in the amount of requests to microservices could be considered as a reason to adapt the virtual infrastructure underlying the cloud application. By forecasting the amount of requests to cloud application, it is possible to estimate the upcoming demand to replicate the microservices in advance. Hence, anticipation of the demand on the cloud application helps to evolve its self-adaptive properties. In the scope of the paper, the authors have tested various extrapolation models on the real anonymized requests time series data for 261 microservices provided by the industry partner Instana. The tested models are: various seasonal ARIMA models with GARCH modifications and outliers detection, exponential smoothing models, singular spectrum analysis (SSA), support vector regression (SVR), and simple linear regression. In order to evaluate the accuracy of these models, an interval score was used. The time required to fit and use each model was also evaluated. Comparative results of this research and the classification of forecasting models based on the interval accuracy score and model fitting time are provided in the paper. The study provides an approach to evaluate the quality of forecasting models to be used for self-adapting cloud applications and virtual infrastructure.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: When using standard learning techniques for uncertainty approximation, persistently exciting inputs are necessary in order to achieve good parameter estimation. It has been shown in recent years that, through concurrent learning (CL), it is possible to achieve better learning without requiring persistency of excitation. We define good learning here by how well an uncertainty can be reconstructed given the estimated parameters. Most studies concerning CL have however been done in the continuous-time (CT) framework. While working with discrete-time (DT) structured uncertainties, we have shown in an earlier study that the concept of CL could be used to solve the parameter identification problem granted, much like in the CT domain, a less restrictive condition compared to that of persistency of excitation is verified. This paper furthers our fundamental study of CL in the DT framework while drawing comparisons with the traditional but fundamental gradient descent technique. As a main contribution, via formal derivations, we present a generalized gradient-based CL motivated DT algorithm for online approximation of both DT structured and unstructured uncertainties. Numerical simulations are provided to show how well the designed algorithm leverages memory usage to achieve better learning.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Since deep neural networks (DNNs) are very intensive in terms of memory, compressing them with weight sharing is known to be an effective technique to deploy the networks on embedded systems with limited on-chip memory capacity, resulting in a considerable saving of energy consumption. However, the extreme compression sacrifices the accuracy of DNNs. In this context, this work introduces two unique optimization problems and proposes solutions for DNNs with \u201cunfitted compression\u201d to maintain the accuracy, in which all distinct weights of the compressed DNNs could not be entirely contained in on-chip memory. Precisely, given an access sequence of weights, (1) the first problem is to arrange the weights in off-chip memory, so that the number of memory accesses to the off-chip memory (equivalently the energy consumed by the accesses) be minimized, and (2) the second problem is to devise a strategy of selecting a weight block in on-chip memory for replacement when a block miss occurs, with the objective of minimizing the total energy consumed by the off-chip memory accesses and the overhead of scanning indexes for block replacement. Through experiments with the model of compressed AlexNet, it is shown that our solutions are able to reduce the total energy consumption of the off-chip memory accesses including the scanning overhead by 34.2% on average over the use of unoptimized memory layout and LRU replacement scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Lane information is essential for safe autonomous driving. In this article, we present a multisensor fusion framework for ego and adjacent lanes with a novel fusion quality measure and dynamic lane mode strategies for erroneous management. The framework fuses road marking lines based on Dempster-Shafer theory and tracks lanes with a particle filter. Then, a quality measure for each line is computed, integrating sensor coherence, availability as well as temporal continuity. This quality is essential to deploy different lane management strategies in order to avoid integrating erroneous data. The proposed framework was evaluated in a lateral control architecture with autonomous driving on open roads and proved its robustness and availability.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Proof-of-Work (PoW) in Blockchains (BC), which is a widely used consensus algorithm, suffers from high power consumption of miners and low transaction rates. This work demonstrates a Proof-of-Stake (PoS)-based BC called Bazo, which is specially designed and adapted for Internet of Things (IoT) data streams. Bazo displays enhanced performance in terms of energy consumption and transactions processing in comparison to PoW-based BC. To further improve performance of Bazo, sharding and transaction aggregation methods are employed. Moreover, IoT-BC adaptation helpers of a modular and layered architecture are provided to allow wireless devices to submit data into the BC. The designed architecture can support multiple hardware and software platforms as well as network technologies.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Uniform opposition-based particle swarm optimization (NOPSO) is proposed to overcome the drawbacks, such as, slow convergence speed, falling into local optimization, of opposition-based particle swarm optimization. Two mechanisms are introduced to balance the contradiction between exploration and exploitation during searching process. 1) Firstly, a new particle's position update rule in which uniform term replaces the inertia term is designed to accelerate its convergence; 2) Secondly, an adaptive elite mutation strategy (AEM) is included to avoid trapping into local optimum. Experimental results show that the proposed method has a significant improvement in performance compared with some state-of-art PSOs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Automated Tongue Diagnosis (ATD) is a growing research field in recent years due to global demand of personal health care. Automated tongue segmentation is the fundamental technology for automated tongue diagnosis. It is significant to find an efficient and accurate tongue segmentation algorithm for mobile and embedded devices. However, researches in this field are inadequate. Existing methods are either not efficient or not accurate enough. Our paper first addresses these problems by presenting a class of efficient cascaded CNN models for tongue detection and segmentation, which is both lightweight and accurate compared with other high-performance algorithms. Furthermore, a new annotation method is proposed to reduce the workload of establishing a tongue segmentation database.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The cloud data centers are going through an unprecedented growth from past few years. In an era of real-time video streaming, on-demand gaming, door-step e-commerce services, and highly inter-connected social networks, cost-effective service models, adaptive resources provisioning and upfront applications availability contribute significantly towards such a stellar growth. However, there are many challenges that must be addressed in a systematic manner to meet the requirements of increasingly demanding current and upcoming applications of the cloud computing paradigm. Optimum resources management, instant response time, interoperability among a diverse set of emerging technologies and innovative applications are a few of these challenges. On the other hand, the recent trend in softwarization of networks, particularly enabled by network function virtualization (NFV) and software-defined networking (SDN) principles, provides immense opportunities to better utilize the network resources by programmable abstractions with an efficient control and management techniques. Furthermore, machine learning based solutions are gaining prominence in resource optimization problems and autonomous systems. Therefore, in this paper, we strive to connect the dots by state-of-the-art methodologies in networking and machine learning domains and utilize these developments to grapple with the challenges of the cloud-based systems. We propose DeepSDN, an SDN-based solution that harnesses existing machine learning techniques to move a step closer towards self-driving networks. The comparative results obtained from an experimental testbed corroborates effectiveness of our approach and suggest a way forward towards autonomous network management.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The main challenges of the anatomical segmentation of automated whole breast ultrasound (AWBUS) image are shadow effect, blurred boundary, low contrast and large target. To tackle them, a novel and effective framework named AttentionNet is developed via self-attention mechanism during both feature extraction and up-sampling phase. Specifically, features are firstly extracted based on ResNeXt-50 to explore the information of intra-channels. With the goal of extracting features and utilizing channel information effectively, a module named spatial attention refinement (SAR) is devised using the basic ResNeXt-50 module (a.k.a., ResNeXt-SAR). Then, a weighted up-sampling block (WUB) module for precise pixel localization is designed by introducing high-level semantic concept during up-sampling phase, playing an important role in guiding the low-level features by the category information. The extensive experiments are conducted on AWBUS image for multi-class image segmentation. Our proposed AttentionNet achieves the superior results over the state-of-the-art approaches and may help to assist the calculation of breast density.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Analog-to-Information Converter (AIC) is the physical implementation of Compressed Sensing (CS) systems to digitize signals directly from the analog domain. Due to non-idealities of components AIC hardware needs a calibration step to adjust parameters before its utilization. In this paper, we consider a theoretical linear model of an AIC architecture composed by parallel channels that suffers from synchronization problems between the input signal and the internal signals used to perform the measurement. Using a radar inspired technique we propose a method to estimate the delay between these signals. Obtained results show the accuracy of the proposed method and its potential to be used in AIC calibration techniques.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Endoscopy is a routine imaging technique used for both diagnosis and minimally invasive surgical treatment. While the endoscopy video contains a wealth of information, tools to capture this information for the purpose of clinical reporting are rather poor. In date, endoscopists do not have any access to tools that enable them to browse the video data in an efficient and user friendly manner. Fast and reliable video retrieval methods could for example, allow them to review data from previous exams and therefore improve their ability to monitor disease progression. Deep learning provides new avenues of compressing and indexing video in an extremely efficient manner. In this study, we propose to use an autoencoder for efficient video compression and fast retrieval of video images. To boost the accuracy of video image retrieval and to address data variability like multi-modality and view-point changes, we propose the integration of a Siamese network. We demonstrate that our approach is competitive in retrieving images from 3 large scale videos of 3 different patients obtained against the query samples of their previous diagnosis. Quantitative validation shows that the combined approach yield an overall improvement of 5% and 8% over classical and variational autoencoders, respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a programmable dynamic simulation interface (PDSI)-based online fault configuration approach of network simulation is proposed to dynamically set link fault as well as route fault in the case of joint power grid simulation. The proposed approach allows the user to dynamically configure the fault situations of communication network via a graphical user interface (GUI) during the simulation process. When QualNet receives the configuration messages from GUI, it will parse the messages and push the corresponding events into the event queue to close the link port or rewriting the routing tables, which is utilized to formulate the link fault or the route fault. Experiment results demonstrate that based on the proposed approach the fault configuration of communication network can be successfully performed with an acceptable compromise at interface interaction delay.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A class of the preventive and reactive cyber defense dynamics has recently been proven to be globally convergent, meaning that the dynamics always converges to a unique equilibrium whose location only depends on the values of the model parameters (but not the initial state of the dynamics). In this paper, we unify the aforementioned class of preventive and reactive cyber defense dynamics models and the closely related class of N-intertwined epidemic models into a single framework. We prove that the unified dynamics is still globally convergent under some mild conditions, which are naturally satisfied by the two specific classes of dynamics models mentioned above and are inevitable when analyzing a more general framework. We also characterize the convergence speed of the unified dynamics. As a corollary, we obtain that the N-intertwined epidemic model and its extension are globally convergent, together with a full characterization on their convergence speed, which is only partially addressed in the literature.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The Internet of symmetric traffic flows between networks and a hierarchical topology, has long given way to one with a significantly asymmteric traffic flows and a flatter topology. The Internet topology of today may be characterised as having three key types of networks-content providers, user access providers, and transit providers. Further, in this Internet, best-effort routing of centrally stored content using distributed protocols has been seen to be inadequate to provide a suitably reliable transport service to provide the requisite quality of service to the end user. Two important developments to mitigate this gap in the capability of the Internet and the needs of modern content are the following. (1) Proliferation of content distribution networks (2) direct peering between content networks and ISPs. In this paper we analyze the economics of such interconnections. Using microeconomic models from industrial organization literature, we first develop the conditions for a content provider to connect directly to a service provider. Further, when such a direct link is indeed sought, we analyze the quality of the link vis-a-vis the default option of using a transit service. We then analyze the content provider market coverage by the content distribution networks. Finally, we discuss the implications of these results on the objectives sought by net neutrality regimes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recently, visible light communication (VLC) networks have emerged as a promising alternative for indoor data access, due to high data rate, low implementation cost, and immunity to radio frequency (RF) interference. However, the co-existence of VLC with the RF access points as well as the dependence of VLC to room illumination compel both technologies to work in parallel and thus, to form a hybrid heterogeneous VLC/RF network. This network offers the advantages of both technologies, namely increased capacity and ubiquitous coverage. Furthermore, non-orthogonal multiple access (NOMA) is a very promising candidate technique for the next generation of wireless networks, mainly due to its increased spectrum efficiency compared to orthogonal access schemes. However, the optimal user grouping in NOMA is a combinatorial NP-complete problem, which calls for low complexity techniques. To this end, in this paper, we propose the use of coalitional game theory, where the users served by the same access point (VLC or RF) form a single coalition, while the users can switch through coalitions based on their payoff. A novel utility function is proposed that takes into account the peculiarities of the NOMA hybrid VLC/RF network. Finally, a coalition formation algorithm is presented as well as an efficient power allocation policy. Computer simulations validate the presented analysis and reveal the effectiveness of the proposed user grouping scheme compared to an opportunistic approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Voice-over IP (VoIP) technology is a kind of digital transmission technology based on IP network. It is one of the important methods to use voice service in VoIP as steganographic carrier to ensure secure transmission. However, the traditional steganographic code has some problems, such as low embedding efficiency and weak concealment, which cannot meet the requirements of VoIP streaming media information hiding for the security of secret information. Therefore, a steganographic algorithm combining F5 and simplified wet paper code (SWPC) algorithm is proposed. The main idea is to embed secret messages in each row of the carrier matrix using the F5 algorithm, and then the SWPC algorithm is used to embed the columns according to the wet and dry characteristics of the wet paper code without affecting the results before row embedding. We use the VoIP streams encoded by the ITU-T G.729a codec as a carrier to verify the proposed scheme. The experimental results demonstrate that the proposed scheme can achieve relatively better IP speech data steganographic transparency and that it can outperform F5-WPC and SWPC approaches.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Integrated circuit (IC) camouflaging is a promising defense against so-called IC extraction attacks that seek to reverse engineer the netlist of a packaged IC using delayering and imaging techniques. Camouflaging works by hiding the Boolean functionality of selected gates in the netlist from reverse engineering, albeit at the cost of increased gate area and power. The intuitive security claim then is that the attacker cannot infer the netlist's exact Boolean functionality. This paper describes a powerful class of attacks on IC camouflaging referred to as SAT attacks; the attacks use the input/output (I/O) behavior of a functional camouflaged IC along with the Boolean satisfiability (SAT)-based inference to reverse the Boolean functionalities of camouflaged gates. The SAT attack is rooted in a foundational complexity theory mindset and is shown to defeat defenses that previously claimed to secure against even the most determined adversaries. This paper then highlights the subsequent impact of the SAT attack in terms of new SAT-resilient defenses that emerged, their vulnerability to enhancements of the SAT attack, and implications of the attack on provably secure defense mechanisms.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The booming of High Definition (HD) and UltraHD (UHD) videos in the last decade has greatly promoted the use of video display on diversified terminals such as TV, tablets and smartphones. To present more video contents with high quality to users, it is imperative to assess the visual quality of videos. The approach denoted as Video Quality Assessment (VQA) has become an appealing problem to researchers. In this work, we take the video contents into consideration besides of media format and pixel quality. This paper utilizes four measures, picture resolution, bitrates, Spatial Information (SI) and Temporal Information (TI), to represent the visual quality in four separate dimensions. Then, a subjective database is constructed to train a neural network that is capable of scaling content-aware video qualities. Experiments have shown that our method is superior to human observers in terms of correlations to Mean Opinion Score (MOS) values.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper investigates a grant-free non-orthogonal multiple access (NOMA) system where massive number of users wake up and send data right away without performing a grant-based initial access procedure. Since no scheduling grant is used in this system, the base station (BS) does not know which user is transmitting on each resource. Also, due to the lack of an uplink (UL) timing control process, multiple user's signals are asynchronously received at the BS. This causes asynchronous interference, or multiuser inter-carrier and inter-symbol interference. We propose two ideas to address these two problems. We first propose an auxiliary preamble structure to successfully detect the user activity, even in the presence of a massive number of users. We, then, propose a modification to the interleave-division multiple access (IDMA) receiver to mitigate asynchronous interference. The simulation results show that the proposed scheme significantly improves the preamble detection performance and BER performance compared to the conventional schemes. Furthermore, we show that the proposed grant-free NOMA system can achieve much better performance than the grant-based NOMA system in terms of transmission time and signaling overhead.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The development of Vehicular Ad-hoc NETwork (VANET) has brought many conveniences to human beings, but also brings a very prominent security problem. The traditional solution to the security problem is based on centralized approach which requires a trusted central entity which exists a single point of failure problem. Moreover, there is no approach of technical level to ensure security of data. Therefore, this paper proposes a security architecture of VANET based on blockchain and mobile edge computing. The architecture includes three layers, namely perception layer, edge computing layer and service layer. The perception layer ensures the security of VANET data in the transmission process through the blockchain technology. The edge computing layer provides computing resources and edge cloud services to the perception layer. The service layer uses the combination of traditional cloud storage and blockchain to ensure the security of data.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This article describes a specially developed system for automatic control of temperature in the weld area during lap welding. The major aim of the system is to hold the required temperature constant while joining two plastic materials differing in optical properties, where one part is transparent to the laser radiation and the other absorbs it. The system uses a real-time reconfigurable National Instruments CompactRio controller featuring conditioned input-output modules. The control application, based on fuzzy logic control, was created using LabVIEW software. A sampling rate of 1kHz was employed. The feedback signal is derived from an optical pyrometer, which measures the temperature in the fusion zone. The study involved empirically determining the optimal settings and testing the controller performance. The article includes waveform data concerning the process variable, the control signal and the error signal.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent research, clustering protocols have gained a lot of interest in heterogeneous wireless sensor networks (HWSNs) because of handling many critical applications. This paper proposes the extension of MICHB (Modified Intelligent cluster head election based on Bacterial foraging optimization) for three level HWSNs. For this, we apply the MICHB algorithm to the existing protocols (ESEP and EDEEC), and propose MIESEP and MIEDEEC protocols which search sensor nodes (SNs) of high residual energies for cluster heads (CHs) election in distributed HWSNs. Consequently, provides minimized energy consumption, elongated stable period and proper load distribution in the network. Simulation results validate that proposed MIESEP and MIEDEEC protocols are capable in enhancing the overall network performance in distributed HWSNs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Current SDN controllers are not cognitive. We propose a new architecture for an SDN controller to enable intelligence. The proposed new architecture is based on Multi-agent systems. As a prototype, we have built a MAS-SDN controller using the GOAL agent programming language. We highlight the motivation behind the new architecture, describe the architecture and provide some initial results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Brain-Computer Interface (BCI) combined with assistive robots has been developed as a promising method for stroke rehabilitation. However, most of the current studies are based on complex system setup, expensive and bulky devices. In this work, we designed a wearable Electroencephalography(EEG)-based BCI system for hand function rehabilitation of the stroke. The system consists of a customized EEG cap, a small-sized commercial amplifer and a lightweight hand exoskeleton. In addition, visualized interface was designed for easy use. Six healthy subjects and two stroke patients were recruited to validate the safety and effectiveness of our proposed system. Up to 79.38% averaged online BCI classification accuracy was achieved. This study is a proof of concept, suggesting potential clinical applications in outpatient environments.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Annotation of the perceived emotion of a music piece is required for an automatic music emotion recognition system. Most music emotion datasets are developed for Western pop songs. The problem is that a music emotion recognizer trained on such datasets may not work well for non-Western pop songs due to the differences in acoustic characteristics and emotion perception that are inherent to cultural background. The problem was also found in cross-cultural and cross-dataset studies; however, little has been done to learn how to adapt a model pre-trained on a source music genre to a target music genre of interest. In this paper, we propose to address the problem by an unsupervised adversarial domain adaptation method. It employs neural network models to make the target music indistinguishable from the source music in a learned feature representation space. Because emotion perception is multifaceted, three types of input feature representations related to timbre, pitch, and rhythm are considered for performance evaluation. The results show that the proposed method effectively improves the prediction of the valence of Chinese pop songs from a model trained for Western pop songs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Advances in automated face analysis have made possible webcam-based assessment of viewer emotion during presentation of commercials and other video content. A key assumption of this technology is that viewer emotion is in response to the media. Is that assumption warranted? Because viewer attention is seldom assessed, emotional responses could result from other sources, such as talking to a friend, enjoying a meal, or attending to a pet. We developed a CNN-LSTM approach that detects attention and nonattention to commercials using webcam and mobile devices in settings of viewer's choice. Because cultural variation in viewer response is likely, we included participants from both Western and Eastern countries. Participants were 28,911 adults (ages 18 to 69 years) in Europe, USA, Russia, and China. A total of 15,543 sessions (ca. 6.5 million video frames) was analyzed. Accuracy was quantified using a variety of metrics. Our approach outperformed baseline and achieved moderate to high accuracy that approached that of human annotators.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Identification and classification of Power Quality (PQ) events have been a subject of recent interest for researchers from the point of view of initiating suitable control actions in Distributed Generation Systems (DGs) for achieving improved performance. This requires carrying out a detailed analysis of the various characteristics of real time electrical signals through a set of signal processing techniques. In this context, two major parameters are considered to arrive at suitable control actions needed for monitoring and control of DGs. These parameters include PQ distortions due to environmental factors, such as change in wind speed and solar irradiations. Signal features are extracted using S-Transform while signal-classification is done by Least Square Support Vector Machine (LS-SVM) technique. A 17-bus test system is modeled using the open source software, Open Distribution System Simulator (OpenDSS). LS-SVM and S-Transform are implemented using MatLab. Smart converter control is realized with inputs received from signal classifier, so as to initiate proper grid-support functions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose an extension to Taylor model of opinion dynamics in social networks. Motivated by bounded confidence models, we propose an extended Taylor model in which the influence graph is built based on the confidence levels of individuals. Furthermore, we establish a link between structural controllability of this model and containment of individuals' opinions by the sources. In addition, an algorithm is proposed to find the minimum values of the confidence levels ensuring that structural controllability and containment can be achieved. An illustrative example is provided to verify the proposed scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Image stitching is an important part of computer vision, and how to do it more efficiently with high quality is a heated topic. In this paper, the authors propose a new method called TMGA for image stitching to get an improved performance in calculating Transform Matrix by using Genetic Algorithm. The proposed TMGA not only counts the number of interior points, but also takes standard error and degree of dispersion into consideration compared the traditional methods. The results demonstrate that the proposed algorithm can gain a high-quality transform matrix and improves the result of the stitching.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Falls are a primary accident for elderly people living independently. Obviously, timely and accurate fall detection is critical to reduce the injuries and avoid the loss of life. In order to improve existing smartphone-based fall detection systems, this paper investigates the features of triaxial acceleration values acquired from built-in accelerometers of a smartphone, identifies crucial thresholds of the falls and non-falls, and then proposes an enhanced threshold-based fall detection approach, which could not only distinguish fall events from the most of daily activities (including walking, running, and sitting down), but also support four directions (forward, backward, left lateral, and right lateral) of the falls. In addition, once a falling accident is identified, the user position would be instantaneously transmitted to an emergency center in order to have timely medical assistance. As a consequence, experimental results show the feasibility of our enhanced approach with accuracy and detection rates of 99.38% and 96%, respectively, while a set of 650 test activities including 11 different kinds of daily activities are performed.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Objective: In this paper, a framework to visualize and model internal fixation plates is presented for computer-aided personalized and minimally invasive curved bone fracture surgery. Methods: We focus on personalized reverse reconstruction of the bone fracture plate based on three-dimensional (3-D) mesh models obtained from a 3-D optical scanner. The steps of the method are as follows. First, principal component analysis and the K-means method are used to reconstruct a Bezier curve (ridge line) of broken bones. Second, based on the geometric shape of the curved broken bones, a capsule projection model of the broken bones is proposed to obtain the feature information of the broken bone sections. Third, the ordering points to identify the clustering structure (OPTICS) method is utilized for preregistration (rough registration). Fourth, a regional self-growth strategy is designed to extract the cross-section points. Fifth, the iterative closest point method is applied for the accurate registration of the fracture surface models. Finally, a personalized internal fixation plate model is reconstructed based on several user points. Results: The internal fixation plate model can be reconstructed according to the patient's bone parameters. Conclusion: Clinicians can use this framework to obtain personalized and accurate internal fixation plate models that effectively represent the broken bones of patients. Via X-ray navigation, the personalized forged plate can be fixed on the target area through a small incision. Significance: This framework provides a reasonable and practicable technical approach for computer-aided minimally invasive curved bone fracture surgery.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We describe a distributed framework for resource sharing problems that we face in communications, microeconomics and various networking applications. In particular, we consider a hierarchical multi-layer decomposition for network utility maximization (NUM), where functionalities are assigned to different layers. The proposed methodology creates solutions having central management and distributed computations. The technique aims to respond to the dynamics of the network by decreasing the communication cost, while shifting more computational load to the edges of the network. The main contribution of this work is the provision of a detailed analysis under the assumption that the network changes are in the same time-scale with the convergence time of the algorithms used for local computations. For this scenario, assuming strong concavity and smoothness of the users' objective functions, we present convergence rates for each layer.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing is an Internet-based service which provides shared virtual resource and data to accomplish certain computation. In order for the servers to have sufficient resources when the request arrives, as well as save server resources as much as possible, we propose a prediction-based server capacity planning and dynamic scheduling algorithm. There are mainly three steps in our capacity planning algorithm. The first step characterizes the given data on several indices and then present an effective model in order to predict the oncoming demands in the near future. The second step generates the workload of servers combined with the predicted demands and then make capacity planning based on this workload. Thus it's obvious that the effectiveness of capacity planning depends on the accuracy of prediction to a great extent. Finally, a demand prediction based strategy on workload allocation is brought out. A dynamic resource allocation strategy is given to ensure the quality of service at any moment in future meanwhile taking energy consumption into consideration. The results of the experiment show that the required server number decreases by 33% after the prediction-based capacity planning applying on server scheduling.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Increasing longevity remains one of the open challenges for Lithium-ion (Li-ion) battery technology. We envision a health-conscious advanced battery management system, which implements monitoring and control algorithms that increase battery lifetime while maintaining performance. For such algorithms, real-time battery capacity estimates are crucial. In this paper, we present an online capacity estimation scheme for Li-ion batteries. The key novelty lies in: 1) leveraging thermal dynamics to estimate battery capacity and 2) developing a hierarchical estimation algorithm with provable convergence properties. The algorithm consists of two stages working in cascade. The first stage estimates battery core temperature and heat generation based on a two-state thermal model, and the second stage receives the core temperature and heat generation estimation to estimate state-of-charge and capacity. Results from numerical simulations and experimental data illustrate the performance of the proposed capacity estimation scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper explores the application of machine learning techniques in finance to predict if a publicly-traded firm will issue a Seasoned Equity Offering (SEO) by analyzing the firm's 10-Q filing documents with Security and Exchange Commissions (SEC). Specifically, using the information content in the Management Discussion and Analysis section (MD&A) of 10-Q filings, we train five different algorithms, including Logistic Regression (LR), Support Vector Classification (SVC), Multinomial Na\u00efve Bayes (NB), Artificial Neural Network (ANN) and Random Forest (RF). Two types of features, unigrams and phrases are considered. Term frequency-inverse document (TF-IDF) scores are used as independent variables in these models. Experimental results show that the accuracy of phrases-only models has a range of 0-2% improvement for LR, NB, and RF compared with unigrams-only models. The accuracy of phrase-only model for SVC is close to that of unigrams-only model. The 74.53% accuracy of unigrams-only model for SVC classifier performs the best among all tested classifiers. The precision of all models varies between 60% and 75%, while the recall varies between 55% and 85%. Further, we tune model parameters of one linear model (LR) and one non-linear model (RF) to see how these parameters will impact the models' performance. Finally, we apply RF to find the most important features on prediction and find that \"merger\" is the most important feature in both unigrams-only model and phrases-only model. We conclude that text mining with SEC financial document filings could be an effective tool to predict important corporate events such as SEO.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Detecting anomalous behavior in wireless spectrum is a demanding task due to the sheer complexity of the electromagnetic spectrum use. Wireless spectrum anomalies can take a wide range of forms from the presence of an unwanted signal in a licensed band to the absence of an expected signal, which makes manual labeling of anomalies difficult and suboptimal. We present, Spectrum Anomaly Detector with Interpretable FEatures (SAIFE), an Adversarial Autoencoder (AAE) based anomaly detector for wireless spectrum anomaly detection using Power Spectral Density (PSD) data which achieves good anomaly detection and localization in an unsupervised setting. In addition, we investigate the model's capabilities to learn interpretable features such as signal bandwidth, class and center frequency in a semi-supervised fashion. Along with anomaly detection the model exhibits promising results for lossy PSD data compression up to 120X and semi-supervised signal classification accuracy close to 100% on three datasets just using 20% labeled samples. Finally the model is tested on data from one of the distributed Electrosense sensors over a long term of 500 hours showing its anomaly detection capabilities.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Smart healthcare technology is one of the highest explored areas which apply modern computing technologies and techniques in healthcare research. By making use of sensors in smart wearable devices, the patient-generated data can be sent to electronic devices or any health records. This enables doctors/caregivers to directly monitor the patient activity in real-time. Moreover, a high volume of medical information is continuously produced with every passing day. It is an intrinsic need to gather, store and learn from the medical data to predict health of such patients. An alarming increase in the number of diabetic patients has become an important area of concern for medical researchers. In this paper, a Cloud IoT based framework for diabetes prediction is proposed and presented. It incorporates sensors in smart wearable devices as set of connected IoT devices for continuous monitoring and collections of blood glucose data which is sent for storage in cloud environment where an ensemble model is used to predict diabetes in patients. Experiment on ten Ensemble models by pairing two out of five different machine learning methods is carried out. The ensemble model of Decision Tree and Neural Network achieved the highest accuracy of 94.5% when evaluated using \u201cPima Indians Diabetes\u201d data set.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the rapid development of natural language processing technologies, more and more text steganographic methods based on automatic text generation technology have appeared in recent years. These models use the powerful self-learning and feature extraction ability of the neural networks to learn the feature expression of massive normal texts. Then, they can automatically generate dense steganographic texts which conform to such statistical distribution based on the learned statistical patterns. In this letter, we observe that the conditional probability distribution of each word in the automatically generated steganographic texts will be distorted after embedded with hidden information. We use recurrent neural networks to extract these feature distribution differences and then classify those features into cover text and stego text categories. Experimental results show that the proposed model can achieve high detection accuracy. Besides, the proposed model can even make use of the subtle differences of the feature distribution of texts to estimate the amount of hidden information embedded in the generated steganographic text.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a novel framework for privacy and security enhancement of power trading in the networked microgrids (MGs) based on the blockchain-enabled Internet of Things (IoT) approach. Utilizing the blockchain-enabled IoT technology in the power trading of the network MGs can potentially lead to some significant advantages such as fewer system risks, mitigate financial fraud, and less the operational cost. A newly stochastic framework based on the unscented transform (UT) is employed to model the uncertainties of renewable energy resources and hourly load demand. Consequently, the proposed framework is tested on the network MG containing residential MG (as a non-crucial load), commercial MG (as an intermediate level load), and hospital MG (as a crucial load), to validate the effectiveness and high performance of the proposed technique.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: These days mobile phones are having advanced onboard sensors embedded in it which makes connecting and computing much easier. This has led to realizing novel application paradigms such as crowd sensing and crowd computing. Crowd sensing relies on the sensing capabilities of the mobile devices as well as its communication efficiency to send collected sensed data to the cloud for further processing. On the other hand, mobile crowd computing is an amalgamation of the machine and human intelligence to achieve a given set of tasks in a distributed manner. Thus, crowd computing utilizes the computation and communication capability of the devices. Here in our work, we have come up with a novel approach integrating these two paradigms in a framework that have addressed both the issues of mobile sensing and crowd computing at the same time and utilized the ability of the crowd to solve problems without involving cloud servers in the backend. We have implemented our framework using 4 smart handheld devices for a route-finding application. The devices are connected to each other through BLE (Bluetooth Low Energy) technology. The results obtained can be received both online (using the machine intelligence) and offline (using human intelligence) when no devices are connected to the internet. The device hence receiving the information, in turn, can itself be a contributor in the crowd for other route-finding queries solicited by another user in the crowd.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Characterization of collagen deposition in immunostained images is relevant to various pathological conditions, particularly in human immunodeficiency virus (HIV) infection. Accurate segmentation of these collagens and extracting representative features of underlying diseases are important steps to achieve quantitative diagnosis. While a first order statistic derived from the segmented collagens can be useful in representing pathological evolutions at different timepoints, it fails to capture morphological changes and spatial arrangements. In this work, we demonstrate a complete pipeline for extracting key histopathology features representing underlying disease progression from histopathology whole-slide images (WSIs) via integration of deep learning and graph theory. A convolutional neural network is trained and utilized for histopathological WSI segmentation. Parallel processing is applied to convert 100K ~ 150K segmented collagen fibrils into a single collective attributed relational graph, and graph theory is applied to extract topological and relational information from the collagenous framework. Results are in good agreement with the expected pathogenicity induced by collagen deposition, highlighting potentials in clinical applications for analyzing various meshwork-structures in whole-slide histology images.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Firewalls as an approved and highly deployed security mechanism have an important role in setting up reliable security policies to ensure the protection of private and critical systems and infrastructures. While a firewall is considered as an essential node in Information Systems (IS) security and represents the backbone of security solutions, its effectiveness is highly dependent on the efficiency of its configuration and the reliability and coherence of its filtering policy. Enhancing the efficiency of access control solutions via improving the quality and the capacity of firewalls attracted several researchers which led to several generations of firewall technologies. In this context, we introduce the novel concept of FW-TR firewall that integrates a trust-risk assessment approach in firewall solutions. Evaluating and involving the trust-risk associated to the filtering rules and policy in a firewall solution helps primary in: (i) strengthening the quality of the firewall filtering service; (ii) discovering firewall misconfigurations; (iii) analyzing firewall rules for anomalies detection; and (iv) changing the firewall behavior facing critical and malicious scenarios. The current paper defines a framework for organizing thinking about incorporating policies and rules trust-risk values in firewall filtering solutions that constitute what we called FW-TR: the new generation of firewalls.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Alzheimer's disease (AD) is the most common cause of dementia and while scientists know that AD involves progressive neuronal cell loss, the reason why this occurs is still not known. As AD exerts a systems-level impact on the brain, therefore the brain structural connectome, derived from whole-brain tractography using diffusion-weighted MRI, has the potential to study the systems-level changes associated with the AD progression. Traditionally, structural connectome is reconstructed based on one single tractography algorithm and commonly involves the comparison of summary graph-theoretical metrics, which could be biased and also discard important informative graph structure. In this paper, we proposed to study the AD effect on brain structural connectome using a multi-view approach. Our results supported multi-view structural connectomics improved power in detecting early changes associated with AD disease progression.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper aims to clarify the accuracy of a method for assessing the power density in close proximity to a wireless communication device operating above 6 GHz for the assessment of compliance with radio-frequency exposure guidelines. We focused on a near-field reconstruction technique that estimates the power density in close proximity to a wireless communication device using the results of electric field measurement at a plane several wavelengths away from the device. In this paper, the reconstruction technique was first validated by comparing the results evaluated using this technique with those obtained by computational simulation for the case of a standard horn antenna. Second, the reconstruction errors of the technique were assessed using ten planar array antennas at frequencies from 15 to 100 GHz. Reconstruction errors no larger than 0.35 dB were obtained for the maximum spatially averaged power density at a separation distance of over 0.15\u03bb from the antennas using an averaging area of \u03bb2 or larger, where \u03bb denotes the wavelength. Finally, the requirement for electric field measurement was also examined, where the combined error for the compliance assessment of the power density was suggested for an actual testing scenario. These results support the standardization of compliance assessment techniques for wireless communication devices operating above 6 GHz, which are expected to be introduced in the near future.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In online apparel market, there is a high return rate mainly due to the size mismatches. As a solution, Virtual garment technologies have been introduced for finding apparels with precise fit whereas most of these technologies are categorized under commercial 3D applications. Therefore, suitability of these 3D applications to check the fitness of selected garment item to a particular user in online marketplace is less due to network latency issues, the need of high user intervention and computational power albeit to the ability of providing highly accurate fitness for a selected garment. Within this paper, a lightweight solution is presented which assists the decision making process of end users in the online market to select perfectly fitted garment by considering human body measurements and garment measurements. The suggested fitting model is consolidated with 2D patterns of human body and finished garments which has the ability of identifying the fitness and manifesting it using seven predefined areas along with user preference and distance ease values. This research has only concerned on generating the fitting model for short sleeve men's shirts due to the limited time frame. The procedure of generating the 2D block pattern from a finished short sleeve shirt has been discerned through an experiment and standard error values which are associated with aforementioned conversion has been recognized using the experiment results. The implemented model was endeavoured on 20 participants and qualitative approaches were used to evaluate the implemented model resulting an accuracy of 81.2%. A survey was conducted to evaluate the visualization of the system output, and received 76% positive responses", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The course \"The STEAM Education of Flying and Assembly of Drone\" is designed for a total of 12 instructional hours. The course includes these modules: Drone Flight System, which has theory of flight and base knowledge; Simulation Flight of Drone; Innovative Applications in the Field of Drone; Assembly and Maintenance of Drone; Practical Flight of Drone; and using the \"Flying Competition to check the effectiveness of learning. These modules establish a teaching paradigm and mechanism as a reference for other STEAM designs. This course integrates imagination, creativity, and innovation into the UAV STEAM, and the purpose is to cultivate students' innovative thinking, practical assembly ability, and active learning attitude. We use Chicago Arts Partnership in Education (CAPE), which is a four-stage cycle of reviewing and revising through the curriculum content and learning effectiveness. This process includes exploration, collaboration, documentation, and reflection to enrich teaching resources and the implementation of innovative knowledge. This way, imagination and innovative thinking can be properly integrated into the UAV STEAM curriculum. This will inspire students' imagination and creativity in learning.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Structured light with spatially variant amplitude/phase/polarization has seen wide interesting applications ranging from manipulation to communications. In this invited talk, we will review recent advances in structured light communications accessing the space domain of lightwaves. Key devices, techniques and emerging applications will be introduced. Future challenges and perspectives of structured light communications will be also discussed.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wireless sensor-actuator networks (WSANs) technology is appealing for use in the industrial Internet of Things (IoT) applications because it does not require wired infrastructure. Battery-powered wireless modules easily and inexpensively retrofit existing sensors and actuators in the industrial facilities without running cabling for communication and power. The IEEE 802.15.4-based WSANs operate at low-power and can be manufactured inexpensively, which makes them ideal where battery lifetime and costs are important. Almost, a decade of realworld deployments of WirelessHART standard has demonstrated the feasibility of using its core techniques including reliable graph routing and time slotted channel hopping (TSCH) to achieve reliable low-power wireless communication in the industrial facilities. Today, we are facing the fourth Industrial Revolution as proclaimed by political statements related to the Industry 4.0 Initiative of the German Government. There exists an emerging demand for deploying a large number of field devices in an industrial facility and connecting them through the WSAN. However, a major limitation of current WSAN standards is their limited scalability due to their centralized routing and scheduling that enhance the predictability and visibility of network operations at the cost of scalability. This paper decentralizes the network management in WirelessHART and presents the first Distributed Graph routing and autonomous Scheduling (DiGS) solution that allows the field devices to compute their own graph routes and transmission schedules. The experimental results from two physical testbeds and a simulation study shows our approaches can significantly improve the network reliability, latency, and energy efficiency under dynamics.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Human footprint is the biometric system of the individual person. Everyone has specific footprints. It can be used instead of password-based authentication in the security system such as a user authentication for the financial transaction. The password-based system cannot verify that the person who entered the password is valid or not. Therefore the biometric system is more secure than the password-based system. For that reason, it's interesting to use footprint image in the creating of the footprint-based identification system. In this paper, the convolutional neural network training is used for deep learning classification. Convolutional neural networks are essential for deep learning and suited for image recognition.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The increasing deployment of datacenters and cloud resources around the globe escalated by higher electricity prices advanced energy cost, cooling and communication cost, and carbon dioxide consumption. To curb such ever-increasing problem complexity, we have formulated a scientific workflow-based cost-effective paradigm based on rigorous mathematical model. Multiple techniques have been considered to increase system utilization rate within acceptable performance bounds. First, we have applied Dynamic Voltage and Frequency Scaling (DVFS) approach to scale down the power consumption by cloud servers via calculating the best near-optimal frequency. Then, we have reused cloud-based VMs to execute as many scientific workflows as possible using fewer cloud servers which conserves a tremendous amount of energy cost including carbon dioxide emission consumption and electricity cost. This has been done through eliminating the overhead of sharing multiple VMs the same server's capacity. Moreover, the aforementioned objectives have been achieved without degrading the Quality of Service (QoS) specified in Service Level Agreement (SLA). However, we have simulated our heuristic using open source CloudSim and compared with algorithms such as the Rank and EARES-D. The results have showed that our paradigm model is better than other heuristics with an average energy reduction of 70%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this letter, we consider the device-to-device (D2D) communication in downlink cellular networks, where the cellular user (CU) both decodes information and harvests the energy. Unlike the conventional D2D communications, we figure out that it is possible for the D2D pair to communicate without degrading the performance of the CU. Based on this observation, D2D transmit power control schemes are proposed for D2D rate maximization subject to maintaining the CU performance. In a decentralized way, the proposed power control schemes are performed at the D2D transmitter (DT) with instantaneous channel state information (CSI) of only the link between the D2D pair. For performance analysis, an asymptotic upper bound is provided on the D2D rates of the proposed schemes. Numerical results confirm the effectiveness of the proposed schemes.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Traffic congestion and occlusions are major problems nowadays in metropolitan cities which leads to an ever growing traffic accidents. Therefore, the need of traffic flux management in order to avoid these congestions, unnecessary time wastage and tragic accidents is very important. Traffic regulation by optimizing timing of traffic control signals is one of the solutions for this purpose. This paper presents a low cost camera based algorithm in order to control traffic flow on a road. The algorithm is based on mainly three steps: vehicle detection, counting and tracking. Background subtraction is used to isolate vehicles from their background, Kalman filter is used to track the vehicles and Hungarian algorithm is exploited for association of labels to the tracked vehicles. This algorithm is implemented on both daytime and night time videos acquiered from CCTV camera and IR camera. Experimental results show the efficacy of the algorithm.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Traffic congestions recognized as a significant problem in modern urban cities. Jordan considered as one of the top countries worldwide in terms of frequency of traffic accidents where the increase in the number of vehicles had led to the rise in the number of traffic incidents, involving the increase in fatalities and injuries, but the fixed timers still control the current traffic signals system in Jordan. In this paper, an Intelligent Road Traffic Management System based on Human Community Genetic Algorithm (IRTMS) proposed. The IRTMS compared with the current traffic system in the Hashmet Kingdom of Jordan, which concludes that it has the minimum total time and waiting time compared with the current traffic lights system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud Computing provides utility-based IT services. The services are available as pay per use. Cloud gives advantage to organizations in setting up fundamental hardware and software requirements i.e. instead of purchasing hardware or software cloud services can be used. The availability of cloud services any time and anywhere makes it a feasible solution for many applications. cloud services are constrained by some parameters such as Quality of Service (QoS), efficient utilization of cloud resources, user budget, user deadlines, energy consumption etc. In this article, we present a comprehensive review of techniques or algorithms designed to reduce energy consumption in cloud data centers. The review covers Evolutionary Algorithms (EA) such as Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO) and Genetic Algorithms (GA). We discuss each technique with strengths and weaknesses. Target objectives of each algorithm are also compared. The article is concluded with future research directions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep Neural Networks (DNNs) have shown superior performance on a variety of artificial intelligence problems. Reducing the resource usage of DNN is critical to adding intelligence on Internet of Things (IoT) devices. Channel pruning based network compression shows effective reduction simultaneously on storage, memory and computation without specialized software on general platforms. But limited by pruning flexibility, channel pruning methods have relatively low compression rate for a given target performance. In this paper, we demonstrate that channel pruning becomes more robust to decision errors by reducing the granularity of filters. Then we propose a Decouple and Stretch (DS) scheme to enhance channel pruning. Under this scheme, each filter in a specific layer is decoupled into two small spatial-wise filters, and the spatial-wise filters are stretched into two successive convolutional layers. Our scheme obtains up to 49% improvement on compression and 35% improvement on acceleration. To further demonstrate hardware compatibility, we deploy pruned networks on the FPGA, and the network produced by Decouple and Stretch scheme is more hardware-friendly with latency reduced by 42%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Increasing complexity in production calls for a skilled workforce. With increasing demands on employee's qualification, also the pressure on companies to train their workforce is constantly rising. Technical knowledge in the context of managing, operating and interacting with new machines and media, so-called \u201cdigital literacy\u201d, is thereby fundamentally desired within fast-paced changing environments. Here, the organizational learning infrastructure is next to personal variables a crucial factor, especially when it comes to knowledge generation. Due to digital transformation of work-places further development of employees receives evermore attention. In contrast to white-collar workers research on production workers is in this context widely underrepresented in current research and insights not often enough utilized. This study assesses current challenges production workers face and evaluates their work-related state of interaction with media. Furthermore, the study conveys insights into the factors which influence their learning motivations. The underlying data is generated through workshops, multi-perspective interviews and a subsequent questionnaire-based survey gathered within the German automotive industry. It is a first contribution by assessing and hence actively shaping a digital learning infrastructure to enable continuous knowledge management in production.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurate classification and the effective recognition of driving styles are critical for improving control performance of the vehicle powertrain. In this research, a set of driving style classification and recognition methods is built based on the feature engineering. First, a specified road test is conducted considering the influence factors, and meanwhile, the corresponding driving data is collected, followed by a detailed evaluation of the driving styles. Then, the information entropy is applied to discretize the driving data, including the speed, acceleration, and opening degree of the accelerator pedal, and 44 feature quantities are extracted to characterize the driving style. By analyzing strong correlation and redundancy among the constructed feature quantities, the principal component analysis (PCA) is employed to reduce the dimension, and the fuzzy c-means (FCM) clustering algorithm is leveraged to classify the driving style. The successful classification rate reaches 92.16%, which is improved by 9.81% in comparison with traditional features. Finally, a parameter identification algorithm based on the support vector machine (SVM) is applied to identify the classified driving style, and the recognition accuracy reaches 92.86%, which is improved by 7.15% in comparison with traditional features, proving the feasibility of the proposed algorithm.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the Internet of Things (IoT) environment, due to the limited computing power of terminal equipment, edge-cloud computing is a good solution for sharing the computation resources. Terminals can offload partial computations to the edge servers for energy saving and reduce the data transmission delays since the edge servers are usually deployed near the terminal devices. However, in the existing solutions, there is still a lot of unnecessary data transmission when the data used in the offloading jobs of several terminals are related. Adding a shareable cache to the edge server and allowing different computing jobs to share the related data can further reduce the data transmission cost and job delay. In this paper, we study the computation offloading method with cached data and propose a novel cache-aware computation offloading strategy for edge-cloud computing in IoT. First, we formulize the cache-aware computation offloading location problem, our goal is to minimize the equivalent weighted response time of all jobs with computing power and cache capacity constraints. Then, we derive the global optimum solution based on transforming the problem to the transportation problem. Next, since the terminal device is difficult to obtain global status information, we propose an online computation offloading strategy, which is convenient in practical deployment. Finally, experiments show that our online offloading strategy approximates the global optimal solution and it reduces the weighted response time by about 26.13% on average compared to other competing algorithms.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the emergence of in-depth research of emerging technologies or 5G mobile communication technology methods, the IoT applications have been further sublimated. In this paper, the new characteristics and new challenges appearing in the current mobile edge computing are sorted out, and the latest related models and work are summarized. The important optimization models and moving models and wireless block data in mobile edge computing are analyzed and discussed. On this basis, this paper mainly designs and verifies the following three aspects of mobile edge computing. A joint optimization model of task offloading and power allocation is established, and a centralized joint optimization algorithm for task unloading and power allocation is proposed. Based on the equalization delay and the impact of energy consumption on task unloading, the algorithm can use the idle resources that can be used to distribute and unload the computing tasks. The simulation experiments show that the algorithm can not only coordinate task offloading and power allocation effectively, but also improve the balance between system delay and energy consumption. Delay-tolerable data can be modeled as a partially observable Markov decision process in a software-defined transport and compute node selection process. Compared with the existing scheme, the proposed method can effectively reduce system overhead, shorten data calculation execution time, improve data calculation efficiency, and ensure that the delay can tolerate data transmission arrival rate under the condition of transmission delay.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The modern Digital Society is characterized by a high level of internet communications penetration in everyday life of any social object. The level of information noise is rising dramatically, and it tosses communication practitioners a challenge they have to cope with (Joseph Turow, Couldry, N.). The problem of communication hygiene is also reviewed in the article. Case studies of Big Data and Smart Data put forth as tools to formulate effective communication strategy in Digital Society communication pattern are represented.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: As a kind of organized criminal organization, pyramid schemes organization has caused great economic losses to many countries. Social network analysis is a effective methods to study organized crime, and the exponential random graph model is a good tool to study the endogenous structure and network process of the organization. To the best of our knowledge, this work is the first time to use the exponential random graph model to study the communication network of a specific pyramid scheme. Results show that the communication network of pyramid schemes is sparse, and the core staff has less contact with the peripheral staff.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents the enhancements obtained in the performance of the Sample Matrix Inversion (SMI) beamformer by the application of Reduced One Dimensional Mapped Real Transform (R-ID-MRT) algorithm. In SMI beamformers, computational complexity and the time required for computation is reduced when data is transformed with R-ID-MRT algorithm. The beam patterns, convergence to the desired beam output, minimization of mean square error in AWGN, Rayleigh and Rician channels of the R-ID-MRT SMI beamformers are comparable to that of conventional SMI beamformers.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Linear Discriminant Analysis (LDA) has been used as a standard post-processing procedure in many state-of-the-art speaker recognition tasks. Through maximizing the inter-speaker difference and minimizing the intra-speaker variation, LDA projects i-vectors to a lower-dimensional and more discriminative subspace. In this paper, we propose a neural network based compensation scheme(termed as deep discriminant analysis, DDA) for i-vector based speaker recognition, which shares the idea with LDA. Optimized against softmax loss and center loss at the same time, the proposed method learns a more compact and discriminative embedding space. Compared with the Gaussian distribution assumption of data and the learned linear projection in LDA, the proposed method doesn't pose any assumptions on data and can learn a non-linear projection function. Experiments are carried out on a short-duration text-independent dataset based on the SRE Corpus, noticeable performance improvement can be observed against the normal LDA or PLDA methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cloud computing and big data have risen to become the most popular technologies of the modern world. Apparently, the reason behind their immense popularity is their wide range of applicability as far as the areas of interest are concerned. Education and research remain one of the most obvious and befitting application areas. This research paper introduces a big data analytics tool, PABED (Project - Analyzing Big Education Data), for the education sector that makes use of cloud-based technologies. This tool is implemented using Google BigQuery and R programming language and allows comparison of undergraduate enrollment data for different academic years. Although, there are many proposed applications of big data in education, there is a lack of tools that can actualize the concept into practice. PABED is an effort in this direction. The implementation and testing details of the project have been described in this paper. This tool validates the use of cloud computing and big data technologies in education and shall head start development of more sophisticated educational intelligence tools.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Coil gun multistage is used to launch a projectile. To increase the velocity of the projectile can be done by adding the number of stages or the number of layers, but it can't control projectile motion. Controlling of the projectile inside the barrel is very important because for regulating the velocity of the projectile when it comes out of the end of the barrel to be as expected. This problem can be solved by a non-uniform multistage coil design. Each coil is twisted separately with the placement between coils coinciding. Multistage coil operation is designed by injecting the current on the coil simultaneously. The injection current setting is difficult because the magnetic force works together so that the coil works to push or slow down the projectile. The genetic algorithm method is used to regulate the current injection to six coils that work together. The method we propose can control projectile motion in the barrel. Uniform multistage coils are used very effectively to regulate projectile motion inside the barrel. The design is shown through multistage coil simulation as a solution to control projectile motion. The Genetic Algorithm Method used in the calculations shows excellent results as expected with a maximum velocity of 31.43 m/s.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we described a multi-chain ToA positioning method to estimate positions using all received Loran-C signals from multiple chains without constraining to a single chain and an ASF estimation method based on a receiver clock offset estimation. To validate these methods, we carried out a test. We firstly installed one receiver at fixed location in land and two receivers on the vessel. And we collected raw ToA measurements data from three receivers while the vessel moved along the path. Next, we estimated the ASF with the data provided by the land fixed receiver and measured the temporal change of ASF to generate the differential correction. The data provided by one of the receivers installed on the vessel was used to generate the ASF map. Loran-C multi-chain positioning was performed using the remaining receiver data. In this process, we mitigated the error by ASF using differential correction and ASF map. As a result, the horizontal positioning error was 2. 12m and 2DRMS was 24. 12m.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, the opportunistic routing (OR) paradigm has been shown as one of the most viable solutions at the network layer for efficient data delivery under the nosily underwater acoustic channel. Since then, several OR protocols for underwater sensor networks, with major and minor variations, were proposed in the literature. While the performance of these protocols, in terms of data delivery rate, delay, and energy consumption, has been extensively studied in the literature, there is a lack of studies devoted for the evaluation of the topological properties OR will incur. In this paper, we evaluate the performance of the candidate set selection procedures of opportunistic routing protocols designed for underwater sensor networks. We discuss the principles that have been extensively considered for the design of OR protocols for underwater sensor networks. Moreover, we conduct a simulation-based performance evaluation to study topological properties (e.g., number of hops, number of paths and number of candidates), which will impact the performance of underwater wireless sensor network applications.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose a novel deep sparse coding network (SCN) capable of efficiently adapting its own regularization parameters for a given application. The network is trained end-to-end with a supervised task-driven learning algorithm via error backpropagation. During training, the network learns both the dictionaries and the regularization parameters of each sparse coding layer so that the reconstructive dictionaries are smoothly transformed into increasingly discriminative representations. In addition, the adaptive regularization also offers the network more flexibility to adjust sparsity levels. Furthermore, we have devised a sparse coding layer utilizing a \u201cskinny\u201d dictionary. Integral to computational efficiency, these skinny dictionaries compress the high-dimensional sparse codes into lower dimensional structures. The adaptivity and discriminability of our 15-layer SCN are demonstrated on six benchmark datasets, namely Cifar-10, Cifar-100, STL-10, SVHN, MNIST, and ImageNet, most of which are considered difficult for sparse coding models. Experimental results show that our architecture overwhelmingly outperforms traditional one-layer sparse coding architectures while using much fewer parameters. Moreover, our multilayer architecture exploits the benefits of depth with sparse coding's characteristic ability to operate on smaller datasets. In such data-constrained scenarios, our technique demonstrates a highly competitive performance compared with the deep neural networks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the modern world, image transmission, including transmission of television and video signals, is carried out using a preliminary and restorative correction according to the law, which was due to the use of imaging devices on cathode ray tubes. This law differs from the law of dependence of the visibility threshold of a visual stimulus on its magnitude, but continues to be used for compatibility reasons. Therefore, in the work, instead of linearizing the sensations of the quantization scale of a television signal, it is proposed to optimize the choice of code combinations for image transmission. This approach can be effective in television systems without compression of video information with losses, for example, for transmitting signals over short distances or in channels for transmitting signals of the image of Earth remote sensing systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent years have witnessed a paradigm shift in the storage of Electronic Health Records (EHRs) on mobile cloud environments, where mobile devices are integrated with cloud computing to facilitate medical data exchanges among patients and healthcare providers. This advanced model enables healthcare services with low operational cost, high flexibility, and EHRs availability. However, this new paradigm also raises concerns about data privacy and network security for e-health systems. How to reliably share EHRs among mobile users while guaranteeing high-security levels in the mobile cloud is a challenging issue. In this paper, we propose a novel EHRs sharing framework that combines blockchain and the decentralized interplanetary file system (IPFS) on a mobile cloud platform. Particularly, we design a trustworthy access control mechanism using smart contracts to achieve secure EHRs sharing among different patients and medical providers. We present a prototype implementation using Ethereum blockchain in a real data sharing scenario on a mobile app with Amazon cloud computing. The empirical results show that our proposal provides an effective solution for reliable data exchanges on mobile clouds while preserving sensitive health information against potential threats. The system evaluation and security analysis also demonstrate the performance improvements in lightweight access control design, minimum network latency with high security and data privacy levels, compared to the existing data sharing models.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: There has been growing interest in harnessing Artificial Intelligence (AI) to improve situational awareness for disaster management. However, to the authors\u2019 best knowledge, few studies have focused on socio-economic recovery. Here, as a first step toward investigating the possibility of developing an AI-based method for detecting socio-economic recovery, this study provides fundamental insights about the correlations between public sentiment on social media and socio-economic recovery activities as reflected in market data. Our result shows multiple correlations between sentiment on social media and the socio-economic recovery activities involved in restarting daily routines. Conventional socio-economic recovery indicators, such as governmental statistical data, have a significant time lag before publishing. Therefore, by taking advantages of the real timeliness and the effectiveness of seizing communication trends of massive social media data, using public sentiment on social media can improve situational awareness in recovery operations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: It is difficult to find research work about data analytics related with information systems operating in the Third Sector. The entities that are part of the Third Sector use private and public funds. Many are managed with a missionary spirit but without any expertise in management or economics. The internal control measures in use are scarce, loose and difficult to implement and have to face obstacles. This case study focuses on a Charities context and on a specific critical area. This work intends to highlight the importance and the ease of the use of Information Technology in Data Extraction and Analytics context in the control of users' current accounts, in a Residential Structure for the Elderly - ERPI. Throughout this paper, some questions related to current accounts will be raised, and some answers will be given, and, in that sequence, a discussion on the proposed tests is done.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mutual coupling, which is caused by a tight inter sensor spacing in uniform linear arrays (ULAs), will, to a certain extent, affect the estimation result for source localisation. To address the problem, sparse arrays such as coprime array and nested array are considered to achieve less mutual coupling and more uniform degrees-of-freedom (DoFs) than ULAs. However, there are holes in coprime arrays leading to a decrease of uniform DoFs and in a nested array, some sensors may still be located so closely that the influence of mutual coupling between sensors remains significant. This paper proposes a new Loosely Distributed Nested Array (LoDiNA), which is designed in a three-level nested configuration and the three layers are linked end-to-end with a longer inter-element separation. It is proved that LoDiNA can generate a higher number of uniform DoFs with greater robustness against mutual coupling interference and simpler configurations, as compared to existing nested arrays. The feasibility of the proposed LoDiNA structure is demonstrated for Direction-of-Arrival (DoA) estimation for multiple stationarysources with noise.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Enterprise bargaining power index is an important basis for solving the problem of alliance profit by using game theory. Study on bargaining ability evaluation is beneficial to logistics alliance development and the optimization and integration of logistics resources. This paper designs the evaluation index system of enterprise bargaining power in logistics alliance from five aspects: alliance enterprise value, learning ability, resources and skills input, skills dependence and termination cost. Using the experts evaluation to provide the index data and the analytic hierarchy process to determine the index weight. Fuzzy comprehensive evaluation is carried out to get the evaluation value of the bargaining power of ten enterprises in a regional logistics alliance. Taking index data and evaluation results of fuzzy evaluation as input and output item, using the BP neural network training fuzzy evaluation system, and then it will provide enterprises with a powerful evaluation tool.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the stock selection problem, the Sharpe ratio is one of the commonly used indicators, but it tends to identify the portfolio with a flat trend as the best one. This paper uses the trend ratio to access the portfolio with a stable upward trend. By the portfolio trend line with initial funds, the trend ratio can simultaneously consider the daily expected return, daily risk and fairly compare with the different portfolios and different investment periods lengths. In addition to the access indicator, this paper provides diversified investments such as time deposit, buying round lots or buying odd lots. Different situation suits different investment method. Therefore, this paper applies the 2-phase investment sliding windows to avoid the overfitting problem and chooses the best investment method by multiple training using Global-best Guided Quantum-inspired Tabu Search with Not Gate (GNQTS) to find the best portfolio effectively and efficiently. Furthermore, the experimental results show that the proposed method can find the well-performing portfolio with higher return and lower risk in both the training and testing periods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Deep learning models have gained great success in many real-world applications. However, most existing networks are typically designed in heuristic manners, thus these approaches lack rigorous mathematical derivations and clear interpretations. Several recent studies try to build deep models by unrolling a particular optimization model that involves task information. Unfortunately, due to the dynamic nature of network parameters, their resultant deep propagations do not possess the nice convergence property as the original optimization scheme does. In this work, we develop a generic paradigm to unroll nonconvex optimization for deep model design. Different from most existing frameworks, which just replace the iterations by network architectures, we prove in theory that the propagation generated by our proximally unrolled deep model can globally converge to the critical-point of the original optimization model. Moreover, even if the task information is only partially available (e.g., no prior regularization), we can still train convergent deep propagations. We also extend these theoretical investigations on the more general multi-block models and thus a lot of real-world applications can be successfully handled by the proposed framework. Finally, we conduct experiments on various low-level vision tasks (i.e., non-blind deconvolution, dehazing, and low-light image enhancement) and demonstrate the superiority of our proposed framework, compared with existing state-of-the-art approaches.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the development of ITS has brought in the necessity of executing various diverse applications by vehicular drivers or mobile users on vehicles. These applications required complex computation and high processing speed which is not available in the on-board units of vehicles. Thus cloud computing is introduced to execute such resource hungry applications. However, the physical distance of cloud from the users brought in undesired service delay. So cloudlets are introduced to reduce the physical distance and improve the service quality. Along with these more third party infrastructure needs to be introduced to meet the ever increasing demand of cloud resources. The onboard units of smart vehicles can collectively be used as third party infrastructure to execute large applications. This concept is referred to as vehicular cloud computing. In this work we propose a three tier vehicular cloud architecture where on road vehicles form the lower layer of cloud, middle layer consists of cloudlet servers associated with road side unit and the top most layer consists of the centralized cloud. Resources are allocated to users based on their task requirement. Particle Swarm Optimization (PSO) is used to allocate optimized resources. Fitness function is modelled for all three cloud layers taking into account various QoS objective. Simulation analysis shows that the proposed approach outperforms some standard algorithms.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: 360 videos provide an omnidirectional view of the scene with extremely large data. Therefore, representing 360 videos with less data has become more and more important. Cube format is such a popular representation of 360 videos. However, we have to convert cube to Equirectangula(ERP) for displaying convenience. In this paper, we enhance Cube-to-ERP conversion performance by joint using Convolutional Neural Network(CNN) and classical interpolation method. The optimal threshold of boundary is derived according to geometry features of the cube-to-ERP format. This threshold is the guidance of how to combine CNN and classical interpolation method. Our experiment results prove that the derived threshold has a certain degree of guiding significance. Furthermore, we propose a new evaluation criterion with the help of Marsaglia model. It is much easier and more accurate to evaluate geometry conversion process.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose a regression model to establish a relationship between surface electromyography (sEMG) and knee joint angles. In this model, the correlation dimension of wavelet coefficient (WCCD) and an Elman network are developed for the model estimation. In our experiment, the sEMG signals were recorded from five muscles concerned with knee joint motion, and knee joint angles were simultaneously recorded by a Codamotion system. First, we used a feature extraction method based on WCCD to extract optimal feature vectors from multichannel sEMG signals. Then, the Elman network was used to map the optimal sEMG features to the knee joint angles. The results show that the features extracted from the multichannel sEMG signals using the WCCD method proposed in this paper outperform the time-domain and frequency-domain methods. Our method is expected to be applied to intelligent prosthetics, exoskeleton robots, and medical rehabilitation robots.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A prototype framework for formal, machine-checked validation of GPU pseudo-assembly code algorithms using the Coq proof assistant is presented and discussed. The framework is the first to afford GPU programmers a reliable means of formally machine-validating high-assurance GPU computations without trusting any specific source-to-assembly compilation toolchain. A formal operational semantics for the PTX pseudo-assembly language is expressed as inductive, dependent Coq types, facilitating development of proofs and proof tactics that refer directly to the compiled PTX object code. Challenges modeling PTX's complex and highly parallelized computation model in Coq, with sufficient clarity and generality to tractably prove useful properties of realistic GPU programs, are discussed. Examples demonstrate how the prototype can already be used to validate some realistic programs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent topics of interest such as smart cities and autonomous driving are currently in focus of many research activities. In this context, simulations are used to evaluate new algorithms, performance of current technologies, or the impact of upcoming products. In particular, they allow finding errors and optimizing parameter sets prospectively, prior to a real-world implementation. Simulation models of many traffic problems need to handle large-scale scenarios, connect entities from different domains, and run in feasible time. In order to meet these challenges, an extendable multi-level traffic simulation approach is proposed in this paper. We briefly introduce existing traffic simulation techniques, name upcoming problems, available solution approaches, and topics regarding the development of our framework. As a first step, we coupled two different resolution levels of traffic simulation by using High Level Architecture (HLA) and evaluated this approach in light of simulation results and simulation performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In the current paper, we present a comparative analysis of the wireless standards IEEE 802.15.4 and IEEE 802.15.6 over a WBAN healthcare monitoring system based on the MAC sub-layer. The main consideration of this work is to look for determined factors to decide about the norm providing the optimal quality of service (QoS) for such system in normal traffic conditions. For this purpose, a comprehensive set of simulations has been conducted using Castalia Simulator to evaluate the average latency, throughput and reliability of the two standards under the same conditions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The optokinetic reflex (OKR) is a behavioral oculomotor response which serves to stabilize moving images on the retina. As the cerebellum is intimately involved in the adaptive control of such compensatory eye movement, the OKR has been turned to a favorable test bed for modeling and assessing cerebellar learning function. Learning process in the cerebellum consists of two distinct phases: 1) short-term learning, which is acquired by single session of behavioral training and disappears within 24 hours, and 2) long-term learning which is induced by repeating sessions and persists for days. On the other hand, emerging evidences from experimental studies indicates high level of coordination between olivary system activity and cerebellar learning. However, it is still unclear which phase of learning will be affected by olivary system disruption. In this paper, we proposed a simple model for learning adaptation and memory formation of the cerebellum. The model is capable to reproduce the OKR gain adaptation of eye movement in both long and short term phases. The simulation results were found to strongly agree with previously reported experimental data from wild type mice. As a second step, we explore the effects of irreversible olivary system lesion on the gain adaptability of OKR by cutting off the connection of climbing fiber, which originates from the inferior olive (IO) neuron. Thereafter, comparing to the normal case, the gain of OKR undergoes a significant decline in both short and long phases of learning. This suggests that the olivary system plays a critical role in both short-and long-term adaption of OKR.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper we address the problem of Community Question Answering (CQA) for Arabic language. We mainly explore the direction of combining both lexical and semantic features for enhancing the retrieval task of possible answers to a posted question. We show a comprehensive evaluation on the SEMEval2017 CQA dataset for Arabic language. Our Mean Average Precision (MAP) achieves 62.85% when using a supervised machine learning approach (linear SVM). We outperformed best reported results on such dataset. This is achieved by defining a mix of word embedding, latent semantic similarity features and other lexical similarity features.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The driver states, driving styles and aggressiveness strongly influences vehicle control, and energy efficiency. If the driving patterns can be collected and effectively analyzed the resulting classification can greatly improve the effectiveness and design of active safety system, advanced driving assistance system (ADAS) or energy efficient control. For an efficiency oriented analysis, artificial neural network (ANN) is used to classify drivers into aggressive, normal, and calm states through three different driving inputs: vehicle acceleration, speed and throttle pedal angle. The resultant models have fairly accurate classification according to different driving scenarios, with overall accuracy of 90%. The classification can be a reminder for the drivers of their current behavior, in-order for the drivers to take necessary actions to improve the driving condition.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Binary Neural Networks (BNNs) have obtained great attention since they reduce memory usage and power consumption as well as achieve a satisfying recognition accuracy on Image Classification. In particular to the computation of BNNs, the multiply-accumulate operations of convolution-layer are replaced with the bit-wise operations (XNOR and pop-count). Such bit-wise operations are well suited for the hardware accelerator such as in-memory computing (IMC). However, an additional digital processing unit (DPU) is required for the pop-count operation, which induces considerable data movement between the Process Engines (PEs) and data buffers reducing the efficiency of the IMC. In this paper, we present a BNN computing accelerator, namely CORN, which consists of a Spin-Orbit-Torque Magnetic RAM (SOT-MRAM) based data buffer to perform the majority operation (to replace the pop-count process) with the SOT-MRAM-based IMC to accelerate the computing of BNNs. CORN can naturally implement the XNOR operation in the NVM memory array, and feed results to the computing data buffer for the majority write operation. Such a design removes the pop-counter implemented by the DPU and reduces data movement between the data buffer and the memory array. Based on the evaluation results, CORN achieves 61% and 14% power saving with 1.74\u00d7 and 2.12\u00d7 speedup, compared to the FPGA and DPU based IMC architecture, respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To realize an efficient DTN (Delay/Disruption-Tolerant Networking) routing, it is required to quickly deliver the message from the source node to the destination node as well as to quickly delete disused message replicas from the network. Epidemic routing, which indefinitely forwards message replicas to all encountered nodes, realizes the near-optimal message delivery delay when a limited number of messages are transferred. However, its performance is significantly degraded when a number of messages are transferred simultaneously. In our previous work, we have proposed a simple but effective extension to epidemic routing called restrained epidemic routing, which intentionally suppresses message forwardings at the later stage of epidemic-style message dissemination. In this paper, we analyze the characteristics of restrained epidemic routing when the contact relation between nodes is given by a general contact model such as complex networks. Specifically, we describe the dynamics of restrained epidemic routing on a complex network with a given degree distribution as differential equations using the degree-based mean field approximation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We introduce a new cavity-based wireless power transmission (WPT) system that could be applied to any cavity-based equipment regardless of its shape and size. The proposed scheme provides uniform and selective powering modes. In either powering mode, the field is isotropic, which removes the WPT-restrictions on placement and orientation of the energy harvester. The design process has three main steps, randomness creation, frequency selection, and waveform generation. We validate the proposed scheme in a lab lyophiliser's (freeze-drier) chamber. First, we create a random electromagnetic environment using mechanical stirring. Then, we evaluate this randomness in terms of the average to minimum power ratio. To select an appropriate frequency for the WPT system, we consider randomness and power uniformity. To maintain randomness, we extract the lowest usable frequency of the chamber using a Goodness of Fit test; this is found to be 6 GHz. As for the power uniformity, we plot the standard deviation (STD) of a large sample of the received powers at different locations. This plot is used to select the frequency based on an arbitrary uniformity level in terms of STD. At 6 GHz a 2.5 dB standard deviation is calculated. To enable the selective powering mode, we propose the electromagnetic time reversal (EMTR) technique. We show that EMTR can, theoretically, focus 97% of the energy on 0.5\u03bb-diameter area in an ideally random environment.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Mild cognitive impairment (MCI) is an early stage of Alzheimer's disease (AD), which is a neurodegenerative disease. Functional connectivity networks (FCN) provide an effective method for analyzing brain functional regions connectivity. However, most methods only considered the neuroimaging information and focused on group relationship without the subjects' individual features, and ignored the demographic relationship. To handle it, in this paper, we introduce a novel method based on graph convolutional networks (GCN), which combines image and other information for MCI prediction tasks. The proposed model is capable of representing the individual features and data associations among subjects from potentially populations simultaneously. Specifically, we use different collection devices and gender information to build a graph called MCI-graph and modify convolutional neural networks (CNN) to construct GCN for MCI prediction. The experimental results demonstrate that our proposed method has achieved remarkable prediction performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The economics of high performance computing are rapidly changing. Commercial cloud offerings, private research clouds, and pressure on the budgets of institutions of higher education and federally-funded research organizations are all contributing factors. As such, it has become a necessity that all expenses and investments be analyzed and considered carefully. In this paper we will analyze the return on investment (ROI) for three different kinds of cyberinfrastructure resources: the eXtreme Science and Engineering Discovery Environment (XSEDE); the NSF-funded Jetstream cloud system; and the Indiana University (IU) Big Red II supercomputer, funded exclusively by IU for use of the IU community and collaborators. We determined the ROI for these three resources by assigning financial values to services by either comparison with commercially available services, or by surveys of value of these resources to their users. In all three cases, the ROI for these very different types of cyberinfrastructure resources was well greater than 1 - meaning that investors are getting more than $1 in returned value for every $1 invested. While there are many ways to measure the value and impact of investment in cyberinfrastructure resources, we are able to quantify the short-term ROI and show that it is a net positive for campuses and the federal government respectively.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose an algorithm for constructing efficient security strategies in the mobile edge computing (MEC), where the protected targets are nodes connected to the MEC and the mobile users (MUs) are agents capable of preventing undesirable activities on the nodes. The algorithm is designed based on the synthetic principles of a specific set of strategies, and it can quickly construct suboptimal solutions even if the number of targets reaches hundreds of millions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Periodic fractographic analysis of fracture surfaces helps improve performance of mechanical pieces and avoids economical and security problems in many industries, such as the automotive industry. Classifying a fracture into a failure mode is necessary to determine the causes that generated the fracture in the first place. Experts in fracture classification of metallic materials usually use texture and surface marks to determine the type of fracture. Deep Learning is a machine learning technique that learns features directly from input data and has achieved outstanding results in object classification. However, when it comes to texture classification, the results of deep learning are not as good as in other classification tasks. This paper proposes to improve the performance of deep learning for texture analysis in the context of fractographic classification by extracting handcrafted features (Haralick, fractal dimension and local binary patterns) from the output of the convolutional layers of the VGG-19 model. Four datasets are used in this paper. Two common textural databases, KTH-TIPS and KTH-TIPS2-B, are used as benchmark for the texture recognition problem, and two datasets of fractures are used for evaluation of the proposed methods. One of the datasets for evaluation corresponds to real-scale images of ductile, brittle and fatigue fractures; and the other dataset corresponds to images acquired with a Scanning Electron Microscopy (SEM) of ductile, brittle, fatigue and corrosion fatigue fractures. The best performance for the KTH-TIPS and KTH-TIPS2-B datasets was attained with local binary patterns (LBP) extracted from the first feature map of the third convolutional layer with an F1-score of 1.0 and from the second feature map of the fifth convolutional layer group with an F1-score of 0.77, respectively. In the case of the fracture database of real-scale images, the best performance was attained with fractal dimension features extracted from the feature maps of the first convolutional layer with an F1-score of 0.72, and with LBP extracted from the first feature maps of the fourth convolutional layer for the SEM images.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the advent of the Internet of Things (IoT), scalability becomes a significant concern due to the huge amounts of data generated in IoT systems. A centralized data exchange is not desirable as it leads to a single performance bottleneck. Although a distributed data exchange removes the central bottleneck, it has network performance issues as data passes among multiple coordinators. A decentralized approach is the only solution that fully enables the realization of efficient IoT systems, since there is no single performance bottleneck and network overhead is minimized. In this paper, we present an approach that leverages the semantics of DX-MAN for realizing decentralized data flows in IoT systems. The algebraic semantics of such a model allows a well-defined structure of data flows which is easily analyzed by an algorithm that forms a direct relationship between data consumers and data producers. For the analysis, the algorithm takes advantage of the fact that DX-MAN separates control flow and data flow. Thus, our approach prevents passing data alongside control among multiple coordinators, so data is only read and written on a decentralized data space. We validate our approach using smart contracts on the Blockchain, and conducted experiments to quantitatively evaluate scalability. The results show that our approach scales well with the size of IoT systems.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Feature maps in Convolutional neural networks are extracted automatically with some initialization methods and training strategies, which greatly economizes the cost of feature engineering. However, correlation between feature maps are not considered in common networks, resulting in the increase of redundant feature maps with the networks becoming more complicated. In this work, we proposed the correlation layer and designed the correlation loss, which can compute the correlation coefficient matrix of the feature maps in the last convolutional layer and optimize the weights distribution respectively. In the training phase, 2 strategies, namely the supervision and initialization are studied with Gaussian and He initialization methods for the baseline. The experimental results on CIFAR-10 dataset demonstrated that the supervision strategy for the multi-task training could efficiently reduce the correlation between the feature maps learned and increase the classification accuracy from 0.39% to 1.14% on the test set.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: For decades it has been acknowledged that sharing security information and collaboration between security practitioners are a necessity. Yet, effective sharing and collaboration are rare. A gamut of legislative acts, executive orders, academic works, and private sector initiatives have discussed aspects of the problem and aimed to be the catalyst needed to fix the situation. But almost 30 years since these efforts started, the state of sharing and collaboration is still technically complicated, slow, untrusted, and impeded by bureaucratic woes. This work identifies the challenges of sharing security artifacts and uses real-world examples to illustrate our findings. Based on this knowledge, we propose a new model for sharing and collaboration, CARE. The CARE architecture eases many of the privacy, secrecy, lineage, and structure issues that plague current sharing communities and platforms. We then build upon this foundation to introduce a marketplace based on smart contracts with transactional privacy over a distributed blockchain. Therefore, CARE incentivizes sharing, combats free riding, and provides an immutable ledger for the attribution of events. This paradigm shift, overcomes the challenges of sharing while providing new opportunities for business models, insurance risk assessments, and government backed incentivisation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, a large number of researchers investigate the conception of systems that use a unique Unmanned Ariel Vehicles (UAV) or multiple independent UAVs to conduct civil or military missions, with minimal human intervention. In this paper we focus on using multiple UAVs to cooperatively monitor a crowded area. Communication in such UAVs network is an ongoing project. Due to the lack of proper communication standards and rules, designing a reliable communication model is essential for: (i) multi-UAV coordination, (ii) efficient bandwidth sharing according to data priority and urgency and (iii) avoiding useless transmission of the same data by multiple UAVs. To address the above challenges, we propose a centralized data-oriented communication architecture for crowd surveillance allocations using an UAV fleet. The Ground Control Station (GCS) is used as a central coordinator to manage bandwidth usage for the UAV fleet in its coverage area. To allow UAVs to send priority messages urgently to the GCS, we define two classes of urgent messages: critical state and important result. The class of the data as well as other relevant information about the detected event will be used by the GCS to authorize or not UAV data transmission and hence to optimize the bandwidth usage efficiency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper addresses a new method of fault diagnosis for parallel shaft gearbox. Aiming at the nonlinear and non-stationary characteristics of gearbox vibration signals, the Multifractal Detrended Fluctuation Analysis (MFDFA) is introduced to calculate the multifractal spectrum parameters as fault features, and combined with the improved K means clustering to detect the failure of gearbox. The above methods are verified through using the fault data of gearbox preposition failure experiment, and the result shows that the methods have good effect.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we study the problem of integrated berth and quay crane allocation (I-BCAP) in general seaport container terminals and propose the model predictive allocation (MPA) algorithm and preconditioning methods for solving the I-BCAP. First, we propose a dynamical modeling framework based on discrete-event systems (DESs), which describes the operation of a berthing process with multiple discrete berthing positions and multiple quay cranes. Second, based on the discrete-event model, we propose the MPA algorithm for solving the I-BCAP using the model predictive control (MPC) principle with a rolling event horizon. The validation and performance evaluation of the proposed modeling framework and allocation method are done using: 1) extensive Monte Carlo simulations with realistically generated datasets; 2) real dataset from a container terminal in Tanjung Priuk port, located in Jakarta, Indonesia; and 3) real life field experiment at the aforementioned container terminal. The numerical simulation results show that our proposed MPA algorithm can improve the efficiency of the process where the total handling and waiting cost is reduced by approximately 6%-9% in comparison with the commonly adapted method of first-come first-served (FCFS) (for the berthing process) combined with the density-based quay cranes allocation (DBQA) strategy. Moreover, the proposed method outperforms the state-of-the-art hybrid particle swarm optimization (HPSO)-based and genetic algorithm (GA)-based method proposed in the recent literature. The real life field experiment shows an improvement of about 6% in comparison with the existing allocation method used in the terminal.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a novel method that addresses the selection of the dominant patterns of the histograms of oriented gradients (DPHOGs) in vehicle detection. HOG features lead to an expensive classification with high misclassification rates since HOG generates a long vector containing both redundant and ambiguous features (similarities between the vehicle and non-vehicle images). Several modifications of HOG were proposed to resolve these issues such as the vertical histograms of oriented gradient and one that includes position and intensity with HOG; however, these methods still contain some ambiguous features. A feature selection method can exclude these ambiguous features, allowing for better classification rates and a reduction in classification times. The proposed method uses the ideal vectors of the vehicle and non-vehicles images for selecting features in dominant patterns. The segments indicating the differences between the vehicle and non-vehicle classes are the dominant patterns, in which the length of the feature vector is shortened. We performed DPHOG on three standard datasets, in which the kernel extreme learning machine, the support vector machine, K-nearest neighbor, random forest, and deep neural network were used as classifiers. We then compared the performance of the DPHOG with eight well-known feature selection methods and three existing feature extraction methods for vehicle detection. In evaluations with each comparative method concerning the accuracy, true positive, false positive, and F1-score, the DPHOG presented the highest performances with less running time in each dataset.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, a new behavioural mixer model is introduced. In addition to amplitude nonlinearity; phase nonlinearity and memory effect of the mixer are modelled in a multi-box structure. Each nonlinearity source is modelled in distinct blocks of which cascade connections compose the behavioural mixer model. A test-bed has been constructed including a reference mixer and through measurements, amplitude and phase characteristics of the main tones and IMD products are modelled. The proposed behavioural model delivers a good match between measured and simulation results. The mixer model may be curve fitted to the characteristics of any mixer by optimizing its variable parameters and thus can be used in modelling different types of mixers. Thus, the proposed mixer model may be used in computer simulation tools and in linearization applications.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Video-based human action recognition is currently one of the most active research areas in computer vision. Various research studies indicate that the performance of action recognition is highly dependent on the type of features being extracted and how the actions are represented. Since the release of the Kinect camera, a large number of Kinect-based human action recognition techniques have been proposed in the literature. However, there still does not exist a thorough comparison of these Kinect-based techniques under the grouping of feature types, such as handcrafted versus deep learning features and depth-based versus skeleton-based features. In this paper, we analyze and compare 10 recent Kinect-based algorithms for both cross-subject action recognition and cross-view action recognition using six benchmark datasets. In addition, we have implemented and improved some of these techniques and included their variants in the comparison. Our experiments show that the majority of methods perform better on cross-subject action recognition than cross-view action recognition, that the skeleton-based features are more robust for cross-view recognition than the depth-based features, and that the deep learning features are suitable for large datasets.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper investigates the applicability of deep and machine learning techniques to perform beam selection in the uplink of a mmWave communication system. Specifically, we consider a hybrid beamforming setup comprising an analog beamforming (ABF) network followed by a zero-forcing baseband processing block. The goal is to select the optimal configuration for the ABF network bsed on the estimated angles-of-arrival (AoAs) and received powers. To that aim, we consider three machine/deep learning schemes: k-nearest neighbors (kNN), support vector classifiers (SVC), and the multilayer perceptron (MLP). We conduct an extensive performance evaluation to assess the impact of using the Capon or MUSIC methods to estimate the AoAs and powers, the size of the training dataset, the number of beamformers in the codebook, their beamwidth, or the number of active users. Computer simulations reveal that performance, in terms of classification accuracy and sum-rate, is very close to that achievable via exhaustive search.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes novel reflectarray resonant elements designed by genetic algorithm(GA), which have arbitrarily phase difference from -180\u00b0 to 180\u00b0 between the TE (V-pol.) and the TM (H-pol.) polarizations from 13 GHz to 17 GHz. Moreover, these elements have two axially symmetric structure in the unit cell so that the cross polarization level is suppressed less than about -60 dB for the wide frequency range. A reflectarray antenna with two-separated offset feeds is designed by using proposed elements in the Ku-band, and its effectiveness is verified by evaluating radiation patterns numerically.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Scattering properties of multiple-layer anisotropic metasurfaces are characterized based on generalized boundary conditions. Once surface susceptibilities of all layers are determined, reflection and transmission coefficients could be readily derived for arbitrary incident angles and polarizations. The validity of this analytical method is demonstrated through comparison between computed results and full-wave simulations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: WiFi is increasingly used by carriers for opportunistically offloading the cellular network infrastructure or even for increasing their revenue through WiFi-only plans and WiFi on-demand passes. Despite the importance and momentum of this technology, the current deployment of WiFi access points (APs) by the carriers follows mostly a heuristic approach. In addition, the prevalent free-of-charge WiFi access policy may result in significant opportunity costs for the carriers, as this traffic could yield non-negligible revenue. In this paper, we study the problem of optimizing the deployment of WiFi APs and pricing the WiFi data usage with the goal of maximizing carrier profit. Addressing this problem is a prerequisite for the efficient integration of WiFi to next-generation carrier networks. Our framework considers various demand models that predict how traffic will change in response to alteration in price and AP locations. We present both optimal and approximate solutions and reveal how key parameters shape the carrier profit. Evaluations on a dataset of WiFi access patterns indicate that WiFi can indeed help carriers reduce their costs while charging users about 50% lower than the cellular service.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Video games like movies and music are considered as a popular entertainment. Because of their intractability, people get the chance to alleviate daily stress, find satisfaction and be the heroic version of themselves. Although 3D games dominate the video game market, 2D games find a specified place for their straightforward and simple gameplay. This paper develops a 2D platformer action game that player should jump on platforms and whether shoot enemies or run from them to end a level. The three unique enemies in the game which have different skills like shooting projectiles, flying, and pursuing the player, use reinforcement learning methods to improve and enhance their decision-making ability according to their previous decisions and results to decide more wisely when facing the player. The novelty of this game is that enemies would be able to change their behaviors and actions according to their current state and the thresholds will be modified at the end of each level based on the player's actions in previous level to improve the effectiveness of the game. Since every player has a specific style of play, the enemies will take different appropriate action towards the different players. The game is developed for Android devices by Unity game engine and C# programming language.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In a parallel application which is divided into several tasks, choosing the right resource and sending tasks to the resources is an important challenge. Selecting the resources for each task in a way that the execution time of the application and usage of resources be appropriate is known as mapping tasks to the resources. In this paper, mapping and scheduling parallel applications, which have communications between their tasks, is investigated in the cloud environment. Our proposed method tries to reduce the execution time and also resource usage time by utilizing the fuzzy logic and reducing the communication overhead. In the end, the proposed method is compared with Max-Min-C and Min-Min-C algorithms, and results show a significant improvement.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Wind is one of the most essential sources of clean, renewable energy, and is, therefore, a critical element in responsible power consumption and production. The accurate prediction of wind speed plays a key role in decision-making and management in wind power generation. This study proposes a model using a deep belief network with genetic algorithms (DBNGA) for wind speed forecasting. The genetic algorithms are used to determine parameters for deep belief networks. Wind speed and weather-related data are collected from Taiwan's central weather bureau for this purpose. This paper uses both time series data and multivariate regression data to forecast wind speed. The seasonal autoregressive integrated moving average (SARIMA) method and the least squares support vector regression for time series with genetic algorithms (LSSVRTSGA) are used to forecast wind speed in a time series, and the least squares support vector regression with genetic algorithms (LSSVRGA) and DBNGA models are used to predict wind speed in a multivariate format. The empirical results show that forecasting wind speed by the DBNGA models outperforms the other forecasting models in terms of forecasting accuracy. Thus, the DBNGA model is a feasible and effective approach for wind speed forecasting.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A speed estimation method using the acceleration data measured by a wearable device during exercising on a treadmill is described in this paper. The moving speed is estimated by a regression algorithm that implements a deep convolutional neural network (CNN) model. This model is trained on a set of test speeds for walking and running conditions. The mean square error between the inferred speeds and test speeds is minimized during the training to optimize the internal model parameters. The speed inference mean error is shown to be accurate within 7% and 18% of the actual running and walking test speeds, respectively. The deep CNN model parameters, e.g., data sample size, convolution kernel size, and fully connected layer sizes, are optimized for inference accuracy and sized to enable a compact hardware design. The feasibility of designing a wearable device that can infer speed from the acceleration measurement for wearable applications is demonstrated by device simulation.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: An imbalanced dataset is a dataset that has a majority class which is a class has far more example distributions than other classes. It is difficult to deal with unbalanced datasets in classification problems, and many classification algorithms do not perform well in unbalanced datasets. In this paper, we present our logistic regression analysis with Python on imbalanced datasets and determine different thresholds for classification according to the data proportion of imbalanced datasets.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Cataract is defined as a lenticular opacity presenting usually with poor visual acuity. It is considered the most common cause of blindness. Early diagnosis and treatment can reduce the suffering of patients and prevent visual impairment from turning into blindness. Recently, cataract diagnosis applying pattern recognition is in a rising period. For retinal fundus images, the task is usually cataract classification. However, it needs complex manual processing, which demands dexterous people and time taking exertion. Besides, it faces the challenge of effective interpretability and dependability. In this paper, we develop a deep-learning algorithm to intuitively identify cataract attributes to solve these limitations. Our model, is a 18(50)-layer convolutional neural network that inputs retinal image in G channel and outputs the prediction with heatmap. The heatmap localizes the areas where most indicative of different levels of cataract. Furthermore, we extend the training strategy for the corresponding task, which aims at improving the performance of the network. Comparing with other methods in cataract classification, we succeeded to achieve state of the art accuracy of proposed method on detection and grading task. Most importantly, our model provides a compelling reason via localizing the areas revealing cataract in the image.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Today, it is a common practice to establish technology transfer centers that are technically capable of transferring technology that are funded by European or national sources. The key players in such centers are experts who connect innovators with potential users of their innovations. Since technology and knowledge transfer to small and medium-sized enterprises are considered a turning point in building an innovative and strong European economy, it is essential to ensure the preconditions for a permanent transfer of technology to such companies. Particularly important is the education of experts involved in such processes. This paper describes the results of the Erasmus KA2 Project T4 (Transnational Technology Transfer Training: Training Blueprints for Accelerated Growth), which subsequently formulated a set of competencies and the corresponding curriculum for Transnational Technology Transfer Manager (TTTM) in a small and medium-sized enterprises (SMEs).", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper the performance of the Genetic Algorithm (GA), the Particle Swarm Optimization (PSO) algorithm, the Modified Particle Swarm Optimization (MPSO) algorithm, and the L\u00e9vy Flight Firefly Algorithm (LFFA) are compared for system identification with various types of nonlinear systems. When performing system identification with Volterra nonlinear adaptive structures, matched-order fixed-nonlinearity LNL filter structures, reduced-order adaptable-nonlinearity LNL filter structures and neural networks the LFFA generally demonstrates faster convergence rates and lower minimum mean square errors (MMSE) compared to the GA, PSO, and MPSO algorithms. This work includes performance comparisons of these algorithms when applied to nonlinear system identification.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: By considering the different cumulant combinations of the 2FSK, 4FSK, 2PSK, 4PSK, 2ASK, and 4ASK, this paper established new identification parameters to achieve the recognition of those digital modulations. The deep neural network (DNN) was also employed to improve the recognition rate, which was designed to classify the signal based on the distinct feature of each signal type that was extracted with high order cumulants. The extensive simulations demonstrated the exceptional classification performance for new key features based on high order cumulants. The overall success rate of the proposed algorithm was over 99% at the signal to noise ratio (SNR) of \u22125 dB and 100% at the SNR of \u22122 dB. The results of the experiments also showed the robustness of the proposed method for a variety of conditions, such as frequency offset, multi-path, and so on.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The technological development of the energy sector also produced complex data. In this study, the relationship between smart grid and big data approaches have been investigated. After analyzing which areas of the smart grid system use big data technologies and technologies, big data technologies for detecting smart grid attacks have received attention. Big data analytics can produce efficient solutions and it is especially important to choose which algorithms and metrics to use. For this reason, an application prototype has been proposed that uses a big data method to detect attacks on the smart grid. The algorithm with high accuracy was determined to be 92% for random forests and 87% for decision trees.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a hybrid probabilistic interval prediction method for short-term load forecasting. The method combines K-means clustering based feature selection approaches and online Gaussian processes regression(OGPR) to generate better prediction results. The K-means clustering algorithm based feature selection are used to select the most relevant features during a dynamical process to better capture the load characters along with time. OGRP, includes dynamically updating the hyper-parameters and training sample sets as two key features, is served as a forecasting engine to carry out load probability interval prediction. The load data from Queensland market, Australia is used to validate the model proposed. The comparative results show that the proposed approach can obtain higher quality prediction interval.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the development of the web's high usage, the number of malware affecting the system are incresing. Various techniques have been used but they are incapable to identify unknown malware. To counter such threats, the proposed work makes utilization of dynamic malware investigation systems based on machine learning technique for windows based malware recognization. In this paper two methods to analyses the behaviour of the malware and feature selection of windows executables file. Cuckoo is a malicious code analysis apparatus which analyzes the malware more detail and gives the far-reaching results dependent on the arrangement of tests made by it and second, the feature selection for windows dynamic malware anaysis has been done by using Genetic Algorithm. Three classifiers have been used to compare the detection result of Windows-based malware: Support Vector Machine with detection accuracy of 81.3%, Naive Bayes classifier with accuracy of 64.7% and Random Forest classifier achieving 86.8% accurate results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We propose a solution for Electric Vehicles (EVs) energy management in smart cities, where a deep learning approach is used to enhance the energy consumption of electric vehicles by trajectory and delay predictions. Two Recurrent Neural Networks are adapted and trained on 60 days of urban traffic. The trained networks show precise prediction of trajectory and delay, even for long prediction intervals. An algorithm is designed and applied on well known energy models for traction and air conditioning. We show how it can prevent from a battery exhaustion. Experimental results combining both RNN and energy models demonstrate the efficiency of the proposed solution in terms of route trajectory and delay prediction, enhancing the energy management.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A multivariate time series forecasting is critical in many applications, such as signal processing, finance, air quality forecasting, and pattern recognition. In particular, determining the most relevant variables and proper lag length from multivariate time series is challenging. This paper proposes an end-to-end recurrent neural network framework equipped with an adaptive input selection mechanism to improve the prediction performance for multivariate time series forecasting. The proposed model, named AIS-RNN, consists of two main components: the first neural network learns to generate context-dependent importance weights to dynamically select the input. The selected input is then fed into the second module for predicting the target variable. The experimental results show that our proposed end-to-end approach outperforms machine learning-based baselines on several public benchmark datasets. The AIS-LSTM model achieves higher performance on a public M3 dataset than the M3-specialized models. Furthermore, the AIS-RNN gives a beneficial advantage to interpret variable importance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Diseases such as stroke and proprioception loss may cause inability to control the arm in the activities of daily living. Studies show that intensive motor training with virtual reality games can help patients to restore arm functions. This paper proposes a pseudo-haptic feedback method to add motion assistance or resistance to virtual reality-mediated upper-limb rehabilitation. The alterations to the cursor speed produced by the proposed technique create the illusion of virtual forces providing motion assistance or resistance to the arm movement during rehabilitation training. The proposed method is experimentally evaluated involving human subjects to demonstrate the feasibility. The experimental results reveal that the motion assistance mode is more time-efficient and is claimed to be easier than the motion resistance mode by human subjects. The difficulty of the path following task of virtual reality-mediated upper-limb rehabilitation training can be modulated by the proposed pseudo-haptic feedback method without using any haptic feedback devices.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a tracking control scheme with quantization mechanism for hypersonic flight vehicles (HFVs) with prescribed performance using an interval type-2 fuzzy neural network (IT2FNN). A parameterized tracking error model of the HFV is derived with some considered uncertainties, which are approximated by an IT2FNN. The tracking control of the velocity and altitude of the HFV is designed by using a prescribed performance control technique. It allows that transient characteristics of the tracking errors can be improved and adjusted by some prescribed performance functions. According to an adaptive backstepping control design procedure, novel continuous control laws of the fuel equivalency ratio, canard deflection, and elevator deflection are designed with logarithmic quantization mechanism, for the sake of avoiding inadvertently increasing the effective gains of continuous controllers as well as reducing loads of the communication from controller unit to actuator unit. Besides, the limited tracking errors of the flight path angle and angle-of-attack can be achieved by applying the designed controllers. Finally, the presented tracking controllers with quantization mechanism are validated by comparative simulations.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Surface exploration by the distributed robots is a countermeasure way to the solo exploration by using a large and expensive rover. The distributed rover can explore a large area at the same time, and it is expected to improve the exploration speed. However, the mass resource of the distributed small rover is limited. A small and lightweight position and attitude estimation system is also required for rover guidance and control. In this paper, we propose a compact, lightweight attitude and position estimation system for distributed rover using light from the leader agent. A small photodiodes are used as a sensor of a position estimation system using light for the small exploration rover. Photodiodes mounted on the follower rover in different directions detect the light from the leader to estimate the vector in the light source direction with respect to the rover body coordinate system. The attitude is decided by using the estimated light source direction vector and the gravitational vector by the acceleration sensor in the rover. After determining the attitude, the distance between the leader rover and the follower rover is estimated using the elevation angle of the light source and the light source height information. If there is communication between the leader and followers, more complicated mutual position recognition is possible, but here we discuss the limits that can be achieved with minimum hardware. We also describe the light source recognition from the leader rover under ambient light, and also discuss the overall scheme of the exploration system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper proposes a new multi-objective technique to solve the problem of optimal location and sizing of Distributed Generation Photovoltaic (DGPV) in the power system transmission network. The technique: Multi-objective Chaotic Mutation Immune Evolutionary Programming (MOCMIEP) was developed based on Pareto optimality to solve the DGPV location and sizing problem. The proposed technique determines the optimal location and sizing of DGPV, therefore will minimize the multiple objective functions, namely the active power losses and Fast Voltage Stability Index (FVSI) simultaneously. The method was tested on IEEE 118-Bus Reliability Test System (RTS). The results revealed that the proposed technique had the ability to acquire a set of Pareto solutions for the decision maker to choose depending on the system priorities.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Effective storage, processing and analyzing of power device condition monitoring data faces enormous challenges. A framework is proposed that can support both MapReduce and Graph for massive monitoring data analysis at the same time based on Aliyun DTplus platform. First, power device condition monitoring data storage based on MaxCompute table and parallel permutation entropy feature extraction based on MaxCompute MapReduce are designed and implemented on DTplus platform. Then, Graph based k-means algorithm is implemented and used for massive condition monitoring data clustering analysis. Finally, performance tests are performed to compare the execution time between serial program and parallel program. Performance is analyzed from CPU cores consumption, memory utilization and parallel granularity. Experimental results show that the designed framework and parallel algorithms can efficiently process massive power device condition monitoring data.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A robust scheme for the attitude stabilization and trajectory tracking in presence of parameter uncertainties and perturbations of the 3 DOF Hover is presented. The proposed control is based on an adaptive Super Twisting algorithm (ASTW) and a continuous differentiator. The continuous differentiator can estimate the unmeasurable states and fixe the problem of chattering in the output signals. The proposed control is validated by experimental tests under external disturbances and parameter uncertainties using the Quanser Hover platform. Experimental results show the smoothness of the continuous differentiator's signals even with noised measurements induced by sensors. Morever, in spite of uncertainties and disturbances, the robustness, accuracy and finite-time convergence are shown by the experimental tests.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The remote sensing images are captured by high-altitude satellites, and the coverage of each image scene is relatively large. It tends to have large intraclass differences and small differences between classes. Using conventional Hierarchical ELM (H-ELM) and multilayer kernel ELM (ML-KELM) to classify remote sensing image scenes, the network structure of the learning model is deep and the parameters are many. This leads to long training time and large memory consumption. In order to solve this problem, based on the ML-KELM, this paper proposes a densely connected kernel ELM (Dense-KELM) learning model, which is used to classify remote sensing image scenes. Experimental results show that at the same model depth, the Dense-KELM model has higher classification accuracy in remote sensing image scenes than the H-ELM and the ML-KELM. Its training time is slightly larger than the ML-KELM but much smaller than the H-ELM. This densely connected learning model can extract high-level features of remote sensing images more effectively, represent the details between remote sensing scenes, and improve the classification accuracy of remote sensing image scenes. Moreover, the densely connected network structure can effectively reduce the number of parameters of the depth model, improve the training speed of the model, and save the storage space of the model.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The reservation of shape features is the key problem of filling hole. In this paper, we present a hole filling method based on feature lines, including the hole boundary lines, ridge lines and valley lines. First, the feature lines are extracted and used to generate a series of curves. Then the control points sequence is obtained by fitting these curves. Finally, the NURBS surface is fitted by control points sequence to fill the hole. The obtained results show that our method can preserve the initial shape characteristics of hole.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The development of eye tracking-based applications has witnessed a number of advancements over the past few years. As a result, a number of low cost commercial remote vision-based eye trackers started to appear in the market. Consequently, a number of research communities started to explore the feasibility of extending the eye-tracking capabilities beyond single computer screen and utilize it in multi-screen setup. One of the main challenges for the wide adoption of such eye trackers in multi-screen setup, is their limitations when it comes to an intuitive and reliable way for tracking human eye movements across these multiple screens without losing much of the eye tracking data itself. In this work, a novel data-driven approach based on deep recurrent neural networks for a reliable and responsive switching mechanism between low cost multi-screen eye trackers is proposed. Our approach has achieved a competent results in terms of higher accuracy and lower positive rate in detecting accurately the screen the subject is attending to with F1 measure score of 85%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: With the development of deep learning, image super-resolution has made great breakthroughs. However, compared with a color image, the performance of depth map super-resolution is still poor. To address this problem, multilevel recursive guidance and progressive supervised network (MRG-PS) is proposed in this paper. First, a multilevel recursive guidance architecture is presented to extract features of a color stream and depth stream, in which the depth stream is guided by the color features at each level. Second, a progressive supervision module is developed to supervise the multilevel recursion to obtain depth residual information on different levels. Finally, a residual fusion and construction strategy is designed to fuse all residual information and reconstruct the high-resolution depth map. The experimental results demonstrate that the proposed method outperforms the state-of-the-art methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The PowerFlex 750-Series products with TotalFORCE control are Architecture Class AC drives, bus supplies, and common bus inverters for the Low Voltage market from 160kW to 6000kW. An Opal-RT Hardware-in-the-Loop (HIL) system was chosen to perform a wide variety of product software and hardware verification and validation during the product design phase and will be used for regression testing over the life of the product. As with any simulation, fidelity and accuracy of the simulation must adequately match the product itself in order to guarantee usefulness and confidence in any testing and verification. This paper demonstrates through simulation and experimental results that the HIL system provides an accurate and flexible test and verification platform for the PowerFlex 755TM Common Bus Inverter.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The study of bioelectric potential of plant has been conducted using some methods. Such as, decision tree (J48), multilayer perceptron, ANN, CNN, and etc. However, to find the best accuracy is a seriously challenge because the previous studies did not obtain a satisfied result. Furthermore, Because the data is sequence form, it is interesting if analyzed by deep learning (LSTM) and time series method (ARIMA). Both of methods are have the same characteristics. This approach is a new contribution for this topic. Therefore, the aim of this research is to compare LSTM and ARIMA method for analyzing bioelectric potential data set. For determining the accuracy, we use root mean square error (RMSE) and mean absolute error (MAE). Finally, in this case, the ARIMA model is better than LSTM method and presented a promise result.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Trust modeling has received a remarkable attention in recent research. It is a decision making procedure which quantifies the major concern in information exchange, social exchange, and cognitive motivation. Trust has temporal behavior and it grows and declines on the basis of some cogitative parameters. Current study is based on psychological approach through which complicated relationships among different individuals can be model. In this study, we have proposed a computation cognitive trust model between pet and owner using difference equations. Results revels that proposed model have similar response as the assumptions was made. Moreover results were also obtained using simulation language.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Internet of Things (IoT) refers to networks with billions of physical devices for collecting, sharing, and utilizing data in the virtual world. Most of IoT applications centralize security assurance in creating, authenticating, transferring, or delating system components. However, the centralization exposes its limitations to meet security needs of a rapidly growing number of things world-widely. How to scale up the applications with assured security becomes a critical challenge. Blockchain technology (BCT) is a promising solution to provide security and protect privacy in a large scale; especially, smart contracts offer opportunities to improve the reliability of IoT applications. Smart contracts establish trusts for both of data and executed processes. Recently, many literature surveys and positioning articles have been published on the integration of BCT with IoT, but they are limited to superficial discussions of technical potentials, and very few of them have a thorough exploration of the challenges in developing BCT for IoT at technical levels. This paper uses the system design approach to scrutinize the state of the art of study on BCT-based applications and clarify critical research areas of enabling BCT for security assurance: 1) the relations of BCT and IoT are modeled and discussed; 2) the needs of eliminating threats in IoT-based applications are defined as functional requirements (FRs), existing works on enabling technologies of BCT are defined as the physical solutions (PSs); and 3) the mappings between FRs and PSs are established to identify the limitations and the critical areas for the applications of BCT in large-scale distributed environment.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Accurately identifying DNA-binding proteins (DBPs) from protein sequence information is an important but challenging task for protein function annotations. In this paper, we establish a novel computational method, named TargetDBP, for accurately targeting DBPs from primary sequences. In TargetDBP, four single-view features, i.e., AAC (Amino Acid Composition), PsePSSM (Pseudo Position-Specific Scoring Matrix), PsePRSA (Pseudo Predicted Relative Solvent Accessibility), and PsePPDBS (Pseudo Predicted Probabilities of DNA-Binding Sites), are first extracted to represent different base features, respectively. Second, differential evolution algorithm is employed to learn the weights of four base features. Using the learned weights, we weightedly combine these base features to form the original super feature. An excellent subset of the super feature is then selected by using a suitable feature selection algorithm SVM-REF+CBR (Support Vector Machine Recursive Feature Elimination with Correlation Bias Reduction). Finally, the prediction model is learned via using support vector machine on the selected feature subset. We also construct a new gold-standard and non-redundant benchmark dataset from PDB database to evaluate and compare the proposed TargetDBP with other existing predictors. On this new dataset, TargetDBP can achieve higher performance than other state-of-the-art predictors. The TargetDBP web server and datasets are freely available at http://csbio.njust.edu.cn/bioinf/targetdbp/ for academic use.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Artificial Intelligence has genuine imperative influence in our daily life rendering to the problem resolving. Many approaches were presented recently for developing TCBR applications. A classical and fundamental application is QA system with the assistance of textual cases, expertise acquaintance accumulation in the library full of past cases explanations and with the help of multi-agent. This challenge essentially needs an intelligent approach to overcome this problem. Many QA systems have been developed for English and other well-known languages however, no significant work was found for Urdu language. Despite spoken by millions of people, Urdu is an under-resourced language in terms of available computational resources. It inherits a lot of vocabulary from Arabic, Persian and the native languages of South Asia. Due to this influence, it has a complex morphology. In the proposed system, we meet the challenges in this domain by the assist of Question Answer system and utility assembly of Urdu language processing. Moreover, the solution shows the working of the knowledge system and deliberates good results.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Object detection is one of the key software components in the next generation of autonomous cars. Classical computer vision and machine learning approaches for object detection usually suffer from the slow response time. Modern algorithms and architectures based on artificial neural networks, such as YOLO (You Only Look Once) algorithm, solve this problem without precision losses. In this paper we provide the demonstration of the usage of the newest YOLOv3 algorithm for the detection of traffic participants. We have trained the network for 5 object classes (car, truck, pedestrian, traffic signs, and lights) and have demonstrated the effectiveness of the approach in the variety of the driving conditions (bright and overcast sky, snow, fog, and night).", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In a world where electric mobility is defining our way of living, electric storage is of great importance especially in applications such as electric vehicles. Although battery technologies are diverse, Lithium-ion technology dominates the market due to its high performance. However, in order to keep the security of this part, it is essential to use a battery management system (BMS) to ensure safe and optimum operation. As the key function of this system, accurate state of charge (SOC) estimation is crucial. In this paper, we propose an Extended Kalman Filter (EKF) for the state of charge estimation. Firstly, to achieve the best operation of the EKF an accurate model is required; in this work the first-order Thevenin is presented to model the behaviors of the battery. The internal parameters of the selected model are then identified using the least square algorithm. Simulation results of the model alongside the EKF algorithm for SOC estimation of 3.7V/2.6Ah capacity lithium battery are presented, followed by their implementation on electronic card, which consists of a PIC18F4550 microcontroller.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The incremental distribution network operating income will be the focus of attention of the company that has the power of incremental distribution network operation under the electricity reform. Based on this, this passage establishes the dynamic economic dispatch model of incremental distribution network considering P2G. It also proposes a catastrophe genetic algorithm based on double iterative optimization genetic algorithm to solve the time coupling problem under the constraints of control means, such as energy storage and demand-side response. Taking the improvement IEEE33 node model as an example, the influence of various regulatory methods on the operating income of incremental distribution network is analyzed and discussed. It is verified that the consideration of 2PG and demand-side response is essential to improve the operating income of incremental distribution network.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In recent years, AC power flow equations have been gaining relevance to represent the transmission planning problem. This trend will continue growing because of fast improvements of both computational resources and robust optimization techniques. Relevant problems associated with the voltage magnitude and reactive power may be difficult to represent in the transmission planning problem if the DC mathematical formulation is used instead of the AC formulation. However, the AC formulation remains difficult to solve even with the nowadays solution methodologies. Therefore, the development of new methods that allows to solve effectively the AC formulation are highly relevant. In this paper, novel heuristic methods to solve the transmission planning problem are proposed. The results show that the proposed heuristics for the AC transmission planning problem are both effective and efficient to solve the problem. Moreover, it has been determined the most suitable heuristic criteria for the proposed heuristics. The simulations are performed in the North Northeast Brazilian test system.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Bitcoin is a decentralized digital currency that has gained significant attention and growth in recent years. Unlike traditional currencies, Bitcoin does not rely on a centralized authority to control the supply, distribution, and verification of the validity of transactions. Instead, Bitcoin relies on a peer-to-peer (P2P) network of volunteers to distribute pending transactions and confirmed blocks, to verify transactions, and to collectively implement a replicated ledger that everyone agrees on. This P2P network is at the heart of Bitcoin and many other blockchain technologies. In this paper, we present a comparative measurement study of nodes in the Bitcoin network. We measure and analyze how many the so-called \u201cvolunteers\u201d are in the Bitcoin P2P network by scanning the live Bitcoin network for 37 days in 2018 and compare them with the data reported by prior work in 2013~2016. This paper is motivated by the fact that Bitcoin has experienced explosive growth in terms of a number of users, transactions, value, and interest over a recent couple of years. Our investigation includes the IP addresses of Bitcoin nodes, size of the network, power law in the geographic distribution, protocol, and client versions, and network latencies and shows how today's network is different from early days. In addition, based on the observations made from the measurement study, we propose a simple distance-based peer selection rule for improved connectivity and faster data propagation. The evaluation results show that our proposed lightweight and backward-compatible peer selection rule has the potential to reduce data dissemination latency.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In order to solve the problem that the semantic gap cannot be alleviated effectively in the field of medical image retrieval, we propose a new graph semi-supervised learning method for medical image automatic annotation. Because the model adopts semi-supervised technology, it can learn from abundant unlabeled instances to avoid the decreasing of generalization ability which is induced by the relative lack of labeled data. Furthermore, by improving graph based semi-supervised learning technology with normalization and modification of decision boundary on its iterative results, the scoring model effectively reduces the bad impact of asymmetric dataset. In view of the relationship between word extraction and image in image learning model, we analyze image similarity calculation in detail. It effectively combines together into the physician's diagnosis information as high-level semantic feature of image, to calculate the similarity between images more effectively. Finally, the Toy data and clinical data sets gastroscope image sets are conducted with a series of experiments, the results show that the method is superior to traditional image annotation method in this paper.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Information fusion is an essential part of numerous engineering systems and biological functions, e.g., human cognition. Fusion occurs at many levels, ranging from the low-level combination of signals to the high-level aggregation of heterogeneous decision-making processes. While the last decade has witnessed an explosion of research in deep learning, fusion in neural networks has not observed the same revolution. Specifically, most neural fusion approaches are ad hoc, are not understood, are distributed versus localized, and/or explainability is low (if present at all). Herein, we prove that the fuzzy Choquet integral (ChI), a powerful nonlinear aggregation function, can be represented as a multilayer network, referred to hereafter as ChIMP. We also put forth an improved ChIMP (iChIMP) that leads to a stochastic-gradient-descent-based optimization in light of the exponential number of ChI inequality constraints. An additional benefit of ChIMP/iChIMP is that it enables explainable artificial intelligence (XAI). Synthetic validation experiments are provided, and iChIMP is applied to the fusion of a set of heterogeneous architecture deep models in remote sensing. We show an improvement in model accuracy, and our previously established XAI indices shed light on the quality of our data, model, and its decisions.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we present a new approach to design photonic crystal based optical filters using machine learning based mathematical model. The presented optical filter device finds its application in near infrared spectral range. The design and spectral response of the filter can be predicted using the proposed mathematical model which can considerably reduce simulation time and efforts. The numerical simulation of the optical filter device along with its spectral results and mathematical modeling are described.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The paper provides detailed description of experience from practical use of a newly developed \"TechPedia\" educational system tailored mainly for professional secondary schools specializing in ICT, electrical engineering and automation. The content for the system has been authored by experts from several European universities. Its purpose is to increase the attractiveness of secondary education and training, so that students' professional growth can seamlessly continue at technical universities. The project consortium has developed modern electronic learning environment and hundreds of learning objects, such as learning modules, worksheets, tests, multilingual dictionary and encyclopedia that gave the name to the whole project - TechPedia, which is currently in its pilot phase, i.e. practical testing with students and teachers at many schools across Europe. Students can also compare their knowledge thanks to an international \"Technical Olympiad\". The project results were also awarded the first prize at eLearning 2016 nationwide competition.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Millimeter wave (mmWave) communication has become a key enabling technology for 5G and beyond networks because of its large bandwidth and high transmission rate. In a vehicular mmWave system, beam tracking is a challenging task due to the user's fast mobility and narrow beam of mmWave transmission. In this paper, we study the intelligent beam tracking scheme with low training overhead for mmWave vehicular transmission. Specifically, we utilize the past channel state information (CSI) to efficiently predict the future channel by designing a machine learning prediction model. Using such predicted CSI, the base stations (BSs) reduce the number of channel estimations and save the overhead of pilots. We build the prediction model based on a long short term memory (LSTM) structure whose dataset is composed of the channel vectors of each coherence time duration. The experiments show that the proposed LSTM can accurately predict the channel of the vehicular user and achieve satisfactory transmission rate with less pilot overhead than that of traditional beam training scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Network Slicing is one of the key features of the new 5G cellular network communication, it proposes the division of one physical network into multiple virtual networks to achieve specific goals such as security, flexibility and control over the network that will provide logical isolation in the devices, services and core networks set up for different characteristics and different types of services. Our proposed work uses end-to-end network slicing concept in 5G networks to solve the key issue of isolating the slices via prioritizing them in order to increase performance and decrease latency for high priority applications. Simulation results using NS-3 network simulator prove our claims and show enhancements in latency and performance.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Due to the spectrum scarcity issue in conventional cellular networks, millimeter-wave (mmWave) technology has been proposed and is expected to be used in small cells to meet the requirements for spectrum resources, especially for those small cells that are densely deployed and form a multi-hop backhaul structure. For this novel architecture, a cross-layer optimization problem aiming at maximizing the energy efficiency while taking into account both the route selection and resource allocation is formulated. To find the optimal solution, we first decouple the original problem into a resource allocation sub-problem at the link-physical layer and a route selection sub-problem at the network layer. Since the resource allocation sub-problem at the link-physical layer is a non-convex problem and NP-hard, we apply stochastic algorithms to search for the optimal solution. Since the network layer sub-problem is simplified as a linear programming problem, an LP solver is applied to optimize energy efficiency. To reflect the interplay between these two problems, we further propose two joint optimization strategies. One uses stochastic algorithms that are composed of four schemes with given routes for resource allocation and linear programming for the route selection based on the given resource allocation. The other uses the linear programming to evaluate the fitness of each individual in the stochastic algorithms. The simulation results reveal the following: 1) our proposed schemes are capable of finding near-optimal solutions and can improve the energy and spectrum efficiency; and 2) properly finding the route without overloading any one of the base stations is capable of improving the throughput of the networks.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Currently, the security situation of data security and user privacy protection is increasingly serious in cloud environment. Ciphertext data storage can prevent the risk of user's privacy disclosure. But how to search keyword on ciphertext data without revealing keyword information becomes a new challenge. Searchable encryption (SE) is put forward for this reason, which can be used to realize ciphertext-search directly. In terms of multi-user data sharing, public-key encryption with keywords search (PEKS) is more widely used than symmetric searchable encryption. PEKS has been widely studied and developed by researchers in recent years. However, the existing PEKS scheme often lacks flexible access policy. Therefore, combining the advantage of policy control on attribute-based encryption (ABE), and as that the bilinear pairing related assumption is fragile in post-quantum era, lattice-based cryptography is considered as one of secure encryption technology against quantum attack. With this background, in this paper, we give a keyword-searchable ABE scheme based on the hardness of lattice problems, our scheme supports flexible attribute control policy by integrating ABE and PEKS, and the security of new scheme is proved under the learning with errors (LWE) assumption. As lattice-based cryptographic technology is currently thought to be resistant to quantum attacks, so the new scheme has stronger security in a quantum era.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Matrix completion has been widely used in image processing, in which the popular approach is to formulate this issue as a general low-rank matrix approximation problem. This paper proposes a novel regularization method referred to as truncated Frobenius norm (TFN), and presents a hybrid truncated norm (HTN) model combining the truncated nuclear norm and truncated Frobenius norm for solving matrix completion problems. To address this model, a simple and effective two-step iteration algorithm is designed. Further, an adaptive way to change the penalty parameter is introduced to reduce the computational cost. Also, the convergence of the proposed method is discussed and proved mathematically. The proposed approach could not only effectively improve the recovery performance but also greatly promote the stability of the model. Meanwhile, the use of this new method could eliminate large variations that exist when estimating complex models, and achieve competitive successes in matrix completion. Experimental results on the synthetic data, real-world images, and recommendation systems, particularly the use of the statistical analysis strategy, verify the effectiveness and superiority of the proposed method, i.e., the proposed method is more stable and effective than the other state-of-the-art approaches.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions. However, due to the insufficient 3D pose data for training and the domain gap between 2D space and 3D space, these methods have limited scalabilities for all practical scenarios (e.g., outdoor scene). Attempt to address this issue, this paper proposes a simple yet effective self-supervised correction mechanism to learn all intrinsic structures of human poses from abundant images. Specifically, the proposed mechanism involves two dual learning tasks, i.e., the 2D-to-3D pose transformation and 3D-to-2D pose projection, to serve as a bridge between 3D and 2D human poses in a type of \u201cfree\u201d self-supervision for accurate 3D human pose estimation. The 2D-to-3D pose implies to sequentially regress intermediate 3D poses by transforming the pose representation from the 2D domain to the 3D domain under the sequence-dependent temporal context, while the 3D-to-2D pose projection contributes to refining the intermediate 3D poses by maintaining geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, these two dual learning tasks enable our model to adaptively learn from 3D human pose data and external large-scale 2D human pose data. We further apply our self-supervised correction mechanism to develop a 3D human pose machine, which jointly integrates the 2D spatial relationship, temporal smoothness of predictions and 3D geometric knowledge. Extensive evaluations on the Human3.6M and HumanEva-I benchmarks demonstrate the superior performance and efficiency of our framework over all the compared competing methods.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unlike traditional rigid robots, soft robots can deform in a wide range to suit environment and contact compliantly with operating objects. Therefore, soft robots have wide potential application prospects in terms of physical rehabilitation, minimally invasive surgery and other fields. In this paper, we present an omni-directional flexural inflatable flexible arm model, which consists of three independent controllable pneumatic components. The components have different elongations under the effect of pressure and flexible arm is bending deformation. Under ideal conditions, it can be achieved 90\u00b0 bending and 360\u00b0 twisting. Based on the Yeoh model of hyperelastic rubber material and the geometric analysis method, we present the mathematical model of the flexible arm and FEM capabilities. We use finite element analysis to simulate the actuation characteristics of these modules. We compared the analytical and computational results to experimental results and can be used for the future design and control of soft robotic actuators.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Motor imagery brain-computer interfaces (BCIs) can control external machines with neurophysiological signals during limb movement imagination without real movements. An electroencephalography (EEG)-based motor imagery task has notable characteristics of event-related (de) synchronization (ERD/ERS) in specific frequency bands, including alpha and beta rhythms in the sensorimotor area. Based on this phenomenon, motor imagery features are extracted typically using common spatial patterns (CSPs). However, some researchers have reported that ERD features have severe inter-subject variation. In this study, we investigated the correlation between various ERD features and classification accuracy during a motor imagery task. We found that ERDs may not be useful in estimating classification accuracy, although they are representative features of a motor imagery task.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: A new approach for diminution of side lobe Ratio or level (SLL)maximum with half power beam width (HPBW) of thinned large multiple concentric circular antenna of the uniformly exuberated isotropic element, is proposed. Itis a novel optimization algorithm based on Seeker Optimization Algorithm (SOA) is used. Concentric Circular Antenna Array (CCAA) has been greatly recognized within subject of base radio station of the radio and telecommunications system. Since it has a 360-degree scan around its focus, beam pattern and all-azimuth scan capability, it has numerous advantages over other array geometries. A 9 ringed CCAA with a central element feeding is taken into consideration. CCAA which is having isotropic elements is examined by this algorithm and is shown appropriate.. Optimization efficiency is justified by antenna array synthesis, which is the result of themassive simulation. In all the cases mentioned in the paper, relative sidelobe level (SLL) is less than -20 dB with fixed HPBW compared to case of a uniform array. By using SOA with isotropic elements, the results of the simulation depict the amount of effective antenna elements can be pared from 279 to 120 with consistent abatement in side lobe ratio by 21.04 dB relative to that of the main beam with a fixed HPBW. Standard Particle Swarm Optimization (PSO) is used here to scrutinize outcome of SOA.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Today, a wide variety of applications with different requirements are rapidly developed in industrial wireless sensor networks, and providing the Quality of Service (QoS) for this kind of communication network is inevitable. It is difficult to solve the problems of poor adaptability and difficulty in the implementation of the QoS-based network configuration and management in traditional network architecture. We present a hierarchical software-defined network architecture for wireless sensor networks, which makes the complex network management possible and the system more adaptable. Furthermore, we propose a QoS-based routing protocol, called QSDN-WISE, which consists of a clustering algorithm, a routing algorithm, and local network maintenance. A double-cluster head-based uneven clustering algorithm, called DCHUC, avoids the energy hole phenomenon and reduces the workload of a single cluster head. The centralized QSDN-WISE routing algorithm constructs two heterogeneous forwarding paths for nodes, which meets the requirements for different data levels. Local network maintenance reduces the number of control messages in the network. The simulation results indicate that the QSDN-WISE can provide the QoS support for data with different requirements, balance the network energy consumption, and prolong the network's lifetime.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: We consider the problem of estimating an undisturbed, scalar, linear process over a \u201ctiming\u201d channel, namely a channel where information is communicated through the timestamps of the transmitted symbols. Each transmitted symbol is received at the decoder subject to a random delay. The encoder can encode messages in the holding times between successive transmissions and the decoder must decode the message from the inter-reception times of successive symbols. This set-up is analogous to a telephone system where a transmitter signals a phone call to the receiver through a \u201cring\u201d and, after the random time required to establish the connection, is aware of the \u201cring\u201d being received. We show that for the estimation error to converge to zero in probability, the timing capacity of the channel should be at least as large as the entropy rate of the process. In the case the symbol delays are exponentially distributed, we show a tight sufficient condition using a random-coding strategy.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This brief presents analytical results on the effect of additive weight/bias noise on a Boltzmann machine (BM), in which the unit output is in {-1, 1} instead of {0, 1}. With such noise, it is found that the state distribution is yet another Boltzmann distribution but the temperature factor is elevated. Thus, the desired gradient ascent learning algorithm is derived, and the corresponding learning procedure is developed. This learning procedure is compared with the learning procedure applied to train a BM with noise. It is found that these two procedures are identical. Therefore, the learning algorithm for noise-free BMs is suitable for implementing as an online learning algorithm for an analog circuit-implemented BM, even if the variances of the additive weight noise and bias noise are unknown.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: To increase noise immunity of the MFCC (mel-frequency cepstral coefficients) widely used for voice signal parametrisation it is suggested to use a psychoacoustic model of frequency masking. Additionally, taking into account the formation mechanism of the formant regions in the voice signal spectrum it is suggested to influence the spectral samples corresponding to multiple harmonics of the fundamental tone. The modified algorithm is investigated on the basis of the single word recognition system adapted for MFCC voice signal parametrisation only. The positive effect of using proposed additional voice signal transformation in the parametrisation algorithm is shown.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: The security situation of the Internet of Things (IoT) is more serious than ever, and there is an urgent need to detect and patch device vulnerability rapidly. With the astronomical numbers of IoT devices, it is very difficult to execute regular security inspections. Existing vulnerability detection technology based on simple feature matching cannot reach high accuracy to detect firmware vulnerabilities while using a control flow graph matching directly has proven to be too expensive. To address the problem of accurate and efficient, we present a method of staged firmware vulnerability detection based on code similarity. The first stage, function embedding based on neural network is used to analyze the similarities among functions, and large-scale firmware security inspection can be achieved efficiently. The second stage, the similarity among function local call flow graphs is calculated for fine-grained firmware security analysis, and this stage can improve the accuracy of vulnerability detection. We compared our method with state-of-the-art approaches, and the experimental results demonstrate that our method is more accurate. The average retraining time of our method is 1 h, and the real-world firmware vulnerability detection experiment of our method demonstrates that the true positive rate of the top 30 is as high as 86%.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Unmanned aerial vehicle (UAV) communication is a promising technology for Internet of Things (IoT) systems. In this paper, we combine UAV communication and nonorthogonal multiple access (NOMA) for constructing high capacity IoT uplink transmission systems, where UAVs are used as aerial base stations for collecting data from IoT nodes while NOMA is invoked for uplink transmission. We aim to maximize the system capacity by jointly optimize the subchannel assignment, the uplink transmit power of IoT nodes, and the flying heights of UAVs. We commence by proposing an efficient subchannel assignment algorithm relying on the classic K-means clustering method and matching theory. Then, we determine both the distributed uplink transmit power of IoT nodes and flying heights of UAVs based on successive optimization approach. An alternative optimization algorithm is also proposed for finding the near-optimal solutions. Finally, the numerical results demonstrate the superiority of our proposed scheme.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Many applications of hyperspectral remote sensing involve automatic detection of solid targets. The design and applicability of such systems depend on their detection performance under various deployment scenarios; thus, it is useful to develop an accurate and realistic performance model. In this paper, we present a hyperspectral target detection performance prediction model for the common matched filter and normalized matched filter detection algorithms. A statistical model for hyperspectral data is discussed, which includes a replacement model for target pixels and a finite mixture of t-elliptically contoured distributions for background materials. For this general input model, analytic forms for detector output distributions are not always available. The main contribution of this paper is to develop an efficient and robust performance simulation technique using Monte Carlo (MC) with importance sampling. Compared to standard MC, our simulation technique requires a substantially smaller sample size to obtain accurate performance estimates at low false alarm rates. The proposed technique is useful for predicting detection performance over a wide range of input models when analytic solutions are unavailable.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper presents a simulation model for a novel method of indoor localization that is based on the phase difference between elements in an antenna array. The localization system is used to identify the position of a semi-passive radio frequency identification (RFID) tag. For proof of concept, a set of experiments for phase measurements are designed using a commercially available transceiver. Results demonstrate that the proposed simulation model comparatively provides an accurate estimation as well as assessment tool of phase measurements for indoor localization application.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In view of the disadvantages of the traditional static measurement methods, such as low efficiency, inconvenient storage of data and higher requirements for the testers, a dynamic measurement method for real-time online measurement of the amplitude frequency response of the broadcasting transmitter is proposed, and the model and test principle of the measurement system are explained. In this method, the input and output signals of the broadcast transmitter are collected in real time. First, the generalized cross correlation algorithm is used to find the system delay, and then the system amplitude-frequency response is calculated by the FFT algorithm of time delay data interception. Experiments show that the method is accurate and reliable, so it has good practicability and generalization.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: VANET broadcasting algorithm plays an important role in ensuring the reliability and speed of information interaction between vehicles. Recently, the multi-hop broadcast has become a promising direction. In this paper, we propose a multi-hop broadcasting algorithm called delay-constraint broadcast combined with resource reservation mechanism (DBCRRM) which can increase the packet delivery ratio (PDR) effectively. The proposed broadcasting algorithm can make use of the distance between the source node and receiving nodes to select the forwarding node and uses the resource reservation mechanism (RRM) to select the resource of timeslot and frequency. Meanwhile, the algorithm can ensure the delay in the permissible range. Due to the difficulty of the field test in VANET broadcasting algorithms, most VANET broadcasting algorithms are only evaluated by simulation but lack the results of field tests. To make up the deficiency, we design a field test platform for broadcasting algorithms in VANET, and practically test the delay-constraint broadcast (DB) and DBCRRM. Compared to the test results of two-hop broadcast and the one-hop broadcast, the DBCRRM and DB show the superiority and practicability.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Existing forensic techniques for image manipulation localization crucially assume that probe pixels belong to one of exactly two classes, genuine or manipulated. This letter argues that this convention fuels mis-labeling particularly in unsupervised settings, where singular but genuine content or the presence of multiple distinct manipulations may easily induce non-optimal partitions of the feature space. We propose to relax constraints via a greedy n-ary clustering approach, which we instantiate exemplarily in the popular pixel descriptor space of residual co-occurrences. Experimental results on widely used public benchmark datasets highlight the benefits of our approach.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: In this paper, we propose a novel way for analysis of manufacturing data related to production of circuit breakers which are critical components of power distribution lines. Typically, the life span of a circuit breaker is of the order of 30 to 40 years. However, certain circuit breakers fail early within short span of 0-5 years and it's important to find out the reasons for early failure of these critical components. In this paper, we propose a systematic approach to look into the manufacturing data to gain insight into the reasons for failures and provide recommendations to the manufacturer for improving the manufacturing process/operations based on these insights. The proposed approach is based on machine learning and data analytics.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Integration of solid-liquid phase change materials (PCMs) into electronics packaging has demonstrated the potential to reduce the transient temperature rise of components that experience pulsed thermal loads. However, the impact on local temperature histories resulting from incorporating PCMs in different locations and configurations within an electronics package is not easy to analytically determine, due in large part to the non-linear response of PCMs to a transient heat load. ParaPower is a new parametric design tool to facilitate the design of electronics packages. The tool has the capability of easily incorporating arbitrarily located PCM volumes and evaluating their effect on temperature distributions within the electronics package as a function of time. This paper determines the spatial and temporal order of convergence for the reduced order phase-change thermal model which underlies ParaPower. The results are compared and validated against both an analytical solution and a higher-fidelity commercial finite element analysis (FEA) tool. This paper has shown the fast-solving methods used in the ParaPower tool give results with comparable accuracy to those obtained using high-fidelity commercial software. Reasonably good accuracy can be obtained with fairly large time steps and grid spacing allowing fast-solving design space exploration with this option to increase fidelity within the tool to obtain higher accuracy when necessary. This research quantifies the trade-off between time steps, grid size, and accuracy such that a useful balance can be obtained. Design tools, such as ParaPower, have the potential to significantly advance design theory to reduce size and cost as well as minimize the prevalence of overdesign.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Public blockchain network (PBN) has been widely used in wired networks such as bitcoin network, in which proof-of-work (PoW) algorithm is deployed among miners to reach consensus on users data during the mining process. However, the PoW consensus mechanism is computation-consuming which obstacles the application of PBN in wireless mobile networks since most Internet of Things/mobile devices (IMDs) are resource limited. Recently, mobile edge computing (MEC) has been regarded as a promising technology which can allow IMDs to offload their computation tasks to the edge nodes. Although IMDs can offload their computation tasks to the edge nodes, there is still lots of competition among enormous solo mining IMDs when reaching consensus. In this paper, we first formulate the computation resource allocation problem of PBN from the viewpoint of coalition game theory under the MEC environment. Then, we propose a coalition formation game-based algorithm to maximize the system sum utility and take both the individual profit of IMD and coalition profit into consideration. Furthermore, we prove the proposed algorithm converges to a Nash-stable partition in a fast convergence rate and finally reaches the near-optimal solution with low computational complexity. The simulation results demonstrate the optimality and convergence of the proposed algorithm, and the proposed algorithm outperforms other schemes in terms of system sum profit and ratio of rewarded IMDs to overall IMDs.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Good action recognition relies on correct interpretation of two critical attributes related to action: the spatial attribute on the detected person's posture, and the temporal attribute on the detected person's body movement. Whereas deep learning has greatly improved image recognition, we have not found a similar progress for action recognition. One of the main reasons is due to the complexity caused by the additional temporal dimension; another, to the fact that there are less annotated training data samples for action recognition than that for image recognition. In this regard, this paper proposes a handcrafted cued LSTM model for human action recognition based on RGB-D data, as a collection of 25 skeleton joints in 3D coordinates, found in NTU-RGB-D, currently the most comprehensive dataset for action recognition. As opposed to the raw data of skeleton joints, handcrafted cues, pre-processed results geared to facilitate focused learning, are proposed as input to the LSTM structure. In particular, pertaining to the spatial cue, the SVIT cue derived by Skeleton View-invariant Transformation is adopted; pertaining to the temporal cue, the Diff cue computed by taking the displacements of all joint across down-sampled raw data is utilized. Based on the train/test protocol, the experiment we conducted on NTU-RGB-D shows that the recognition result based on either of the proposed handcrafted cues is better than that based on the raw data. In addition, by our proposed techniques of feature fusion and/or decision fusion of these two handcrafted cues, the recognition performance is better than that of the state-of-the-art approaches conducting on the same dataset by same train/test protocol.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: Reducing delay and latency in cloud computing environment is a challenging task for the research community. There are several smart cities in the world. These smart cities contain numerous Smart Communities (SCs), which have number of Smart Buildings (SBs) and Smart Homes (SHs). They require resources to process and store data in cloud. To overcome these challenges, another infrastructure fog computing environment is introduced, which plays an important role to enhance the efficiency of cloud. The Virtual Machines (VMs) are installed on fog server to whom consumers' requests are allocated. In this paper, the cloud and fog based integrated environment is proposed. To overcome the delay and latency issues of cloud and to enhance the performance of fog. When there are a large number of incoming requests on fog and cloud, load balancing is another major issue. This issue has also been resolved in this paper. The load balancing algorithm Cuckoo search with Levy Walk distribution (CLW) and Flower Pollination (FP) are proposed. The proposed algorithms are compared with existing Cuckoo Search (CS) and BAT algorithm. The comparative analysis of these proposed and existing techniques are performed on the basis of Closest Data Center (CDC), Optimize Response Time (ORT) and Reconfigure Dynamically with Load (RDL). The RT of DCs of cloud and clusters, Processing Time (PT) of fogs is also optimized on the basis of CLW and FP.", "response": "0"}
{"prompt": "Belowing text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0. Abstract: This paper deals with theoretical evaluation and practical determination of the parameters of a 3-phase, 4 pole, 320 W, 5 m/s linear permanent magnet synchronous motor (LPMSM). The motor had been designed in the laboratory and fabricated at the works of local manufacturer. The parameters have been estimated by analytical calculations and validated using standard FEM packages. The practically determined machine parameters are found to be in excellent agreement with theoretically estimated values. This fact highlights the accuracy of the design, precision of fabrication and the correctness of the testing exercise. The field weakening capability of the LPMSM motor has also been investigated.", "response": "0"}
